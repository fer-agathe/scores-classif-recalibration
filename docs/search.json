[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Recalibration",
    "section": "",
    "text": "1 Introduction\nThis notebook is the online appendix of the article titled xxx.” It provides supplementary materials to the main part of the paper.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#abstract-of-the-paper",
    "href": "index.html#abstract-of-the-paper",
    "title": "Recalibration",
    "section": "1.1 Abstract of the Paper",
    "text": "1.1 Abstract of the Paper",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#outline",
    "href": "index.html#outline",
    "title": "Recalibration",
    "section": "1.2 Outline",
    "text": "1.2 Outline",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#replication-codes",
    "href": "index.html#replication-codes",
    "title": "Recalibration",
    "section": "1.3 Replication Codes",
    "text": "1.3 Replication Codes\nThe codes to replicate the results displayed in the paper are presented in this ebook. We also provide the codes in an archive file with the following structure:\nSupplementary-materials\n├ ── ebook\n├ ── scripts\n│   └── functions\n|   |   └── data-ojeda.R\n|   |   └── data-setup-dgp-scenarios.R\n|   |   └── metrics.R\n|   |   └── real-data.R\n|   |   └── subsample_target_distribution.R\n|   |   └── utils.R\n│   └── 01_data_targeted_distrib.R\n│   └── 02_data-simulated.R\n│   └── 03_simul-trees.R\n│   └── 04_simul-random-forests.R\n│   └── 05_simul-random-forests-ntrees.R\n│   └── 06_simul-xgb.R\n│   └── 07_simul-glm.R\n│   └── 08_simul-gam.R\n│   └── 09_simul-gamsel.R\n│   └── 10_simul-comparison.R\n│   └── 11_real-priors-illustration.R\n│   └── 12_real-datasets-priors.R\n│   └── 13_real-estimations.R\n│   └── 14_real_results.R\n│   └── proj.Rproj\n Download the Replication Codes (Zip archive)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "target-distribution.html",
    "href": "target-distribution.html",
    "title": "2  Targeted Distribution",
    "section": "",
    "text": "2.1 Algorithm\nIn our generated sample, \\(\\mathcal{D}=\\{(\\boldsymbol{x}_i,y_i,{s}_i),i\\in\\{1,\\cdots,n\\}\\}\\), let \\(\\widehat{f}\\) denote the (empirical) density of scores. For the various scenarios, suppose that we want a specific distribution for the scores, denoted \\(g\\) (uniform, Beta, etc.). A classical idea is to use ‘’rejection sampling’’ techniques to create a subsample of the dataset. Set \\[\nc = \\sup_{s\\in(0,1)} \\frac{\\widehat{f}(s)}{g(s)} \\leq \\infty.\n\\] If \\(c\\) is finite, and not too large, we can use the standard rejection technique, described in Algorithm 2.1. In a nutshell, point \\(i\\) is kept with probability \\((cg(s_i))^{-1}\\widehat{f}(s_i)\\).\nNote: the reference mentioned: Chen (1999)\nIf \\(c\\) is too large, we use an iterative algorithm, described in Algorithm 2.2, inspired by Rumbell et al. (2023) (alternative options could be the ‘’Empirical Supremum Rejection Sampling’’ introduced in Caffo, Booth, and Davison (2002), for instance)\nTo implement this, we define the subset_target() function.\n#' @param data dataset\n#' @param score_name name of the column in data that contains the scores\n#' @param target_fun target distribution function.\n#' @param iter number of iterations.\n#' @param draw if TRUE (default) the distribution of scores (gray bars) and the\n#'  target distribution (in red) are plotted at each iteration.\n#' @seed if not `NULL`, seed to use\n#' @param data dataset\n#' @param probs_name name of the column in data that contains the observed\n#'  probabilities\n#' @param target_fun target distribution function.\n#' @param iter number of iterations.\n#' @param draw if TRUE (default) the distribution of scores (gray bars) and the\n#'  target distribution (in red) are plotted at each iteration.\n#' @seed if not `NULL`, seed to use\n#' @param verbose if `FALSE`, size of subsamplings at each iteration and KS test\n#'  results are hiddent\nsubset_target &lt;- function(data,\n                          probs_name,\n                          target_fun = function(x) dbeta(x,2,2),\n                          iter = 1,\n                          draw = TRUE,\n                          seed = NULL,\n                          verbose = TRUE){\n  select &lt;- rep(nrow(data),iter + 1)\n  if (!is.null(seed)) set.seed(seed)\n\n  # Get the scores from the dataset\n  probs_01 &lt;- data |&gt; pull(!!probs_name)\n  if (verbose == TRUE) cat(\"1) Size ...... \", nrow(data), \"\\n\", sep = \"\")\n\n  # Kolmogorov-Smirnov Test\n  fun &lt;- Vectorize(function(x) integrate(target_fun, 0, x)$value)\n  K &lt;- ks.test(probs_01, fun)\n\n  if (verbose) {\n    cat(\"1)  ks ............ \", K$statistic, \"\\n\", sep = \"\")\n    cat(\"1)  (pvalue) ...... \", K$p.value, \"\\n\", sep = \"\")\n  }\n\n  if (draw) {\n    # Histogram of scores (gray) and target distribution (red)\n    hist(probs_01,probability = TRUE, xlab = \"\", ylab = \"\", main = \"Initial\")\n    val_x &lt;- seq(0,1,length = 601)\n    lines(val_x,target_fun(val_x), col = \"red\")\n  }\n\n  data_subset &lt;- data\n\n  for (k in 1:iter) {\n    n &lt;- nrow(data_subset)\n    initial_density &lt;- kde(x = probs_01, eval.points = probs_01)\n    # Probability to include each observation in the current subset\n    prob_acceptation &lt;- target_fun(probs_01) / initial_density$estimate\n    prob_acceptation &lt;- pmin(prob_acceptation / max(prob_acceptation), 1)\n    # For each scores from the current data subset, decide whether or not to\n    # include it based on a random draw from a Ber(prob_acceptation)\n    index_acceptation &lt;- rbinom(n, size = 1, prob = prob_acceptation)\n    # Use this index to keep only the selected data\n    data_subset &lt;- data_subset[which(index_acceptation ==1 ), ]\n    select[k + 1] &lt;- nrow(data_subset)\n    probs_01 &lt;- data_subset |&gt; pull(!!probs_name)\n    if (verbose == TRUE)\n      cat(k + 1, \") Size ...... \", nrow(data_subset), \"\\n\", sep = \"\")\n    # Kolmogorov-Smirnov Test\n    K &lt;- ks.test(probs_01, fun)\n    if (verbose) {\n      cat(k + 1, \")   ks ............ \", K$statistic, \"\\n\", sep = \"\")\n      cat(k + 1, \")   (pvalue) ...... \", K$p.value, \"\\n\", sep = \"\")\n    }\n    if (draw) {\n      hist(\n        probs_01, probability = TRUE, xlab = \"\", ylab = \"\",\n        main = paste(\"Iteration \", k)\n      )\n      val_x &lt;- seq(0, 1, length = 601)\n      lines(val_x, target_fun(val_x), col = \"red\")\n    }\n  }\n  data_subset\n}",
    "crumbs": [
      "I. Subsampling",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Targeted Distribution</span>"
    ]
  },
  {
    "objectID": "target-distribution.html#algorithm",
    "href": "target-distribution.html#algorithm",
    "title": "2  Targeted Distribution",
    "section": "",
    "text": "\\begin{algorithm} \\caption{Subsample a dataset so that the distribution of scores has density $g$ (Rejection, $c$ small)} \\begin{algorithmic} \\Require $\\mathcal{D}=\\{(\\boldsymbol{x}_i,y_i,{s}_i),i\\in\\{1,\\cdots,n\\}\\}$ and $g$ (target density) \\State $\\mathcal{I} \\gets ,i\\in\\{1,\\cdots,n\\}$ \\State $\\widehat{f} \\gets$ density of $\\{({s}_i),i\\in\\mathcal{I}\\}$, using Chen (1999) \\State $c = \\displaystyle\\sup_{s\\in(0,1)} \\frac{\\widehat{f}(s)}{g(s)} \\gets \\max_{i=1,\\cdots,n}\\displaystyle\\frac{\\widehat{f}(s_i)}{g(s_i)} $ \\For{$i\\in\\{1,\\cdots,n\\}$} \\State $U \\gets \\mathcal{U}([0,1])$ \\If{$\\displaystyle U &gt; \\frac{\\widehat{f}(s_i)}{c\\,g(s_i)}$} \\State $\\mathcal{I} \\gets \\mathcal{I}\\backslash\\{i\\}$ , i.e. ``reject\" \\EndIf \\EndFor \\State $s\\mathcal{D}=\\{(\\boldsymbol{x}_i,y_i,{s}_i),i\\in\\mathcal{I}\\}$ \\end{algorithmic} \\end{algorithm}\n\n\n\n\n\n\n\\begin{algorithm} \\caption{Subsample a dataset so that the distribution of scores has density $g$ (Iterative Rejection, $c$ large)} \\begin{algorithmic} \\Require $\\mathcal{D}=\\{(\\boldsymbol{x}_i,y_i,{s}_i),i\\in\\{1,\\cdots,n\\}\\}$, $\\epsilon&gt;0$ and $g$ (target density) \\State $\\mathcal{I} \\gets \\{1,\\cdots,n\\}$ \\State $\\widehat{f} \\gets$ density of $\\{({s}_i),i\\in\\mathcal{I}\\}$, using Chen (1999) \\State $d \\gets \\|\\widehat{F}-G\\|_{\\infty}$ (Kolmogorov-Smirnov distance) \\While{$d&gt;\\epsilon$} \\State $\\mathcal{J} \\gets \\mathcal{I}$ \\For{$i\\in\\mathcal{I}$} \\State $U \\gets \\mathcal{U}([0,1])$ \\If{$\\displaystyle U&gt;\\frac{\\widehat{f}(s_i)}{g(s_i)}$} \\State $\\mathcal{J} \\gets \\mathcal{J}\\backslash\\{i\\}$ , i.e. 'reject' observation $i$ \\EndIf \\EndFor \\State $\\mathcal{I} \\gets \\mathcal{J}$ \\State $\\widehat{f} \\gets$ density of $\\{({s}_i),i\\in\\mathcal{I}\\}$ \\State $d \\gets \\|\\widehat{F}-G\\|_{\\infty}$ \\EndWhile \\State $s\\mathcal{D}=\\{(\\boldsymbol{x}_i,y_i,{s}_i),i\\in\\mathcal{I}\\}$ \\end{algorithmic} \\end{algorithm}",
    "crumbs": [
      "I. Subsampling",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Targeted Distribution</span>"
    ]
  },
  {
    "objectID": "target-distribution.html#first-example",
    "href": "target-distribution.html#first-example",
    "title": "2  Targeted Distribution",
    "section": "2.2 First Example",
    "text": "2.2 First Example\nLet us begin with generating some binary data, using a linear predictor for the true probability.\n\nn &lt;- 1e5 # Number of obs.\n# Covariates\nx1 &lt;- rnorm(n)\nx2 &lt;- rnorm(n)\n# True probabilities\np &lt;- function(x1, x2) .4 * x1 - .2*x2\n# Observed event\ny &lt;- rnorm(n,p(x1, x2), .4)\ntb &lt;- tibble(y = y, x1 = x1, x2 = x2)\n\nLet us consider a linear model to predict the observed event:\n\nreg &lt;- lm(y ~ x1 + x2, data = tb)\nscores &lt;- predict(reg)\ntb$scores &lt;- scores\n\nKeeping only scores between 0 and 1 (would not need to do so for glm)\n\ntb_01 &lt;- tb[(scores &gt; 0) & (scores &lt; 1), ]\ndata &lt;- tb_01\n\n\nB &lt;- subset_target(data = data, probs_name = \"scores\", iter = 4)\n\n1) Size ...... 48546\n1)  ks ............ 0.3030684\n1)  (pvalue) ...... 0\n\n\n\n\n\n\n\n\n\n2) Size ...... 17255\n2)   ks ............ 0.008122053\n2)   (pvalue) ...... 0.2050541\n\n\n\n\n\n\n\n\n\n3) Size ...... 16775\n3)   ks ............ 0.003340476\n3)   (pvalue) ...... 0.9920448\n\n\n\n\n\n\n\n\n\n4) Size ...... 16569\n4)   ks ............ 0.002640538\n4)   (pvalue) ...... 0.9998302\n\n\n\n\n\n\n\n\n\n5) Size ...... 16274\n5)   ks ............ 0.002374038\n5)   (pvalue) ...... 0.9999881\n\n\n\n\n\n\n\n\n\nLet us consider another example.\n\nlibrary(splines)",
    "crumbs": [
      "I. Subsampling",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Targeted Distribution</span>"
    ]
  },
  {
    "objectID": "target-distribution.html#second-examplle",
    "href": "target-distribution.html#second-examplle",
    "title": "2  Targeted Distribution",
    "section": "2.3 Second Examplle",
    "text": "2.3 Second Examplle\nWe generate another dataset.\n\nn &lt;- 1e6\nx &lt;- rbeta(n, 1, 2)\ny &lt;- rbinom(n, size = 1, prob = x)\nbase &lt;- tibble(\n  x = x,\n  y = y,\n  id = 1:n\n)\n\nLet us assume that the scores are estimated using a logistic model.\n\nreg &lt;- glm(y ~ bs(x), data = base, family = binomial)\nbase$scores &lt;- predict(reg, type = \"response\")\n\nLet us further assume that we want the scores to be distributed according to a Beta(2,1).\n\nB &lt;- subset_target(\n  data = base, \n  probs_name = \"scores\", \n  iter = 1, \n  target_fun = function(x) dbeta(x,2,1)\n)\n\n1) Size ...... 1000000\n\n\nWarning in ks.test.default(probs_01, fun): ties should not be present for the\none-sample Kolmogorov-Smirnov test\n\n\n1)  ks ............ 0.5037986\n1)  (pvalue) ...... 0\n\n\n\n\n\n\n\n\n\n2) Size ...... 7399\n2)   ks ............ 0.0551336\n2)   (pvalue) ...... 5.830865e-20\n\n\n\n\n\n\n\n\n\nWe check the new observations:\n\nreg2 &lt;- glm(y ~ bs(x), data = B, family = binomial)\n\n\nval_x &lt;- seq(0, 1, length = 601)\nplot(\n  val_x,\n  predict(reg, type = \"response\", newdata = data.frame(x = val_x)),\n  type = \"l\", lwd = 2\n)\n\nWarning in bs(x, degree = 3L, knots = numeric(0), Boundary.knots =\nc(6.10598735812861e-07, : some 'x' values beyond boundary knots may cause\nill-conditioned bases\n\nlines(\n  val_x,\n  predict(reg2, type = \"response\", newdata = data.frame(x = val_x)),\n  type = \"l\", lwd = 2, col = \"red\"\n)\n\nWarning in bs(x, degree = 3L, knots = numeric(0), Boundary.knots =\nc(0.00484668961124553, : some 'x' values beyond boundary knots may cause\nill-conditioned bases\n\n\n\n\n\n\n\n\n\n\n\n\n\nCaffo, Brian S, James G Booth, and AC Davison. 2002. “Empirical Supremum Rejection Sampling.” Biometrika 89 (4): 745–54.\n\n\nChen, Song Xi. 1999. “Beta Kernel Estimators for Density Functions.” Computational Statistics & Data Analysis 31 (2): 131–45.\n\n\nRumbell, Timothy, Jaimit Parikh, James Kozloski, and Viatcheslav Gurev. 2023. “Novel and Flexible Parameter Estimation Methods for Data-Consistent Inversion in Mechanistic Modelling.” Royal Society Open Science 10 (11): 230668.",
    "crumbs": [
      "I. Subsampling",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Targeted Distribution</span>"
    ]
  },
  {
    "objectID": "metrics.html",
    "href": "metrics.html",
    "title": "3  Metrics",
    "section": "",
    "text": "3.1 Performance and Calibration Metrics\nTo measure performance, we chose to compute:\nTo measure calibration, we compute two metrics:\nAll these metrics are computed in a function we name compute_metrics() which takes three arguments:\n#' Brier Score\n#'\n#' The Brier Score \\citep{brier_1950}, is expressed as: \\deqn{\\text{BS} =\n#' \\frac{1}{n}\\sum_{i=1}^{n} \\big(\\hat{s}(\\mathbf{x}_i) - d_i\\big)^{2}} where\n#' \\eqn{d_i \\in \\{0,1\\}} is the observed event for observation \\eqn{i}.\n#'\n#' @param scores vector of scores\n#' @param obs vector of observed binary events\n#'\n#' @references Brier, G. W. (1950). Verification of forecasts expressed in terms\n#' of probability. Monthly Weather Review 78: 1–3.\n#'\n#' @export\nbrier_score &lt;- function(obs, scores) mean((scores - obs)^2)\n#' Computes the calibration metrics for a set of observed and predicted\n#' probabilities\n#'\n#' @returns\n#' \\itemize{\n#'   \\item \\code{mse}: True Mean Squared Error based on true probability.\n#'   \\item \\code{acc}: accuracy with a .5 probability threshold.\n#'   \\item \\code{AUC}: Area Under the ROC Curve.\n#'   \\item \\code{brier}: Brier score.\n#'   \\item \\code{ici}: Integrated Calibration Index.\n#' }\n#'\n#' @param obs observed events\n#' @param scores predicted scores\n#' @param true_probas true probabilities from the PGD (to compute MSE)\n#'\n#' @importFrom purrr map\n#' @importFrom tibble tibble\n#' @importFrom dplyr bind_rows\n#'\n#' @export\ncompute_metrics &lt;- function(obs,\n                            scores,\n                            true_probas = NULL) {\n\n  # True MSE\n  if (!is.null(true_probas)) {\n    mse &lt;- mean((true_probas - scores)^2)\n  } else {\n    mse &lt;- NA\n  }\n\n  # True MAE\n  if (!is.null(true_probas)) {\n    mae &lt;- mean(abs(true_probas - scores))\n  } else {\n    mae &lt;- NA\n  }\n\n  # AUC\n  AUC &lt;- pROC::auc(obs, scores, levels = c(\"0\", \"1\"), quiet = TRUE) |&gt;\n    as.numeric()\n\n  # Brier Score\n  brier &lt;- brier_score(obs = as.numeric(as.character(obs)), scores = scores)\n  # gmish::brier(pred = scores, obs = obs) #same results\n\n  # ICI\n  ici_quiet &lt;- purrr::quietly(gmish::ici)\n  ici &lt;- ici_quiet(pred = scores, obs = as.numeric(as.character(obs)))\n  ici &lt;- ici$result\n\n  # Accuracy\n  pred_class &lt;- ifelse(scores &gt; .5, yes = 1, no = 0)\n  acc &lt;- sum(diag(table(obs = obs, pred = pred_class))) / length(scores)\n\n  tibble(\n    mse = mse,\n    mae = mae,\n    acc = acc,\n    AUC = AUC,\n    brier = brier,\n    ici = ici\n  )\n}",
    "crumbs": [
      "II. Metrics",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Metrics</span>"
    ]
  },
  {
    "objectID": "metrics.html#performance-and-calibration-metrics",
    "href": "metrics.html#performance-and-calibration-metrics",
    "title": "3  Metrics",
    "section": "",
    "text": "the true Mean Squared Error (MSE): the average of the quadratic difference between predicted scores and true probabilities (only if the true probabilities are available thanks to the knowledge of the PGD)\nthe accuracy, which gives the proportion of correctly predicted instances; we use a probability threshold of 0.5)\nthe AUC.\n\n\n\nthe Brier score (Brier (1950))\nthe Integrated Calibration Index (Austin and Steyerberg (2019)).\n\n\n\n\n\n\n\nBrier Score\n\n\n\nGiven a sample size \\(n\\), the Brier Score Brier (1950), is expressed as: \\[\n\\begin{equation}\n\\text{BS} = \\frac{1}{n}\\sum_{i=1}^{n} \\big(\\hat{s}(\\mathbf{x}_i) - d_i\\big)^{2}\\enspace ,\n\\end{equation}\n\\tag{3.1}\\]\nwhere \\(\\hat{s}(\\mathbf{x}_i)\\) and \\(d_i \\in \\{0,1\\}\\) are the predicted score and observed event, respectively, for observation \\(i\\).\n\n\n\n\n\n\n\n\nIntegrated Calibration Index\n\n\n\nInstead of defining bins, the Integrated Calibration Index or ICI (Austin and Steyerberg (2019)) measures calibration using a local estimation (loess if the number of observation is lower than 1000 ; using a GAM otherwise).\nThe occurrence of the binary event is regressed on the predicted scores, employing either locally estimated scatterplot smoothing (LOESS) when the number of observations is small (\\(n &lt; 1000\\)) or cubic regression splines for larger datasets. The ICI is defined as \\[\n\\begin{equation}\n    \\text{ICI} = \\int_{0}^{1} f(p)  \\phi(p)\\, dp\n\\end{equation}\n\\tag{3.2}\\] where \\(f(p) = | p - \\g(p) |\\) is the absolute difference between the calibration curve and the bisector where \\(p\\) denotes a predicted score (i.e., \\(p=\\hat{s}(\\mathbf{x})\\)) and \\(\\g(p)\\) is the value of the calibration curve at this predicted score. The density function of the distribution of predicted scores is denoted \\(\\phi(p)\\).\n\n\n\n\nobs: a vector of observed binary events\nscores: a vector of predicted scores\ntrue_probas: if available, a vector of true probabilities from the PGD (to compute MSE).",
    "crumbs": [
      "II. Metrics",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Metrics</span>"
    ]
  },
  {
    "objectID": "metrics.html#divergence-and-dispersion-metrics",
    "href": "metrics.html#divergence-and-dispersion-metrics",
    "title": "3  Metrics",
    "section": "3.2 Divergence and Dispersion Metrics",
    "text": "3.2 Divergence and Dispersion Metrics\nWe compute the Kullback-Leibler divergence Kullback and Leibler (1951) between the distribution of the estimated scores and the distribution of the true probabilities. Denoting (Q) the distribution of the scores and (P) the distribution of the true probabilities, the Kullback Leibler divergence of \\(Q\\) with respect to \\(P\\) is :% \\[\\begin{equation}\nD_{KL}(Q || P) = \\sum_{i} Q(i) \\log \\frac{Q(i)}{P(i)}.\n\\end{equation}\\]\nThe distributions both need to be discretized. We divide the segment ([0,1]) into (m) bins.\nIn the dispersion_metrics() that we define to that end, we consider \\(m=20\\) bins. We also consider switching the reference distribution (where \\(Q\\) denotes the distribution of the true probabilities and \\(P\\) denotes the distribution of scores).\n\n#' Computes the dispersion metrics for a set of observed and predicted\n#' probabilities\n#'\n#' @returns\n#' \\itemize{\n#'   \\item \\code{inter_quantile_25_75}: Difference of inter-quantile between 25% and 75%\n#'   \\item \\code{inter_quantile_10_90}: Difference of inter-quantile between 10% and 90%\n#'   \\item \\code{KL_20_true_probas}: KL of of predicted probabilities w.r. to true probabilities with 20 bins\n#'   \\item \\code{KL_20_scores}: KL of of true probabilities w.r. to predicted probabilities with 20 bins\n#'   \\item \\code{ind_cov}: Difference between the variance of true probabilities and the covariance between true probabilities and predicted scores\n#' }\n#'\n#' @param true_probas true probabilities from simulations\n#' @param scores predicted scores\n#'\ndispersion_metrics &lt;- function(true_probas, scores){\n\n  # Inter-quantiles\n  inter_q_80 &lt;- diff(quantile(scores, c(.9, .1))) /\n    diff(quantile(true_probas, c(.9, .1)))\n  inter_q_50 &lt;- diff(quantile(scores, c(.75,.25))) /\n    diff(quantile(true_probas, c(.75, .25)))\n\n  # KL divergences\n  m &lt;- 20 # Number of bins\n  h_p &lt;- hist(true_probas,breaks = seq(0, 1, length = m + 1), plot = FALSE)\n  h_phat &lt;- hist(scores, breaks = seq(0, 1, length = m + 1), plot = FALSE)\n  # Densities\n  h1 &lt;- rbind(h_phat$density / m,h_p$density / m) # Reference : true probabilities\n  h2 &lt;- rbind(h_p$density / m, h_phat$density / m) # Reference : predicted scores\n  KL_20_true_probas &lt;- distance(\n    h1, method = \"kullback-leibler\", unit = \"log2\", mute.message = TRUE)\n  KL_20_scores &lt;- distance(\n    h2, method = \"kullback-leibler\", unit = \"log2\", mute.message = TRUE)\n\n  # Indicator of the difference between variance and covariance\n  var_p &lt;- var(true_probas)\n  cov_p_phat &lt;- cov(true_probas, scores)\n  ind_cov &lt;- abs(cov_p_phat - var_p)\n\n  # Collection\n  dispersion_metrics &lt;- tibble(\n    \"inter_quantile_25_75\" = as.numeric(inter_q_50),\n    \"inter_quantile_10_90\" = as.numeric(inter_q_80),\n    \"KL_20_true_probas\" = as.numeric(KL_20_true_probas),\n    \"KL_20_scores\" = as.numeric(KL_20_scores),\n    \"ind_cov\" = ind_cov\n    )\n\n  dispersion_metrics\n}\n\nLastly, we estimate \\(\\mathbb{P}(q_1 &lt; \\hat{s}(\\mathbf{x}) &lt; q_2)\\), with \\(q_2 = 1-q_1\\), for different values of \\(q_1\\) and \\(q_2\\). To do so, we simply calculate the sample proportion of scores between \\(q_1\\) and \\(q_2\\). The prop_btw_quantiles() does it.\n\n#' Computes \\hat{P}(q_1 &lt; s &lt; q_2)\n#'\n#' @param s scores\n#' @param q1 lower quantile\n#' @param q2 upper quantile (default to 1-q2)\nprop_btw_quantiles &lt;- function(s, q1, q2 = 1 - q1) {\n  tibble(q1 = q1, q2 = q2, freq = mean(s &lt; q2 & s &gt; q1))\n}\n\n\n\n\n\nAustin, Peter C., and Ewout W. Steyerberg. 2019. “The Integrated Calibration Index (ICI) and Related Metrics for Quantifying the Calibration of Logistic Regression Models.” Statistics in Medicine 38 (21): 4051–65. https://doi.org/10.1002/sim.8281.\n\n\nBrier, Glenn W. 1950. “Verification of Forecasts Expressed in Terms of Probability.” Monthly Weather Review 78 (1): 1–3.\n\n\nKullback, S., and R. A. Leibler. 1951. “On Information and Sufficiency.” The Annals of Mathematical Statistics 22 (1): 79–86. https://doi.org/10.1214/aoms/1177729694.",
    "crumbs": [
      "II. Metrics",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Metrics</span>"
    ]
  },
  {
    "objectID": "simul-data.html",
    "href": "simul-data.html",
    "title": "4  Simulated Data",
    "section": "",
    "text": "4.1 Functions\nWe load the functions from Chapter 2 to subsample from a dataset so that the true probability follows a beta distribution.\nlibrary(ks)\nsource(\"../scripts/functions/subsample_target_distribution.R\")\nThe simulate_data() function generates data for one of the 12 first scenarios described in the article or one of our additional 4 scenarios. This is a helper function that is called in the second one, simulate_data_wrapper() which generates datasets.\nCode\n#' Simulates train/validation/calibration/test\n#'\n#' @details\n#' This function is a modified version of the function 'simulateData' in the\n#' R script 'functions-for-calibrating-random-forests.R' provided in the\n#' supplementary material of  Dankowski, T., & Ziegler, A. (2016). Calibrating\n#' random forests for probability estimation. Statistics in medicine, 35(22),\n#' 3949-3960.\n#'\n#' @param n_num number of numerical covariates\n#' @param add_categ if `TRUE`, add 5 categorical variables\n#' @param coeff vector of coefficients (of length n_num + 5)\n#' @param n_noise number of noise variables (drawn from N(0,1))\n#' @param mean_num vector of mean for the numerical variables\n#' @param sd_num vector of standard deviations for the numerical variables\n#' @param size_train size for the train set\n#' @param size_valid size for the validation set\n#' @param size_calib size for the calibration set\n#' @param size_test size for the test set\n#' @param transform_probs if `TRUE`, the true probability is taken to the power of 3\n#' @param linear_predictor if `TRUE`, the predictor of the true probability is a\n#'  linear combination of the covariates. Otherwise, the squared term for x1 is\n#'  added, as well as an interaction term between x2 and x3 (`n_num` thus need\n#'  to be at least 3).\n#' @param seed desired seed (default to `NULL`)\n#' @param linear_predictor_factor if `transform_probs = TRUE`, scalar used to\n#'  draw more observation before subsampling. Default to 3 (a sample 3 times\n#'  larger than `the size of the samples will first be generated before\n#'  subsampling so that the true probability follows a Beta(2,2).\n#'\n#' @returns A list with the following components:\n#'  - train: train set\n#'  - valid: validation set\n#'  - calib: calibration set\n#'  - test: test set\n#'  - probs_train: true probabilities for binary event in train set\n#'  - probs_valid: true probabilities for binary event in validation set\n#'  - probs_calib: true probabilities for binary event in calibration set \n#'  - probs_test: true probabilities for binary event in test set\nsimulate_data &lt;- function(n_num = 2,\n                          add_categ = FALSE,\n                          coeff,\n                          n_noise = 0,\n                          mean_num,\n                          sd_num,\n                          size_train,\n                          size_valid,\n                          size_calib,\n                          size_test,\n                          transform_probs = FALSE,\n                          linear_predictor = TRUE,\n                          linear_predictor_factor = 3,\n                          seed = NULL) {\n\n  n_obs &lt;- size_train + size_valid + size_calib + size_test\n  if (linear_predictor == FALSE) {\n    n_obs &lt;- n_obs * linear_predictor_factor\n  }\n\n  if (!is.null(seed)) {\n    set.seed(seed)\n  }\n\n  # Numerical covariates\n  covariates &lt;- map2(\n    .x = mean_num,\n    .y = sd_num,\n    .f = ~rnorm(n = n_obs, mean = .x, sd = .y)\n  )\n  names(covariates) &lt;- str_c(\"x\", 1:n_num)\n  covariates &lt;- as_tibble(covariates)\n\n  # Categorical covariates\n  if (add_categ == TRUE) {\n    x_c1 &lt;- base::sample(c(0, 1), n_obs, replace = TRUE)\n    x_c2 &lt;- base::sample(c(0, 1), n_obs, replace = TRUE)\n    x_c3 &lt;- base::sample(c(1, 2, 3), n_obs, replace = TRUE)\n    x_c4 &lt;- base::sample(c(1, 2, 3, 4), n_obs, replace = TRUE)\n    x_c5 &lt;- base::sample(c(1, 2, 3, 4, 5), n_obs, replace = TRUE)\n\n    categ_covariates &lt;- tibble(x_c1, x_c2, x_c3, x_c4, x_c5)\n    colnames(categ_covariates) &lt;- str_c(\"x\", (n_num + 1):(n_num + 5))\n    covariates &lt;- bind_cols(covariates, categ_covariates)\n  }\n\n  if (linear_predictor == TRUE) {\n    # Linear predictor\n    eta &lt;- as.matrix(covariates) %*% coeff\n  } else {\n    if (n_num &lt; 3) stop(\"If linear_predictor=TRUE, n_num must be greater than 2\")\n    eta &lt;- as.matrix(covariates) %*% coeff +\n      covariates$x1^2 + covariates$x2^2 * covariates$x3\n  }\n\n  # True probability\n  true_prob &lt;- as.numeric(1 / (1 + exp(-eta)))\n  if (transform_probs) true_prob &lt;- true_prob^3\n\n  # Observed event\n  y &lt;- rbinom(n_obs, size = 1, prob = true_prob)\n\n  # Create dataset with observed event and covariates\n  tb &lt;- tibble(y, covariates)\n\n  if (linear_predictor == FALSE) {\n    # We would like the probabilities to be distributed as a Beta(2,2)\n    tb &lt;- tb |&gt; mutate(p = true_prob)\n    tb &lt;- subset_target(\n      data = tb,\n      probs_name = \"p\",\n      target_fun = function(x) dbeta(x,2,2),\n      iter = 1, draw = FALSE,\n      seed = seed,\n      verbose = FALSE\n    )\n    n_obs &lt;- size_train + size_calib + size_valid + size_test\n    if (nrow(tb) &lt; n_obs) {\n      stop(\n        str_c(\"The number of observation generated is lower than the \",\n              \"desired number. Increase `linear_predictor_factor`.\")\n      )\n    }\n    true_prob &lt;- tb$p[1:n_obs]\n    tb &lt;- tb |&gt; select(-p) |&gt; dplyr::slice_head(n = n_obs)\n  }\n\n\n  # Noise variables\n  if (n_noise &gt; 0) {\n    noise &lt;- matrix(\n      rnorm(n_noise * n_obs, mean = 0, sd = 1),\n      ncol = n_noise,\n      nrow = n_obs,\n      byrow = FALSE\n    ) |&gt;\n      as_tibble()\n    colnames(noise) &lt;- str_c(\"noise_\", 1:n_noise)\n    tb &lt;- bind_cols(tb, noise)\n  }\n\n  # Split data into train/calib/valid/test\n  tb_train &lt;- tb |&gt; dplyr::slice(1:size_train)\n  true_prob_train &lt;- true_prob[1:size_train]\n  \n  # Validation\n  ind_valid &lt;- (size_train + 1):(size_train + size_valid)\n  tb_valid &lt;- tb |&gt; dplyr::slice(ind_valid)\n  true_prob_valid &lt;- true_prob[ind_valid]\n  \n  # Calibration\n  ind_calib &lt;- (size_train + size_valid + 1):(size_train + size_valid + size_calib)\n  tb_calib &lt;- tb |&gt; dplyr::slice(ind_calib)\n  true_prob_calib &lt;- true_prob[ind_calib]\n  \n  # Test\n  ind_test &lt;- (size_train + size_valid + size_calib + 1):n_obs\n  tb_test &lt;- tb |&gt; dplyr::slice(ind_test)\n  true_prob_test &lt;- true_prob[ind_test]\n  \n  list(\n    train = tb_train,\n    valid = tb_valid,\n    calib = tb_calib,\n    test = tb_test,\n    probs_train = true_prob_train,\n    probs_valid = true_prob_valid,\n    probs_calib = true_prob_calib,\n    probs_test = true_prob_test\n  )\n}\nThe simulate_data_wrapper() is the one we call to generate a dataset, given a scenario and a seed.\nCode\n#' Generates data for a given simulation scenario.\n#'\n#' @details\n#' Wrapper of 'simulate_data' function that generates the data for a given\n#' simulation scenario.\n#'\n#' @param scenario simulation scenario number.\n#' @param params_df data frame containing the parameters to be passed to the\n#'  `simulate_data` for each simulation scenario.\n#' @param repn Number of current replication to be generated for the given\n#'  simulation scenario.\n#'\n#' @returns A list with the following components:\n#'  - scenario: the scenario ID\n#'  - params_df: the parameters used for the data generation for the given\n#'               scenario.\n#'  - repn: Number of current replication that was generated for the given\n#'          simulation scenario.\n#'  - data: list with the simulated data (train, valid, test, probs_train,\n#'          probs_valid and probs_test)\n#'          see result of `simulate_data()`.\nsimulate_data_wrapper &lt;- function(scenario, params_df, repn) {\n  params &lt;- params_df[params_df[[\"scenario\"]] == scenario, ]\n  if(nrow(params) != 1) stop(\"More than one row from params_df chosen\")\n\n  seed_for_repn &lt;- pull(params, \"seed\") + repn\n\n  args &lt;- list(\n    coeff = params |&gt; pull(\"coefficients\") |&gt; pluck(1),\n    n_num = params |&gt; pull(\"n_num\"),\n    add_categ = params |&gt; pull(\"add_categ\"),\n    n_noise = params |&gt; pull(\"n_noise\"),\n    mean_num = params |&gt; pull(\"mean_num\") |&gt; pluck(1),\n    sd_num = params |&gt; pull(\"sd_num\") |&gt; pluck(1),\n    size_train = params |&gt; pull(\"size_train\"),\n    size_valid = params |&gt; pull(\"size_valid\"),\n    size_calib = params |&gt; pull(\"size_calib\"),\n    size_test = params |&gt; pull(\"size_test\"),\n    transform_probs = params |&gt; pull(\"transform_probs\"),\n    linear_predictor = params |&gt; pull(\"linear_predictor\"),\n    seed = seed_for_repn\n  )\n  sim_data &lt;- do.call(\"simulate_data\", args)\n\n  list(\n    scenario = scenario,\n    params_df = params,\n    repn = repn,\n    data = sim_data\n  )\n\n}",
    "crumbs": [
      "III. Simulated Data",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Simulated Data</span>"
    ]
  },
  {
    "objectID": "simul-data.html#scenarios",
    "href": "simul-data.html#scenarios",
    "title": "4  Simulated Data",
    "section": "4.2 Scenarios",
    "text": "4.2 Scenarios\nLet us define the 12 first scenarios, using the code provided in Ojeda et al. (2023).\n\nDGP 1:\n\nScenario 1: basic scenario with two continuous predictors, without noise variable\nScenarios 2, 3, 4: same as 1 but with noise variables (10, 50, 100)\n\nDGP 2:\n\nScenarios 5 to 8: similar to 1 to 4 but with right-skewed true probability distribution (true probability taken to the power of 3)\n\nDGP 3:\n\nScenarios 9 to 12: similar to 1 to 4 but with ten predictors instead of two (5 numerical and 5 categorical)\n\n\nWe add four other scenarios, in which the predictor is nonlinear:\n\nDGP 4:\n\nScenarios 13 to 16: similar to 1 to 4 but with 3 covariates instead of 2 and with a nonlinear predictor which also contains an interaction term (\\(\\eta = \\alpha _1x_1 + \\alpha_2 x_2 + \\alpha_3 x_3 + \\beta x_1^2 + \\gamma x_2 \\times x_3\\)). In addition, the distribution of the true probabilities of the observed data follows a Beta(2,2) distribution.\n\n\nThe desired number of replications for each scenario needs to be set:\n\nrepns_vector &lt;- 1:100\n\nWe set the different parameters for each scenario.\n\n# Coefficients beta\ncoefficients &lt;- list(\n  # First category (baseline, 2 covariates)\n  c(0.5, 1),  # scenario 1, 0 noise variable\n  c(0.5, 1),  # scenario 2, 10 noise variables\n  c(0.5, 1),  # scenario 3, 50 noise variables\n  c(0.5, 1),  # scenario 4, 100 noise variables\n  # Second category (same as baseline, with lower number of 1s)\n  c(0.5, 1),  # scenario 5, 0 noise variable\n  c(0.5, 1),  # scenario 6, 10 noise variables\n  c(0.5, 1),  # scenario 7, 50 noise variables\n  c(0.5, 1),  # scenario 8, 100 noise variables\n  # Third category (same as baseline but with 5 num. and 5 categ. covariates)\n  c(0.1, 0.2, 0.3, 0.4, 0.5, 0.01, 0.02, 0.03, 0.04, 0.05),\n  c(0.1, 0.2, 0.3, 0.4, 0.5, 0.01, 0.02, 0.03, 0.04, 0.05),\n  c(0.1, 0.2, 0.3, 0.4, 0.5, 0.01, 0.02, 0.03, 0.04, 0.05),\n  c(0.1, 0.2, 0.3, 0.4, 0.5, 0.01, 0.02, 0.03, 0.04, 0.05),\n  # Fourth category (nonlinear predictor, 3 covariates)\n  c(0.5, 1, .3),  # scenario 5, 0 noise variable\n  c(0.5, 1, .3),  # scenario 6, 10 noise variables\n  c(0.5, 1, .3),  # scenario 7, 50 noise variables\n  c(0.5, 1, .3)  # scenario 8, 100 noise variables\n)\n\n# Mean parameter for the normal distribution to draw from to draw num covariates\nmean_num &lt;- list(\n  # First category (baseline, 2 covariates)\n  rep(0, 2),  # scenario 1, 0 noise variable\n  rep(0, 2),  # scenario 2, 10 noise variables\n  rep(0, 2),  # scenario 3, 50 noise variables\n  rep(0, 2),  # scenario 4, 100 noise variables\n  # Second category (same as baseline, with lower number of 1s)\n  rep(0, 2),  # scenario 5, 0 noise variable\n  rep(0, 2),  # scenario 6, 10 noise variables\n  rep(0, 2),  # scenario 7, 50 noise variables\n  rep(0, 2),  # scenario 8, 100 noise variables\n  # Third category (same as baseline but with 5 num. and 5 categ. covariates)\n  rep(0, 5),\n  rep(0, 5),\n  rep(0, 5),\n  rep(0, 5),\n  # Fourth category (nonlinear predictor, 3 covariates)\n  rep(0, 3),\n  rep(0, 3),\n  rep(0, 3),\n  rep(0, 3)\n)\n# Sd parameter for the normal distribution to draw from to draw num covariates\nsd_num &lt;- list(\n  # First category (baseline, 2 covariates)\n  rep(1, 2),  # scenario 1, 0 noise variable\n  rep(1, 2),  # scenario 2, 10 noise variables\n  rep(1, 2),  # scenario 3, 50 noise variables\n  rep(1, 2),  # scenario 4, 100 noise variables\n  # Second category (same as baseline, with lower number of 1s)\n  rep(1, 2),  # scenario 5, 0 noise variable\n  rep(1, 2),  # scenario 6, 10 noise variables\n  rep(1, 2),  # scenario 7, 50 noise variables\n  rep(1, 2),  # scenario 8, 100 noise variables\n  # Third category (same as baseline but with 5 num. and 5 categ. covariates)\n  rep(1, 5),\n  rep(1, 5),\n  rep(1, 5),\n  rep(1, 5),\n  # Fourth category (nonlinear predictor, 3 covariates)\n  rep(1, 3),\n  rep(1, 3),\n  rep(1, 3),\n  rep(1, 3)\n)\n\nparams_df &lt;- tibble(\n  scenario = 1:16,\n  coefficients = coefficients,\n  n_num = c(rep(2, 8), rep(5, 4), rep(3, 4)),\n  add_categ = c(rep(FALSE, 8), rep(TRUE, 4), rep(FALSE, 4)),\n  n_noise = rep(c(0, 10, 50, 100), 4),\n  mean_num = mean_num,\n  sd_num = sd_num,\n  size_train = rep(10000, 16),\n  size_valid = rep(10000, 16),\n  size_calib = rep(10000, 16),\n  size_test = rep(10000, 16),\n  transform_probs = c(rep(FALSE, 4), rep(TRUE, 4), rep(FALSE, 4), rep(FALSE, 4)),\n  linear_predictor = c(rep(TRUE, 12), rep(FALSE, 4)),\n  seed = 202105\n)\nrm(coefficients, mean_num, sd_num)",
    "crumbs": [
      "III. Simulated Data",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Simulated Data</span>"
    ]
  },
  {
    "objectID": "simul-data.html#example",
    "href": "simul-data.html#example",
    "title": "4  Simulated Data",
    "section": "4.3 Example",
    "text": "4.3 Example\nLet us draw a sample of 10,000 observations in each set (train, calibration, validation, test), for each scenario. We can then plot the histogram of the true probabilities in each sample (Figure 4.1).\n\n\nCode\npar(mfrow = c(16, 4), mar = c(2, 2, 2, 2))\nfor (scenario in 1:16) {\n  simu_data &lt;- simulate_data_wrapper(\n    scenario = scenario,\n    params_df = params_df,\n    repn = repns_vector[1] # only one replication here\n  )\n  colour_samples &lt;- c(\n    \"Train\" = \"#0072B2\",\n    \"Calib\" = \"#CC79A7\",\n    \"Valid\" = \"#009E73\",\n    \"Test\" = \"#D55E00\"\n  )\n  for (sample in c(\"train\", \"valid\", \"calib\", \"test\")) {\n    if (sample == \"train\") {\n      true_prob &lt;- simu_data$data$probs_train\n      i_scores &lt;- 1\n    } else if (sample == \"valid\") {\n      true_prob &lt;- simu_data$data$probs_valid\n      i_scores &lt;- 2\n    } else if (sample == \"calib\") {\n      true_prob &lt;- simu_data$data$probs_calib\n      i_scores &lt;- 3\n    } else {\n      true_prob &lt;- simu_data$data$probs_test\n      i_scores &lt;- 4\n    }\n    hist(\n      true_prob,\n      breaks = seq(0, 1, by = .05),\n      col = colour_samples[i_scores],\n      border = \"white\",\n      xlab = \"p\", ylab = \"\",\n      main = str_c(\n        \"Scen. \", scenario, \", \", c(\"Train\", \"Valid\", \"Calib\", \"Test\")[i_scores]\n      ),\n      xlim = c(0, 1)\n    )\n  }\n}\n\n\n\n\n\nFigure 4.1: Histogram of true probabilities for each scenario\n\n\n\n\n\n\n\n\nFor each group of scenarios, the only thing that varies is the number of noise variables. This has no impact on the distribution of the true probability. Hence, we can create a simple figure with the distribution of the true probability for each group of scenario.\n\n\nOnly on Train test, for each category of scenarios.\nsave_graph &lt;- FALSE\n\nif (save_graph) {\n  cairo_pdf(\n    \"../figs/sim-data-hist-categories.pdf\", \n    width = 8, height = 2\n  )\n}\n\npar(mfrow = c(1, 4), mar = c(4.1, 3.1, 2.1, 1.1))\nfor (i_dgp in 1:4) {\n  scenario &lt;- c(1, 5, 9, 13)[i_dgp]\n  simu_data &lt;- simulate_data_wrapper(\n    scenario = scenario,\n    params_df = params_df,\n    repn = repns_vector[1] # only one replication here\n  )\n  \n  true_prob &lt;- simu_data$data$probs_test\n  title &lt;- str_c(\"DGP \", i_dgp)\n  \n  hist(\n    true_prob,\n    breaks = seq(0, 1, by = .05),\n    # col = ,\n    # border = \"white\",\n    xlab = \"p\", ylab = \"\",\n    main = title,\n    xlim = c(0, 1)\n  )\n}\n\n\nWarning in ks.test.default(probs_01, fun): ties should not be present for the\none-sample Kolmogorov-Smirnov test\n\n\nOnly on Train test, for each category of scenarios.\nif (save_graph) dev.off()\n\n\n\n\n\nFigure 4.2: Distribution of the underlying probabilities in the different categories of scenarios.\n\n\n\n\n\n\n\n\n\n\n\n\nOjeda, Francisco M., Max L. Jansen, Alexandre Thiéry, Stefan Blankenberg, Christian Weimar, Matthias Schmid, and Andreas Ziegler. 2023. “Calibrating Machine Learning Approaches for Probability Estimation: A Comprehensive Comparison.” Statistics in Medicine 42 (29): 5451–78. https://doi.org/10.1002/sim.9921.",
    "crumbs": [
      "III. Simulated Data",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Simulated Data</span>"
    ]
  },
  {
    "objectID": "simul-xgb.html",
    "href": "simul-xgb.html",
    "title": "5  Extreme Gradient Boosting",
    "section": "",
    "text": "5.1 Data\nWe generate data using the first 12 scenarios from Ojeda et al. (2023) and an additional set of 4 scenarios in which the true probability does not depend on the predictors in a linear way (see Chapter 4).\nsource(\"../scripts/functions/simul-data.R\")\nlibrary(ks)\nsource(\"../scripts/functions/subsample_target_distribution.R\")\nWhen we simulate a dataset, we draw the following number of observations:\nnb_obs &lt;- 10000\nDefinition of the 16 scenarios\n# Coefficients beta\ncoefficients &lt;- list(\n  # First category (baseline, 2 covariates)\n  c(0.5, 1),  # scenario 1, 0 noise variable\n  c(0.5, 1),  # scenario 2, 10 noise variables\n  c(0.5, 1),  # scenario 3, 50 noise variables\n  c(0.5, 1),  # scenario 4, 100 noise variables\n  # Second category (same as baseline, with lower number of 1s)\n  c(0.5, 1),  # scenario 5, 0 noise variable\n  c(0.5, 1),  # scenario 6, 10 noise variables\n  c(0.5, 1),  # scenario 7, 50 noise variables\n  c(0.5, 1),  # scenario 8, 100 noise variables\n  # Third category (same as baseline but with 5 num. and 5 categ. covariates)\n  c(0.1, 0.2, 0.3, 0.4, 0.5, 0.01, 0.02, 0.03, 0.04, 0.05),\n  c(0.1, 0.2, 0.3, 0.4, 0.5, 0.01, 0.02, 0.03, 0.04, 0.05),\n  c(0.1, 0.2, 0.3, 0.4, 0.5, 0.01, 0.02, 0.03, 0.04, 0.05),\n  c(0.1, 0.2, 0.3, 0.4, 0.5, 0.01, 0.02, 0.03, 0.04, 0.05),\n  # Fourth category (nonlinear predictor, 3 covariates)\n  c(0.5, 1, .3),  # scenario 5, 0 noise variable\n  c(0.5, 1, .3),  # scenario 6, 10 noise variables\n  c(0.5, 1, .3),  # scenario 7, 50 noise variables\n  c(0.5, 1, .3)  # scenario 8, 100 noise variables\n)\n\n# Mean parameter for the normal distribution to draw from to draw num covariates\nmean_num &lt;- list(\n  # First category (baseline, 2 covariates)\n  rep(0, 2),  # scenario 1, 0 noise variable\n  rep(0, 2),  # scenario 2, 10 noise variables\n  rep(0, 2),  # scenario 3, 50 noise variables\n  rep(0, 2),  # scenario 4, 100 noise variables\n  # Second category (same as baseline, with lower number of 1s)\n  rep(0, 2),  # scenario 5, 0 noise variable\n  rep(0, 2),  # scenario 6, 10 noise variables\n  rep(0, 2),  # scenario 7, 50 noise variables\n  rep(0, 2),  # scenario 8, 100 noise variables\n  # Third category (same as baseline but with 5 num. and 5 categ. covariates)\n  rep(0, 5),\n  rep(0, 5),\n  rep(0, 5),\n  rep(0, 5),\n  # Fourth category (nonlinear predictor, 3 covariates)\n  rep(0, 3),\n  rep(0, 3),\n  rep(0, 3),\n  rep(0, 3)\n)\n# Sd parameter for the normal distribution to draw from to draw num covariates\nsd_num &lt;- list(\n  # First category (baseline, 2 covariates)\n  rep(1, 2),  # scenario 1, 0 noise variable\n  rep(1, 2),  # scenario 2, 10 noise variables\n  rep(1, 2),  # scenario 3, 50 noise variables\n  rep(1, 2),  # scenario 4, 100 noise variables\n  # Second category (same as baseline, with lower number of 1s)\n  rep(1, 2),  # scenario 5, 0 noise variable\n  rep(1, 2),  # scenario 6, 10 noise variables\n  rep(1, 2),  # scenario 7, 50 noise variables\n  rep(1, 2),  # scenario 8, 100 noise variables\n  # Third category (same as baseline but with 5 num. and 5 categ. covariates)\n  rep(1, 5),\n  rep(1, 5),\n  rep(1, 5),\n  rep(1, 5),\n  # Fourth category (nonlinear predictor, 3 covariates)\n  rep(1, 3),\n  rep(1, 3),\n  rep(1, 3),\n  rep(1, 3)\n)\n\nparams_df &lt;- tibble(\n  scenario = 1:16,\n  coefficients = coefficients,\n  n_num = c(rep(2, 8), rep(5, 4), rep(3, 4)),\n  add_categ = c(rep(FALSE, 8), rep(TRUE, 4), rep(FALSE, 4)),\n  n_noise = rep(c(0, 10, 50, 100), 4),\n  mean_num = mean_num,\n  sd_num = sd_num,\n  size_train = rep(nb_obs, 16),\n  size_valid = rep(nb_obs, 16),\n  size_calib = rep(nb_obs, 16),\n  size_test = rep(nb_obs, 16),\n  transform_probs = c(rep(FALSE, 4), rep(TRUE, 4), rep(FALSE, 4), rep(FALSE, 4)),\n  linear_predictor = c(rep(TRUE, 12), rep(FALSE, 4)),\n  seed = 202105\n)\nrm(coefficients, mean_num, sd_num)",
    "crumbs": [
      "III. Simulated Data",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Extreme Gradient Boosting</span>"
    ]
  },
  {
    "objectID": "simul-xgb.html#metrics",
    "href": "simul-xgb.html#metrics",
    "title": "5  Extreme Gradient Boosting",
    "section": "5.2 Metrics",
    "text": "5.2 Metrics\nWe load the functions from Chapter 3 to compute performance, calibration and divergence metrics.\n\nsource(\"../scripts/functions/metrics.R\")",
    "crumbs": [
      "III. Simulated Data",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Extreme Gradient Boosting</span>"
    ]
  },
  {
    "objectID": "simul-xgb.html#simulations-setup",
    "href": "simul-xgb.html#simulations-setup",
    "title": "5  Extreme Gradient Boosting",
    "section": "5.3 Simulations Setup",
    "text": "5.3 Simulations Setup\nTo train the models, we rely on the {xgboost} R package.\n\nlibrary(xgboost)\n\n\nAttaching package: 'xgboost'\n\n\nThe following object is masked from 'package:dplyr':\n\n    slice\n\n\nHere, we define a function to recalibrate predicted scores using either Platt scaling or isotonic regression. The recalibration algorithm is first trained on the calibration set and then applied to both the calibration and test sets.\n\n#' Recalibrates scores using a calibration\n#' \n#' @param obs_calib vector of observed events in the calibration set\n#' @param scores_calib vector of predicted probabilities in the calibration set\n#' @param obs_test vector of observed events in the test set\n#' @param scores_test vector of predicted probabilities in the test set\n#' @param method recalibration method (`\"platt\"` for Platt scaling, \n#'   `\"isotonic\"` for isotonic regression)\n#' @returns list of two elements: recalibrated scores on the calibration set,\n#'   recalibrated scores on the test set\nrecalibrate &lt;- function(obs_calib,\n                        obs_test,\n                        pred_calib,\n                        pred_test,\n                        method = c(\"platt\", \"isotonic\")) {\n  data_calib &lt;- tibble(d = obs_calib, scores = pred_calib)\n  data_test &lt;- tibble(d = obs_test, scores = pred_test)\n  \n  if (method == \"platt\") {\n    lr &lt;- glm(d ~ scores, family = binomial(link = 'logit'), data = data_calib)\n    score_c_calib &lt;- predict(lr, newdata = data_calib, type = \"response\")\n    score_c_test &lt;- predict(lr, newdata = data_test, type = \"response\")\n  } else if (method == \"isotonic\") {\n    iso &lt;- isoreg(x = data_calib$scores, y = data_calib$d)\n    fit_iso &lt;- as.stepfun(iso)\n    score_c_calib &lt;- fit_iso(data_calib$scores)\n    score_c_test &lt;- fit_iso(data_test$scores)\n    \n  } else {\n    stop(\"Unrecognized method: platt or isotonic only\")\n  }\n  # Format results in tibbles:\n  # For calibration set\n  tb_score_c_calib &lt;- tibble(\n    d = obs_calib,\n    p_u = pred_calib,\n    p_c = score_c_calib\n  )\n  # For test set\n  tb_score_c_test &lt;- tibble(\n    d = obs_test,\n    p_u = pred_test,\n    p_c = score_c_test\n  )\n  \n  list(\n    tb_score_c_calib = tb_score_c_calib,\n    tb_score_c_test = tb_score_c_test\n  )\n  \n}\n\nAs explained in the foreword of this page, we compute metrics based on scores obtained at various boosting iterations. To do so, we define a function, get_metrics_nb_iter(), that will be applied to a fitted model. This function will be called for all the boosting iterations (controlled by the nb_iter argument). The function returns a list with the following elements:\n\nscenario: the ID of the scenario\nind: the index of the grid search (so that we can join with the hyperparameters values, if needed)\nrepn: the ID of the replication\nnb_iter: the boosting iteration at which the metrics are computed\ntb_metrics: the tibble with the performance, calibration, and divergence metrics (one row for the train sample, one row for the calibration sample, one row for the validation sample, and one row for the test sample)\ntb_prop_scores: additional metrics (\\(\\mathbb{P}(q_1 &lt; \\hat{s}(\\mathbf{x}) &lt; q_2)\\) for multiple values for \\(q_1\\) and \\(q_2 = 1-q_1\\))\nscores_hist: elements to be able to plot an histogram of the scores on both the train set and the test set (using 20 equally-sized bins over \\([0,1]\\)).\n\n\n\nFunction get_metrics_nb_iter()\n#' Computes the performance and calibration metrics for an xgb model,\n#' depending on the number of iterations kept.\n#'\n#' @param nb_iter number of boosting iterations to keep\n#' @param params hyperparameters of the current model\n#' @param fitted_xgb xgb estimated model\n#' @param tb_train_xgb train data (in xgb.DMatrix format)\n#' @param tb_valid_xgb validation data (in xgb.DMatrix format)\n#' @param tb_calib_xgb calibration data (in xgb.DMatrix format)\n#' @param tb_test_xgb test data (in xgb.DMatrix format)\n#' @param simu_data simulated dataset\n#' @param true_prob list with true probabilities on train, calibration,\n#'  validation and test sets\nget_metrics_nb_iter &lt;- function(nb_iter,\n                                params,\n                                fitted_xgb,\n                                tb_train_xgb,\n                                tb_valid_xgb,\n                                tb_calib_xgb,\n                                tb_test_xgb,\n                                simu_data,\n                                true_prob) {\n\n  ind &lt;- params$ind\n  max_depth &lt;- params$max_depth\n  tb_train &lt;- simu_data$data$train |&gt; rename(d = y)\n  tb_valid &lt;- simu_data$data$valid |&gt; rename(d = y)\n  tb_calib &lt;- simu_data$data$calib |&gt; rename(d = y)\n  tb_test &lt;- simu_data$data$test |&gt; rename(d = y)\n\n  # Predicted scores\n  scores_train &lt;- predict(fitted_xgb, tb_train_xgb, iterationrange = c(1, nb_iter))\n  scores_valid &lt;- predict(fitted_xgb, tb_valid_xgb, iterationrange = c(1, nb_iter))\n  scores_calib &lt;- predict(fitted_xgb, tb_calib_xgb, iterationrange = c(1, nb_iter))\n  scores_test &lt;- predict(fitted_xgb, tb_test_xgb, iterationrange = c(1, nb_iter))\n  \n  # Recalibration\n  # Platt scaling\n  res_recalibration_platt &lt;- recalibrate(\n    obs_calib = tb_calib$d, \n    obs_test = tb_test$d, \n    pred_calib = scores_calib, \n    pred_test = scores_test, \n    method = \"platt\"\n  )\n  scores_c_platt_calib &lt;- res_recalibration_platt$tb_score_c_calib$p_c\n  scores_c_platt_test &lt;- res_recalibration_platt$tb_score_c_test$p_c\n  \n  # Isotonic regression\n  res_recalibration_iso &lt;- recalibrate(\n    obs_calib = tb_calib$d, \n    obs_test = tb_test$d, \n    pred_calib = scores_calib, \n    pred_test = scores_test, \n    method = \"isotonic\"\n  )\n  scores_c_iso_calib &lt;- res_recalibration_iso$tb_score_c_calib$p_c\n  scores_c_iso_test &lt;- res_recalibration_iso$tb_score_c_test$p_c\n\n  ## Histogram of scores----\n  breaks &lt;- seq(0, 1, by = .05)\n  scores_train_hist &lt;- hist(scores_train, breaks = breaks, plot = FALSE)\n  scores_calib_hist &lt;- hist(scores_calib, breaks = breaks, plot = FALSE)\n  scores_valid_hist &lt;- hist(scores_valid, breaks = breaks, plot = FALSE)\n  scores_test_hist &lt;- hist(scores_test, breaks = breaks, plot = FALSE)\n  scores_c_platt_calib_hist &lt;- hist(scores_c_platt_calib, breaks = breaks, plot = FALSE)\n  scores_c_platt_test_hist &lt;- hist(scores_c_platt_test, breaks = breaks, plot = FALSE)\n  scores_c_iso_calib_hist &lt;- hist(scores_c_iso_calib, breaks = breaks, plot = FALSE)\n  scores_c_iso_test_hist &lt;- hist(scores_c_iso_test, breaks = breaks, plot = FALSE)\n  \n  scores_hist &lt;- list(\n    train = scores_train_hist,\n    valid = scores_valid_hist,\n    calib = scores_calib_hist,\n    test = scores_test_hist,\n    calib_c_platt = scores_c_platt_calib_hist,\n    test_c_platt = scores_c_platt_test_hist,\n    calib_c_iso = scores_c_iso_calib_hist,\n    test_c_iso = scores_c_iso_test_hist,\n    scenario = simu_data$scenario,\n    ind = ind,\n    repn = simu_data$repn,\n    max_depth = params$max_depth,\n    nb_iter = nb_iter\n  )\n\n  ## Estimation of P(q1 &lt; score &lt; q2)----\n  prop_btw_q_h &lt;- function(s, sample_name, recalib_name) {\n    map(\n      c(.1, .2, .3, .4),\n      ~prop_btw_quantiles(s = s, q1 = .x)\n    ) |&gt;\n      list_rbind() |&gt;\n      mutate(sample = sample_name, recalib = recalib_name)\n  }\n  \n  proq_scores_train &lt;- prop_btw_q_h(\n    scores_train, sample_name = \"train\", recalib_name = \"none\"\n  )\n  proq_scores_valid &lt;- prop_btw_q_h(\n    scores_valid, sample_name = \"valid\", recalib_name = \"none\"\n  )\n  proq_scores_calib &lt;- prop_btw_q_h(\n    scores_calib, sample_name = \"calib\", recalib_name = \"none\"\n  )\n  proq_scores_test &lt;- prop_btw_q_h(\n    scores_test, sample_name = \"test\", recalib_name = \"none\"\n  )\n  proq_scores_c_platt_calib &lt;- prop_btw_q_h(\n    scores_c_platt_calib, sample_name = \"calib\", recalib_name = \"platt\"\n  )\n  proq_scores_c_platt_test &lt;- prop_btw_q_h(\n    scores_c_platt_test, sample_name = \"test\", recalib_name = \"platt\"\n  )\n  proq_scores_c_iso_calib &lt;- prop_btw_q_h(\n    scores_c_iso_calib, sample_name = \"calib\", recalib_name = \"isotonic\"\n  )\n  proq_scores_c_iso_test &lt;- prop_btw_q_h(\n    scores_c_iso_test, sample_name = \"test\", recalib_name = \"isotonic\"\n  )\n  \n\n  ## Dispersion Metrics----\n  disp_train &lt;- dispersion_metrics(\n    true_probas = true_prob$train, scores = scores_train\n  ) |&gt; \n    mutate(sample = \"train\", recalib = \"none\")\n  disp_valid &lt;- dispersion_metrics(\n    true_probas = true_prob$valid, scores = scores_valid\n  ) |&gt;\n    mutate(sample = \"valid\", recalib = \"none\")\n  \n  disp_calib &lt;- dispersion_metrics(\n    true_probas = true_prob$calib, scores = scores_calib\n  ) |&gt;\n    mutate(sample = \"calib\", recalib = \"none\")\n  \n  disp_test &lt;- dispersion_metrics(\n    true_probas = true_prob$test, scores = scores_test\n  ) |&gt; \n    mutate(sample = \"test\", recalib = \"none\")\n  \n  \n  disp_c_platt_calib &lt;- dispersion_metrics(\n    true_probas = true_prob$calib, scores = scores_c_platt_calib\n  ) |&gt;\n    mutate(sample = \"calib\", recalib = \"platt\")\n  \n  disp_c_platt_test &lt;- dispersion_metrics(\n    true_probas = true_prob$test, scores = scores_c_platt_test\n  ) |&gt; \n    mutate(sample = \"test\", recalib = \"platt\")\n  \n  disp_c_iso_calib &lt;- dispersion_metrics(\n    true_probas = true_prob$calib, scores = scores_c_iso_calib\n  ) |&gt;\n    mutate(sample = \"calib\", recalib = \"isotonic\")\n  \n  disp_c_iso_test &lt;- dispersion_metrics(\n    true_probas = true_prob$test, scores = scores_c_iso_test\n  ) |&gt; \n    mutate(sample = \"test\", recalib = \"isotonic\")\n  \n  # Performance and Calibration Metrics\n  # We add very small noise to predicted scores\n  # otherwise the local regression may crash\n  scores_train_noise &lt;- scores_train +\n    runif(n = length(scores_train), min = 0, max = 0.01)\n  scores_train_noise[scores_train_noise &gt; 1] &lt;- 1\n  metrics_train &lt;- compute_metrics(\n    obs = tb_train$d, scores = scores_train_noise, true_probas = true_prob$train\n  ) |&gt; mutate(sample = \"train\", recalib = \"none\")\n  \n  scores_valid_noise &lt;- scores_valid +\n    runif(n = length(scores_valid), min = 0, max = 0.01)\n  scores_valid_noise[scores_valid_noise &gt; 1] &lt;- 1\n  metrics_valid &lt;- compute_metrics(\n    obs = tb_valid$d, scores = scores_valid_noise, true_probas = true_prob$valid\n  ) |&gt; mutate(sample = \"valid\", recalib = \"none\")\n  \n  scores_calib_noise &lt;- scores_calib +\n    runif(n = length(scores_calib), min = 0, max = 0.01)\n  scores_calib_noise[scores_calib_noise &gt; 1] &lt;- 1\n  metrics_calib &lt;- compute_metrics(\n    obs = tb_calib$d, scores = scores_calib_noise, true_probas = true_prob$calib\n  ) |&gt; mutate(sample = \"calib\", recalib = \"none\")\n  \n  scores_test_noise &lt;- scores_test +\n    runif(n = length(scores_test), min = 0, max = 0.01)\n  scores_test_noise[scores_test_noise &gt; 1] &lt;- 1\n  metrics_test &lt;- compute_metrics(\n    obs = tb_test$d, scores = scores_test_noise, true_probas = true_prob$test\n  ) |&gt; mutate(sample = \"test\", recalib = \"none\")\n  \n  # With recalibrated scores (platt)\n  scores_c_platt_calib_noise &lt;- scores_c_platt_calib +\n    runif(n = length(scores_c_platt_calib), min = 0, max = 0.01)\n  scores_c_platt_calib_noise[scores_c_platt_calib_noise &gt; 1] &lt;- 1\n  metrics_c_platt_calib &lt;- compute_metrics(\n    obs = tb_calib$d, scores = scores_c_platt_calib_noise, \n    true_probas = true_prob$calib\n  ) |&gt; mutate(sample = \"calib\", recalib = \"platt\")\n  \n  scores_c_platt_test_noise &lt;- scores_c_platt_test +\n    runif(n = length(scores_c_platt_test), min = 0, max = 0.01)\n  scores_c_platt_test_noise[scores_c_platt_test_noise &gt; 1] &lt;- 1\n  metrics_c_platt_test &lt;- compute_metrics(\n    obs = tb_test$d, scores = scores_c_platt_test_noise, \n    true_probas = true_prob$test\n  ) |&gt; mutate(sample = \"test\", recalib = \"platt\")\n  \n  # With recalibrated scores (isotonic)\n  scores_c_iso_calib_noise &lt;- scores_c_iso_calib +\n    runif(n = length(scores_c_iso_calib), min = 0, max = 0.01)\n  scores_c_iso_calib_noise[scores_c_iso_calib_noise &gt; 1] &lt;- 1\n  metrics_c_iso_calib &lt;- compute_metrics(\n    obs = tb_calib$d, scores = scores_c_iso_calib_noise, \n    true_probas = true_prob$calib\n  ) |&gt; mutate(sample = \"calib\", recalib = \"isotonic\")\n  \n  scores_c_iso_test_noise &lt;- scores_c_iso_test +\n    runif(n = length(scores_c_iso_test), min = 0, max = 0.01)\n  scores_c_iso_test_noise[scores_c_iso_test_noise &gt; 1] &lt;- 1\n  metrics_c_iso_test &lt;- compute_metrics(\n    obs = tb_test$d, scores = scores_c_iso_test_noise, \n    true_probas = true_prob$test\n  ) |&gt; mutate(sample = \"test\", recalib = \"isotonic\")\n  \n  tb_metrics &lt;- metrics_train |&gt;\n    bind_rows(metrics_valid) |&gt;\n    bind_rows(metrics_calib) |&gt;\n    bind_rows(metrics_test) |&gt;\n    bind_rows(metrics_c_platt_calib) |&gt;\n    bind_rows(metrics_c_platt_test) |&gt;\n    bind_rows(metrics_c_iso_calib) |&gt;\n    bind_rows(metrics_c_iso_test) |&gt;\n    left_join(\n      disp_train |&gt;\n        bind_rows(disp_valid) |&gt; \n        bind_rows(disp_calib) |&gt; \n        bind_rows(disp_test) |&gt; \n        bind_rows(disp_c_platt_calib) |&gt; \n        bind_rows(disp_c_platt_test) |&gt; \n        bind_rows(disp_c_iso_calib) |&gt; \n        bind_rows(disp_c_iso_test),\n      by = c(\"sample\", \"recalib\")\n    ) |&gt;\n    mutate(\n      scenario = simu_data$scenario,\n      ind = ind,\n      repn = simu_data$repn,\n      max_depth = params$max_depth,\n      nb_iter = nb_iter\n    )\n  \n  tb_prop_scores &lt;- proq_scores_train |&gt;\n    bind_rows(proq_scores_valid) |&gt;\n    bind_rows(proq_scores_calib) |&gt;\n    bind_rows(proq_scores_test) |&gt;\n    bind_rows(proq_scores_c_platt_calib) |&gt;\n    bind_rows(proq_scores_c_platt_test) |&gt;\n    bind_rows(proq_scores_c_iso_calib) |&gt;\n    bind_rows(proq_scores_c_iso_test) |&gt;\n    mutate(\n      scenario = simu_data$scenario,\n      ind = ind,\n      repn = simu_data$repn,\n      max_depth = params$max_depth,\n      nb_iter = nb_iter\n    )\n\n  list(\n    scenario = simu_data$scenario,     # data scenario\n    ind = ind,                         # index for grid\n    repn = simu_data$repn,             # data replication ID\n    nb_iter = nb_iter,                 # number of boosting iterations\n    tb_metrics = tb_metrics,           # table with performance/calib/divergence\n                                       #  metrics\n    tb_prop_scores = tb_prop_scores,   # table with P(q1 &lt; score &lt; q2)\n    scores_hist = scores_hist          # histogram of scores\n  )\n}\n\n\nWe define another function, simul_xgb() which trains an extreme gradient boosting model for a single replication. It calls the get_metrics_nb_iter() on each of the boosting iterations of the model from the second to the last (400th), and returns a list of length 400-1 where each element is a list returned by the get_metrics_nb_iter().\n\n\nFunction simul_xgb()\n#' Train an xgboost model and compute performance, calibration, and dispersion\n#' metrics\n#'\n#' @param params tibble with hyperparameters for the simulation\n#' @param ind index of the grid (numerical ID)\n#' @param simu_data simulated data obtained with `simulate_data_wrapper()`\nsimul_xgb &lt;- function(params,\n                      ind,\n                      simu_data) {\n  tb_train &lt;- simu_data$data$train |&gt; rename(d = y)\n  tb_valid &lt;- simu_data$data$valid |&gt; rename(d = y)\n  tb_calib &lt;- simu_data$data$calib |&gt; rename(d = y)\n  tb_test &lt;- simu_data$data$test |&gt; rename(d = y)\n  true_prob &lt;-\n    list(\n      train = simu_data$data$probs_train,\n      valid = simu_data$data$probs_valid,\n      calib = simu_data$data$probs_calib,\n      test = simu_data$data$probs_test\n    )\n\n  ## Format data for xgboost----\n  tb_train_xgb &lt;- xgb.DMatrix(\n    data = model.matrix(d ~ -1 + ., tb_train), label = tb_train$d\n  )\n  tb_valid_xgb &lt;- xgb.DMatrix(\n    data = model.matrix(d ~ -1 + ., tb_valid), label = tb_valid$d\n  )\n  tb_calib_xgb &lt;- xgb.DMatrix(\n    data = model.matrix(d ~ -1 + ., tb_calib), label = tb_calib$d\n  )\n  tb_test_xgb &lt;- xgb.DMatrix(\n    data = model.matrix(d ~ -1 + ., tb_test), label = tb_test$d\n  )\n  # Parameters for the algorithm\n  param &lt;- list(\n    max_depth = params$max_depth, #Note: root node is indexed 0\n    eta = params$eta,\n    nthread = 1,\n    objective = \"binary:logistic\",\n    eval_metric = \"auc\"\n  )\n  watchlist &lt;- list(train = tb_train_xgb, eval = tb_valid_xgb)\n\n  ## Estimation----\n  xgb_fit &lt;- xgb.train(\n    param, tb_train_xgb,\n    nrounds = params$nb_iter_total,\n    watchlist,\n    verbose = 0\n  )\n\n  # Then, for each boosting iteration number up to params$nb_iter_total\n  # compute the predicted scores and evaluate the metrics\n  resul &lt;- map(\n    seq(2, params$nb_iter_total),\n    ~get_metrics_nb_iter(\n      nb_iter = .x,\n      params = params,\n      fitted_xgb = xgb_fit,\n      tb_train_xgb = tb_train_xgb,\n      tb_valid_xgb = tb_valid_xgb,\n      tb_calib_xgb = tb_calib_xgb,\n      tb_test_xgb = tb_test_xgb,\n      simu_data = simu_data,\n      true_prob = true_prob\n    ),\n  )\n  resul\n}\n\nsimulate_xgb_scenario &lt;- function(scenario, params_df, repn) {\n  # Generate Data\n  simu_data &lt;- simulate_data_wrapper(\n    scenario = scenario,\n    params_df = params_df,\n    repn = repn\n  )\n\n  # Looping over the grid hyperparameters for the scenario\n  res_simul &lt;- vector(mode = \"list\", length = nrow(grid))\n  cli::cli_progress_bar(\"Iteration grid\", total = nrow(grid), type = \"tasks\")\n  for (j in 1:nrow(grid)) {\n    curent_params &lt;- grid |&gt; dplyr::slice(!!j)\n    res_simul[[j]] &lt;- simul_xgb(\n      params = curent_params,\n      ind = curent_params$ind,\n      simu_data = simu_data\n    )\n    cli::cli_progress_update()\n  }\n\n\n  # The metrics computed for all set of hyperparameters (identified with `ind`)\n  # and for each number of boosting iterations (`nb_iter`), for the current\n  # scenario (`scenario`) and current replication number (`repn`)\n  metrics_simul &lt;- map(\n    res_simul,\n    function(simul_grid_j) map(simul_grid_j, \"tb_metrics\") |&gt; list_rbind()\n  ) |&gt;\n    list_rbind()\n\n  # P(q_1&lt;s(x)&lt;q_2)\n  prop_scores_simul &lt;- map(\n    res_simul,\n    function(simul_grid_j) map(simul_grid_j, \"tb_prop_scores\") |&gt; list_rbind()\n  ) |&gt;\n    list_rbind()\n\n  # Histogram of estimated scores\n  scores_hist &lt;- map(\n    res_simul,\n    function(simul_grid_j) map(simul_grid_j, \"scores_hist\")\n  )\n\n  list(\n    metrics_simul = metrics_simul,\n    scores_hist = scores_hist,\n    prop_scores_simul = prop_scores_simul\n  )\n}\n\n\n\n5.3.1 Grid\nWe consider the following grid:\n\ngrid &lt;- expand_grid(\n  max_depth = c(2, 4, 6),\n  nb_iter_total = 400,\n  eta = 0.3\n) |&gt;\n  mutate(ind = row_number())\n\nThe desired number of replications for each scenario needs to be set:\n\nrepns_vector &lt;- 1:100\n\nThe different configurations are reported in Table 5.1.\n\nDT::datatable(grid)\n\n\n\nTable 5.1: Grid Search Values\n\n\n\n\n\n\n\n\n\n\nWe define a function, simulate_xgb_scenario() to train the model on a dataset for all different values of the hyperparameters of the grid. This function performs a single replication of the simulations for a single scenario.\n\n\nFunction simulate_xgb_scenario()\nsimulate_xgb_scenario &lt;- function(scenario, params_df, repn) {\n  # Generate Data\n  simu_data &lt;- simulate_data_wrapper(\n    scenario = scenario,\n    params_df = params_df,\n    repn = repn\n  )\n\n  # Looping over the grid hyperparameters for the scenario\n  res_simul &lt;- vector(mode = \"list\", length = nrow(grid))\n  cli::cli_progress_bar(\"Iteration grid\", total = nrow(grid), type = \"tasks\")\n  for (j in 1:nrow(grid)) {\n    curent_params &lt;- grid |&gt; dplyr::slice(!!j)\n    res_simul[[j]] &lt;- simul_xgb(\n      params = curent_params,\n      ind = curent_params$ind,\n      simu_data = simu_data\n    )\n    cli::cli_progress_update()\n  }\n\n\n  # The metrics computed for all set of hyperparameters (identified with `ind`)\n  # and for each number of boosting iterations (`nb_iter`), for the current\n  # scenario (`scenario`) and current replication number (`repn`)\n  metrics_simul &lt;- map(\n    res_simul,\n    function(simul_grid_j) map(simul_grid_j, \"tb_metrics\") |&gt; list_rbind()\n  ) |&gt;\n    list_rbind()\n\n  # Sanity check\n  # metrics_simul |&gt; count(scenario, repn, ind, sample, nb_iter) |&gt;\n  #   filter(n &gt; 1)\n\n  # P(q_1&lt;s(x)&lt;q_2)\n  prop_scores_simul &lt;- map(\n    res_simul,\n    function(simul_grid_j) map(simul_grid_j, \"tb_prop_scores\") |&gt; list_rbind()\n  ) |&gt;\n    list_rbind()\n\n  # Sanity check\n  # prop_scores_simul |&gt; count(scenario, repn, ind, sample, nb_iter)\n\n  # Histogram of estimated scores\n  scores_hist &lt;- map(\n    res_simul,\n    function(simul_grid_j) map(simul_grid_j, \"scores_hist\")\n  )\n\n  list(\n    metrics_simul = metrics_simul,\n    scores_hist = scores_hist,\n    prop_scores_simul = prop_scores_simul\n  )\n}",
    "crumbs": [
      "III. Simulated Data",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Extreme Gradient Boosting</span>"
    ]
  },
  {
    "objectID": "simul-xgb.html#estimations",
    "href": "simul-xgb.html#estimations",
    "title": "5  Extreme Gradient Boosting",
    "section": "5.4 Estimations",
    "text": "5.4 Estimations\nWe loop over the 16 scenarios and run the 100 replications in parallel.\n\n\nEstimation codes\nlibrary(pbapply)\nlibrary(parallel)\nncl &lt;- detectCores()-1\n(cl &lt;- makeCluster(ncl))\n\nclusterEvalQ(cl, {\n  library(tidyverse)\n  library(locfit)\n  library(philentropy)\n  library(xgboost)\n  library(ks)\n}) |&gt;\n  invisible()\n\nclusterExport(\n  cl, c(\n    # Functions\n    \"brier_score\",\n    \"compute_metrics\",\n    \"dispersion_metrics\",\n    \"prop_btw_quantiles\",\n    \"subset_target\",\n    \"simulate_data\",\n    \"simulate_data_wrapper\",\n    \"simul_xgb\",\n    \"simulate_xgb_scenario\",\n    \"get_metrics_nb_iter\",\n    \"recalibrate\",\n    # Objects\n    \"grid\",\n    \"params_df\",\n    \"repns_vector\"\n  )\n)\n\n# make directory if not existing\nif (!dir.exists(\"../output/simul/\")) {\n  dir.create(\"../output/simul/\", recursive = TRUE)\n}\n\nfor (i_scenario in 13:16) {\n  scenario &lt;- i_scenario\n  print(str_c(\"Scenario \", scenario, \"/\", nrow(params_df)))\n  clusterExport(cl, c(\"scenario\"))\n  resul_xgb_scenario &lt;-\n    pblapply(\n      1:length(repns_vector), function(i) simulate_xgb_scenario(\n        scenario = scenario, params_df = params_df, repn = repns_vector[i]\n      ),\n      cl = cl\n    )\n  save(\n    resul_xgb_scenario,\n    file = str_c(\"../output/simul/resul_xgb_scenario_\", scenario, \".rda\")\n  )\n}\nstopCluster(cl)\n\n\nThe results can be loaded as follows:\n\nscenarios &lt;- 1:16\nfiles &lt;- str_c(\n  \"../output/simul/resul_xgb_scenario_\", scenarios, \".rda\"\n)\nresul_xgb &lt;- map(files[file.exists(files)], ~{load(.x) ; resul_xgb_scenario})\n\nThe resul_rf object is of length 16: each element contains the simulations for a scenario. For each scenario, the elements are a list of length max(repns_vector), i.e., the number of replications. Each replication gives, in a list, the following elements:\n\nmetrics_simul: the metrics (AUC, Calibration, KL Divergence, etc.) for each model from the grid search, for all boosting iterations\nscores_hist: the counts on bins defined on estimated scores (on train, validation, calibration, and test sets ; for calibration and test sets, the counts are given with or without recalibration)\nprop_scores_simul: the estimations of \\(\\mathbb{P}(q_1 &lt; \\hat{\\mathbf{x}}&lt; q_2)\\) for various values of q_1 and q_2.",
    "crumbs": [
      "III. Simulated Data",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Extreme Gradient Boosting</span>"
    ]
  },
  {
    "objectID": "simul-xgb.html#results",
    "href": "simul-xgb.html#results",
    "title": "5  Extreme Gradient Boosting",
    "section": "5.5 Results",
    "text": "5.5 Results\nWe can now extract some information from the results.\nWe first aggregate all the computed metrics performance/calibration/divergence in a single tibble, metrics_xgb_all.\n\n\nCodes to create the metrics table\nmetrics_xgb_all &lt;- map(\n  resul_xgb,\n  function(resul_xgb_sc) map(resul_xgb_sc, \"metrics_simul\") |&gt; list_rbind()\n) |&gt;\n  list_rbind() |&gt;\n  mutate(\n    sample = factor(\n      sample,\n      levels = c(\"train\", \"valid\", \"calib\", \"test\"),\n      labels = c(\"Train\",\"Validation\", \"Calibration\" ,\"Test\")\n    ),\n    recalib = factor(\n      recalib,\n      levels = c(\"none\", \"platt\", \"isotonic\"),\n      labels = c(\"None\", \"Platt\", \"Isotonic\")\n    )\n  )\n\n# Sanity check\n# metrics_xgb_all |&gt; count(scenario, ind, sample, nb_iter) |&gt;\n#   filter(n != max(repns_vector))\n\n\nFor each replication, we made some hyperparameters vary. Let us identify some models of interest:\n\nsmallest: model with the lowest number of boosting iteration\nlargest: model with the highest number of boosting iteration\nlargest_auc: model with the highest AUC on validation set\nlowest_mse: model with the lowest MSE on validation set\nlowest_ici: model with the lowest ICI on validation set\nlowest_kl: model with the lowest KL Divergence on validation set\n\n\n\nCode\n# Identify the smallest tree on the validation set, when the scores are not\n# recalibrated\nsmallest_xgb &lt;-\n  metrics_xgb_all |&gt;\n  filter(sample == \"Validation\", recalib == \"None\") |&gt;\n  group_by(scenario, repn) |&gt;\n  arrange(nb_iter) |&gt;\n  slice_head(n = 1) |&gt;\n  select(scenario, repn, ind, nb_iter, recalib) |&gt;\n  mutate(result_type = \"smallest\") |&gt;\n  ungroup()\n\n# Identify the largest tree\nlargest_xgb &lt;-\n  metrics_xgb_all |&gt;\n  filter(sample == \"Validation\", recalib == \"None\") |&gt;\n  group_by(scenario, repn) |&gt;\n  arrange(desc(nb_iter)) |&gt;\n  slice_head(n = 1) |&gt;\n  select(scenario, repn, ind, nb_iter, recalib) |&gt;\n  mutate(result_type = \"largest\") |&gt;\n  ungroup()\n\n# Identify tree with highest AUC on test set\nhighest_auc_xgb &lt;-\n  metrics_xgb_all |&gt;\n  filter(sample == \"Validation\", recalib == \"None\") |&gt;\n  group_by(scenario, repn) |&gt;\n  arrange(desc(AUC)) |&gt;\n  slice_head(n = 1) |&gt;\n  select(scenario, repn, ind, nb_iter, recalib) |&gt;\n  mutate(result_type = \"largest_auc\") |&gt;\n  ungroup()\n\n# Identify tree with lowest MSE\nlowest_mse_xgb &lt;-\n  metrics_xgb_all |&gt;\n  filter(sample == \"Validation\", recalib == \"None\") |&gt;\n  group_by(scenario, repn) |&gt;\n  arrange(mse) |&gt;\n  slice_head(n = 1) |&gt;\n  select(scenario, repn, ind, nb_iter, recalib) |&gt;\n  mutate(result_type = \"lowest_mse\") |&gt;\n  ungroup()\n\n# Identify tree with lowest brier\nlowest_brier_xgb &lt;-\n  metrics_xgb_all |&gt;\n  filter(sample == \"Validation\", recalib == \"None\") |&gt;\n  group_by(scenario, repn) |&gt;\n  arrange(brier) |&gt;\n  slice_head(n = 1) |&gt;\n  select(scenario, repn, ind, nb_iter, recalib) |&gt;\n  mutate(result_type = \"lowest_brier\") |&gt;\n  ungroup()\n\n# Identify tree with lowest ICI\nlowest_ici_xgb &lt;-\n  metrics_xgb_all |&gt;\n  filter(sample == \"Validation\", recalib == \"None\") |&gt;\n  group_by(scenario, repn) |&gt;\n  arrange(ici) |&gt;\n  slice_head(n = 1) |&gt;\n  select(scenario, repn, ind, nb_iter, recalib) |&gt;\n  mutate(result_type = \"lowest_ici\") |&gt;\n  ungroup()\n\n# Identify tree with lowest KL\nlowest_kl_xgb &lt;-\n  metrics_xgb_all |&gt;\n  filter(sample == \"Validation\", recalib == \"None\") |&gt;\n  group_by(scenario, repn) |&gt;\n  arrange(KL_20_true_probas) |&gt;\n  slice_head(n = 1) |&gt;\n  select(scenario, repn, ind, nb_iter, recalib) |&gt;\n  mutate(result_type = \"lowest_kl\") |&gt;\n  ungroup()\n\nmediocre_ici_xgb &lt;- \n  metrics_xgb_all |&gt;\n  filter(sample == \"Validation\", recalib == \"None\") |&gt;\n  group_by(scenario, repn) |&gt;\n  # For each replication for a scenario, we select a model with a mediocre \n  # calibration\n  mutate(\n    mean_ici = mean(ici),\n    sd_ici = sd(ici),\n    upb_ici = mean_ici + sd_ici,\n  ) |&gt; \n  filter(ici &gt;  upb_ici) |&gt; \n  # Among the configurations for which the calibration is not within 1-sd of the\n  # average calibration, we select the model with the lowest ICI\n  arrange(ici) |&gt; \n  slice_head(n = 1) |&gt; \n  select(scenario, repn, ind, nb_iter, recalib) |&gt;\n  mutate(result_type = \"high_ici\") |&gt;\n  ungroup()\n\n# Merge these\nmodels_of_interest_xgb &lt;-\n  smallest_xgb |&gt;\n  bind_rows(largest_xgb) |&gt;\n  bind_rows(highest_auc_xgb) |&gt;\n  bind_rows(lowest_mse_xgb) |&gt;\n  bind_rows(lowest_brier_xgb) |&gt;\n  bind_rows(lowest_ici_xgb) |&gt;\n  bind_rows(lowest_kl_xgb) |&gt; \n  bind_rows(mediocre_ici_xgb)\n\nmodels_of_interest_metrics &lt;- NULL\nfor (recalibration_method in c(\"None\", \"Platt\", \"Isotonic\")) {\n  # Add metrics now\n  models_of_interest_metrics &lt;-\n    models_of_interest_metrics |&gt;\n    bind_rows(\n      models_of_interest_xgb |&gt; select(-recalib) |&gt;\n        left_join(\n          metrics_xgb_all |&gt;\n            filter(\n              recalib == recalibration_method,\n              sample %in% c(\"Validation\", \"Test\")\n            ),\n          by = c(\"scenario\", \"repn\", \"ind\", \"nb_iter\"),\n          relationship = \"many-to-many\" # (calib, test)\n        )\n    )\n}\n\n\nmodels_of_interest_metrics &lt;-\n  models_of_interest_metrics |&gt;\n  mutate(\n    result_type = factor(\n      result_type,\n      levels = c(\n        \"smallest\", \"largest\", \"lowest_mse\", \"largest_auc\",\n        \"lowest_brier\", \"lowest_ici\", \"lowest_kl\", \"high_ici\"),\n      labels = c(\n        \"Smallest\", \"Largest\", \"MSE*\", \"AUC*\",\n        \"Brier*\", \"ICI*\", \"KL*\", \"High ICI\"\n      )\n    )\n  )\n\n# Sanity check\n# models_of_interest_metrics |&gt; count(scenario, sample, result_type)\n\n\n\n5.5.1 Metrics vs Number of Iterations\nWe define a function, plot_metrics() to plot selected metrics (AUC, ICI, and KL Divergence) as a function of the number of boosting iterations, for a given value for the hyperparameter max_depth. Each curve corresponds to a value of the maximal depth hyperparameter.\nTBD\n\n\n5.5.2 Distribution of Scores\nLet us extract all the histogram information computed over the simulations and put that in a single object, scores_hist_all.\n\nscores_hist_all &lt;-\n  map(\n    resul_xgb,\n    function(resul_xgb_sc) map(resul_xgb_sc, \"scores_hist\")\n  )\n\nWe then define a function, plot_bp_xgb() which plots the distribution of scores on the test set for a single replication (repn), for a scenario, (scenario). We also define a helper function, plot_bp_interest(), which plots the histogram of the scores at a specific iteration number. We will then be able to plot the distributions at the beginning of the boosting iterations, at the end, at a point where the AUC was the highest on the validation set, and at a point where the KL divergence between the distribution of scores on the validation set and the distribution of the true probabilities was the lowest. We will plot the distributions of the scores returned by the classifier, as well as those obtained with the reclibrators.\n\n\nFunction plot_metrics()\nplot_bp_interest &lt;- function(metrics_interest,\n                             scores_hist_interest,\n                             label,\n                             recalib_method) {\n  subtitle &lt;- str_c(\n    \"Depth = \", metrics_interest$max_depth, \", \",\n    \"MSE = \", round(metrics_interest$mse, 2), \", \",\n    \"AUC = \", round(metrics_interest$AUC, 2), \", \\n\",\n    \"Brier = \", round(metrics_interest$brier, 2), \",\",\n    \"ICI = \", round(metrics_interest$ici, 2), \", \",\n    \"KL = \", round(metrics_interest$KL_20_true_probas, 2)\n  )\n\n  if (recalib_method == \"none\") {\n    plot(\n      main = str_c(label, \" (iter = \", metrics_interest$nb_iter,\")\"),\n      scores_hist_interest$test,\n      xlab = latex2exp::TeX(\"$\\\\hat{s}(x)$\"),\n      ylab = \"\"\n    )\n  } else if (recalib_method == \"platt\") {\n    plot(\n      main = str_c(label, \" (iter = \", metrics_interest$nb_iter,\")\"),\n      scores_hist_interest$test_c_platt,\n      xlab = latex2exp::TeX(\"$\\\\hat{s}(x)$\"),\n      ylab = \"\",\n      col = colour_recalib[[\"Platt\"]]\n    )\n  } else if (recalib_method == \"iso\") {\n    plot(\n      main = str_c(label, \" (iter = \", metrics_interest$nb_iter,\")\"),\n      scores_hist_interest$test_c_iso,\n      xlab = latex2exp::TeX(\"$\\\\hat{s}(x)$\"),\n      ylab = \"\",\n      col = colour_recalib[[\"Isotonic\"]]\n    )\n  }\n  mtext(side = 3, line = -0.25, adj = .5, subtitle, cex = .5)\n}\n\nplot_bp_xgb &lt;- function(scenario,\n                        repn,\n                        paper_version = FALSE) {\n  # Focus on current scenario\n  scores_hist_scenario &lt;- scores_hist_all[[scenario]]\n  # Focus on a particular replication\n  scores_hist_repn &lt;- scores_hist_scenario[[repn]]\n  # # Focus on a value for max_depth\n  max_depth_val &lt;- map_dbl(scores_hist_repn, ~.x[[1]]$max_depth)\n  # i_max_depth &lt;- which(max_depth_val == max_depth)\n  # scores_hist &lt;- scores_hist_repn[[i_max_depth]]\n  \n  # True Probabilities\n  simu_data &lt;- simulate_data_wrapper(\n    scenario = scenario,\n    params_df = params_df,\n    repn = repn # only one replication here\n  )\n  true_prob &lt;- simu_data$data$probs_train\n  \n  for (recalib_method in c(\"none\", \"platt\", \"iso\")) {\n    \n    i_method &lt;- match(recalib_method, c(\"none\", \"platt\", \"iso\"))\n    recalib_method_lab &lt;- c(\"None\", \"Platt\", \"Isotonic\")[i_method]\n    \n    # The metrics for the corresponding simulations, on the validation set\n    metrics_xgb_current_valid &lt;-\n      metrics_xgb_all |&gt;\n      filter(\n        scenario == !!scenario,\n        repn == !!repn,\n        sample == \"Validation\",\n        recalib == \"None\"\n      )\n    # and on the test set\n    metrics_xgb_current_test &lt;-\n      metrics_xgb_all |&gt;\n      filter(\n        scenario == !!scenario,\n        repn == !!repn,\n        sample == \"Test\",\n        recalib == recalib_method_lab\n      )\n    \n    if (paper_version == FALSE) {\n      hist(\n        true_prob,\n        breaks = seq(0, 1, by = .05),\n        xlab = \"p\", ylab = \"\",\n        main = \"True Probabilities\",\n        xlim = c(0, 1)\n      )\n      mtext(\n        side = 2, recalib_method_lab, line = 2.5, cex = 1,\n        font.lab = 2\n      )\n      # Iterations of interest----\n      ## Start of iterations\n      scores_hist_start &lt;- scores_hist_repn[[1]][[1]]\n      metrics_start &lt;- metrics_xgb_current_test |&gt;\n        filter(\n          nb_iter == scores_hist_start$nb_iter,\n          max_depth == scores_hist_start$max_depth\n        )\n      \n      plot_bp_interest(\n        metrics_interest = metrics_start,\n        scores_hist_interest = scores_hist_start,\n        label = \"Start\",\n        recalib_method = recalib_method\n      )\n      \n      ## End of iterations\n      scores_hist_end &lt;- scores_hist_repn[[1]][[length(scores_hist_repn[[1]])]]\n      metrics_end &lt;- metrics_xgb_current_test |&gt;\n        filter(\n          nb_iter == scores_hist_end$nb_iter,\n          max_depth == scores_hist_start$max_depth\n        )\n      plot_bp_interest(\n        metrics_interest = metrics_end,\n        scores_hist_interest = scores_hist_end,\n        label = \"End\",\n        recalib_method = recalib_method\n      )\n      \n      ## Iteration with min MSE on validation set\n      metrics_valid_mse_star &lt;- metrics_xgb_current_valid |&gt; arrange(mse) |&gt;\n        dplyr::slice(1)\n      nb_iter_mse &lt;- metrics_valid_mse_star$nb_iter\n      max_depth_mse_star &lt;- metrics_valid_mse_star$max_depth\n      i_max_depth_mse_star &lt;- which(max_depth_val == max_depth_mse_star)\n      # Metrics at the same iteration on the test set\n      metrics_min_mse &lt;-\n        metrics_xgb_current_test |&gt;\n        filter(\n          nb_iter == !!nb_iter_mse,\n          max_depth == max_depth_mse_star\n        )\n      # Note: indexing at 0 here...\n      ind_mse &lt;- which(map_dbl(scores_hist_repn[[i_max_depth_mse_star]], \"nb_iter\") == nb_iter_mse)\n      scores_hist_min_mse &lt;- scores_hist_repn[[i_max_depth_mse_star]][[ind_mse]]\n      plot_bp_interest(\n        metrics_interest = metrics_min_mse,\n        scores_hist_interest = scores_hist_min_mse,\n        label = \"MSE*\",\n        recalib_method = recalib_method\n      )\n    }\n    ## Iteration with max AUC on validation set\n    metrics_valid_auc_star &lt;-\n      metrics_xgb_current_valid |&gt; arrange(desc(AUC)) |&gt; dplyr::slice(1)\n    nb_iter_auc &lt;- metrics_valid_auc_star$nb_iter\n    max_depth_auc_star &lt;- metrics_valid_auc_star$max_depth\n    i_max_depth_auc_star &lt;- which(max_depth_val == max_depth_auc_star)\n    \n    metrics_max_auc &lt;-\n      metrics_xgb_current_test |&gt;\n      filter(nb_iter == !!nb_iter_auc, max_depth == max_depth_auc_star)\n    # Note: indexing at 0 here...\n    ind_auc &lt;- which(map_dbl(scores_hist_repn[[i_max_depth_auc_star]], \"nb_iter\") == nb_iter_auc)\n    scores_hist_max_auc &lt;- scores_hist_repn[[i_max_depth_auc_star]][[ind_auc]]\n    plot_bp_interest(\n      metrics_interest = metrics_max_auc,\n      scores_hist_interest = scores_hist_max_auc,\n      label = \"AUC*\",\n      recalib_method = recalib_method\n    )\n    if (paper_version == TRUE) {\n      mtext(\n        side = 2, recalib_method_lab, line = 2.5, cex = 1,\n        font.lab = 2\n      )\n    }\n    \n    ## Min Brier on validation set\n    metrics_valid_brier_star &lt;-\n      metrics_xgb_current_valid |&gt; arrange(brier) |&gt; dplyr::slice(1)\n    nb_iter_brier &lt;- metrics_valid_brier_star$nb_iter\n    max_depth_brier_star &lt;- metrics_valid_brier_star$max_depth\n    i_max_depth_brier_star &lt;- which(max_depth_val == max_depth_brier_star)\n    \n    metrics_min_brier &lt;-\n      metrics_xgb_current_test |&gt;\n      filter(nb_iter == !!nb_iter_brier, max_depth == max_depth_brier_star)\n    ind_brier &lt;- which(map_dbl(scores_hist_repn[[i_max_depth_brier_star]], \"nb_iter\") == nb_iter_brier)\n    scores_hist_min_brier &lt;- scores_hist_repn[[i_max_depth_brier_star]][[ind_brier]]\n    plot_bp_interest(\n      metrics_interest = metrics_min_brier,\n      scores_hist_interest = scores_hist_min_brier,\n      label = \"Brier*\",\n      recalib_method = recalib_method\n    )\n    \n    ## Min ICI on validation set\n    metrics_valid_ici_star &lt;-\n      metrics_xgb_current_valid |&gt; arrange(ici) |&gt; dplyr::slice(1)\n    nb_iter_ici &lt;-   metrics_valid_ici_star$nb_iter\n    max_depth_ici_star &lt;- metrics_valid_ici_star$max_depth\n    i_max_depth_ici_star &lt;- which(max_depth_val == max_depth_ici_star)\n    \n    metrics_min_ici &lt;-\n      metrics_xgb_current_test |&gt;\n      filter(nb_iter == !!nb_iter_ici, max_depth == max_depth_ici_star)\n    ind_ici &lt;- which(map_dbl(scores_hist_repn[[i_max_depth_ici_star]], \"nb_iter\") == nb_iter_ici)\n    scores_hist_min_ici &lt;- scores_hist_repn[[i_max_depth_ici_star]][[ind_ici]]\n    plot_bp_interest(\n      metrics_interest = metrics_min_ici,\n      scores_hist_interest = scores_hist_min_ici,\n      label = \"ICI*\",\n      recalib_method = recalib_method\n    )\n    \n    ## Min KL on validation set\n    metrics_valid_kl_star &lt;-\n      metrics_xgb_current_valid |&gt; arrange(KL_20_true_probas) |&gt; dplyr::slice(1)\n    nb_iter_kl &lt;- metrics_valid_kl_star$nb_iter\n    max_depth_kl_star &lt;- metrics_valid_kl_star$max_depth\n    i_max_depth_kl_star &lt;- which(max_depth_val == max_depth_kl_star)\n    \n    metrics_min_kl &lt;-\n      metrics_xgb_current_test |&gt;\n      filter(nb_iter == !!nb_iter_kl, max_depth == max_depth_kl_star)\n    ind_kl &lt;- which(map_dbl(scores_hist_repn[[i_max_depth_kl_star]], \"nb_iter\") == nb_iter_kl)\n    scores_hist_min_kl &lt;- scores_hist_repn[[i_max_depth_kl_star]][[ind_kl]]\n    plot_bp_interest(\n      metrics_interest = metrics_min_kl,\n      scores_hist_interest = scores_hist_min_kl,\n      label = \"KL*\",\n      recalib_method = recalib_method\n    )\n    \n    ## Mediocre ICI on validation set\n    identified_mici &lt;- \n      mediocre_ici_xgb |&gt; filter(scenario == !!scenario, repn == !!repn)\n    \n    metrics_valid_mici_star &lt;- metrics_xgb_current_valid |&gt; \n      filter(ind == identified_mici$ind, nb_iter == identified_mici$nb_iter)\n    nb_iter_mici &lt;- metrics_valid_mici_star$nb_iter\n    max_depth_mici_star &lt;- metrics_valid_mici_star$max_depth\n    i_max_depth_mici_star &lt;- which(max_depth_val == max_depth_mici_star)\n    \n    metrics_mici &lt;-\n      metrics_xgb_current_test |&gt;\n      filter(nb_iter == !!nb_iter_mici, max_depth == max_depth_mici_star)\n    ind_mici &lt;- which(map_dbl(scores_hist_repn[[i_max_depth_mici_star]], \"nb_iter\") == nb_iter_mici)\n    scores_hist_mici &lt;- scores_hist_repn[[i_max_depth_mici_star]][[ind_mici]]\n    plot_bp_interest(\n      metrics_interest = metrics_mici,\n      scores_hist_interest = scores_hist_mici,\n      label = \"High ICI\",\n      recalib_method = recalib_method\n    )\n  }\n}\n\n\n\nDGP 1DGP 2DGP 3DGP 4\n\n\n\n0 noise variable10 noise variables50 noise variables100 noise variables\n\n\n\n\nCode\npar(mfrow = c(3,9), mar = c(4.1, 4, 3.5, 1.5))\nplot_bp_xgb(scenario = 1, repn = 1)\n\n\n\n\n\nFigure 5.1: Distribution of scores on the test set (DGP 1, 0 noise variable)\n\n\n\n\n\n\n\n\n\n\n\n\nCode\npar(mfrow = c(3,9), mar = c(4.1, 4, 3.5, 1.5))\nplot_bp_xgb(scenario = 2, repn = 1)\n\n\n\n\n\nFigure 5.2: Distribution of scores on the test set (DGP 1, 10 noise variables)\n\n\n\n\n\n\n\n\n\n\n\n\nCode\npar(mfrow = c(3,9), mar = c(4.1, 4, 3.5, 1.5))\nplot_bp_xgb(scenario = 3, repn = 1)\n\n\n\n\n\nFigure 5.3: Distribution of scores on the test set (DGP 1, 50 noise variables)\n\n\n\n\n\n\n\n\n\n\n\n\nCode\npar(mfrow = c(3,9), mar = c(4.1, 4, 3.5, 1.5))\nplot_bp_xgb(scenario = 4, repn = 1)\n\n\n\n\n\nFigure 5.4: Distribution of scores on the test set (DGP 1, 100 noise variables)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0 noise variable10 noise variables50 noise variables100 noise variables\n\n\n\n\nCode\npar(mfrow = c(3,9), mar = c(4.1, 4, 3.5, 1.5))\nplot_bp_xgb(scenario = 5, repn = 1)\n\n\n\n\n\nFigure 5.5: Distribution of scores on the test set (DGP 2, 0 noise variable)\n\n\n\n\n\n\n\n\n\n\n\n\nCode\npar(mfrow = c(3,9), mar = c(4.1, 4, 3.5, 1.5))\nplot_bp_xgb(scenario = 6, repn = 1)\n\n\n\n\n\nFigure 5.6: Distribution of scores on the test set (DGP 2, 10 noise variables)\n\n\n\n\n\n\n\n\n\n\n\n\nCode\npar(mfrow = c(3,9), mar = c(4.1, 4, 3.5, 1.5))\nplot_bp_xgb(scenario = 7, repn = 1)\n\n\n\n\n\nFigure 5.7: Distribution of scores on the test set (DGP 2, 50 noise variables)\n\n\n\n\n\n\n\n\n\n\n\n\nCode\npar(mfrow = c(3,9), mar = c(4.1, 4, 3.5, 1.5))\nplot_bp_xgb(scenario = 8, repn = 1)\n\n\n\n\n\nFigure 5.8: Distribution of scores on the test set (DGP 2, 100 noise variables)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0 noise variable10 noise variables50 noise variables100 noise variables\n\n\n\n\nCode\npar(mfrow = c(3,9), mar = c(4.1, 4, 3.5, 1.5))\nplot_bp_xgb(scenario = 9, repn = 1)\n\n\n\n\n\nFigure 5.9: Distribution of scores on the test set (DGP 3, 0 noise variable)\n\n\n\n\n\n\n\n\n\n\n\n\nCode\npar(mfrow = c(3,9), mar = c(4.1, 4, 3.5, 1.5))\nplot_bp_xgb(scenario = 10, repn = 1)\n\n\n\n\n\nFigure 5.10: Distribution of scores on the test set (DGP 3, 10 noise variables)\n\n\n\n\n\n\n\n\n\n\n\n\nCode\npar(mfrow = c(3,9), mar = c(4.1, 4, 3.5, 1.5))\nplot_bp_xgb(scenario = 11, repn = 1)\n\n\n\n\n\nFigure 5.11: Distribution of scores on the test set (DGP 3, 50 noise variables)\n\n\n\n\n\n\n\n\n\n\n\n\nCode\npar(mfrow = c(3,9), mar = c(4.1, 4, 3.5, 1.5))\nplot_bp_xgb(scenario = 12, repn = 1)\n\n\n\n\n\nFigure 5.12: Distribution of scores on the test set (DGP 3, 100 noise variables)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0 noise variable10 noise variables50 noise variables100 noise variables\n\n\n\n\nCode\npar(mfrow = c(3,9), mar = c(4.1, 4, 3.5, 1.5))\nplot_bp_xgb(scenario = 13, repn = 1)\n\n\n\n\n\nFigure 5.13: Distribution of scores on the test set (DGP 4, 0 noise variable)\n\n\n\n\n\n\n\n\n\n\n\n\nCode\npar(mfrow = c(3,9), mar = c(4.1, 4, 3.5, 1.5))\nplot_bp_xgb(scenario = 14, repn = 1)\n\n\n\n\n\nFigure 5.14: Distribution of scores on the test set (DGP 4, 10 noise variables)\n\n\n\n\n\n\n\n\n\n\n\n\nCode\npar(mfrow = c(3,9), mar = c(4.1, 4, 3.5, 1.5))\nplot_bp_xgb(scenario = 15, repn = 1)\n\n\n\n\n\nFigure 5.15: Distribution of scores on the test set (DGP 4, 50 noise variables)\n\n\n\n\n\n\n\n\n\n\n\n\nCode\npar(mfrow = c(3,9), mar = c(4.1, 4, 3.5, 1.5))\nplot_bp_xgb(scenario = 16, repn = 1)\n\n\n\n\n\nFigure 5.16: Distribution of scores on the test set (DGP 4, 100 noise variables)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode to create PDF figures\nif(!dir.exists(\"../figs/\")) dir.create(\"../figs/\")\nfor (scenario in 1:16) {\n  pdf(\n    file = str_c(\"../figs/bp_synthetic_xbg_\", scenario, \".pdf\"),\n    height = 4.5, width = 10\n  )\n  par(mfrow = c(3,5), mar = c(4.1, 4, 3.5, 1.5))\n  plot_bp_xgb(scenario = scenario, repn = 1, paper_version = TRUE)\n  dev.off()\n}\n\n\n\n\n5.5.3 KL Divergence and Calibration along Boosting Iterations\nWe can examine the evolution of the relationship between the divergence of score distributions from true probabilities and model calibration across increasing boosting iterations.\n\n\nCodes to create the figure\ndf_plot &lt;-\n  metrics_xgb_all |&gt;\n  mutate(\n    dgp = case_when(\n      scenario %in% 1:4 ~ 1,\n      scenario %in% 5:8 ~ 2,\n      scenario %in% 9:12 ~ 3,\n      scenario %in% 13:16 ~ 4\n    ),\n    dgp = factor(dgp, levels = 1:4, labels = str_c(\"DGP \", 1:4)),\n    no_noise = c(0, 10, 50, 100)[(scenario-1)%%4 + 1],\n    no_noise = factor(\n      no_noise, levels = c(no_noise),\n      labels = str_c(no_noise, \" noise variables\")\n    )\n  ) |&gt;\n  select(\n    dgp, no_noise, scenario, recalib, ind, sample, nb_iter, max_depth,\n    brier, ici, KL_20_true_probas\n  ) |&gt;\n  group_by(dgp, no_noise, scenario, recalib, ind, sample, nb_iter, max_depth) |&gt;\n  summarise(\n    brier = mean(brier),\n    ici = mean(ici),\n    KL_20_true_probas = mean(KL_20_true_probas),\n    .groups = \"drop\"\n  ) |&gt;\n  mutate(\n    max_depth = factor(\n      max_depth,\n      levels = c(2, 4, 6)\n    )\n  )\n\nformatter1000 &lt;- function(x) x*1000\n\n\n\nBrierICI\n\n\n\n\nCodes to create the figure\np_brier &lt;- ggplot(\n  data = df_plot |&gt; arrange(nb_iter) |&gt; filter(max_depth == 2),\n  mapping = aes(x = brier, y = KL_20_true_probas)\n) +\n  geom_path(\n    mapping = aes(colour = sample, linetype = recalib),\n    arrow = arrow(type = \"closed\", ends = \"last\",\n                  length = unit(0.08, \"inches\"))\n  ) +\n  # facet_wrap(~scenario) +\n  ggh4x::facet_grid2(dgp~no_noise, scales = \"free_y\", independent = \"y\") +\n  labs(\n    x = latex2exp::TeX(\"Calibration (Brier), $\\\\times 10^{3}$, log scale\"),\n    y = \"KL Divergence\"\n  ) +\n  scale_x_log10(labels = formatter1000) + scale_y_log10() +\n  scale_colour_manual(\"Sample\", values = colour_samples) +\n  scale_linetype_discrete(\"Recalibration\") +\n  theme_paper() +\n  theme(legend.key.width = unit(1.5, \"cm\"))\n\n\nggsave(\n  p_brier, file = \"../figs/xgb-kl-calib-brier-leaves-all.pdf\",\n       width = 13, height = 8\n)\n\np_brier\n\n\n\n\n\nFigure 5.17: KL Divergence and Calibration (Brier) across increasing boosting iterations (log scales)\n\n\n\n\n\n\n\n\n\n\n\n\nCodes to create the figure\np_ici &lt;- ggplot(\n  data = df_plot |&gt; arrange(nb_iter) |&gt; filter(max_depth == 2),\n  mapping = aes(x = ici, y = KL_20_true_probas)\n) +\n  geom_path(\n    mapping = aes(colour = sample, linetype = recalib),\n    arrow = arrow(type = \"closed\", ends = \"last\",\n                  length = unit(0.08, \"inches\"))\n  ) +\n  # facet_wrap(~scenario) +\n  ggh4x::facet_grid2(dgp~no_noise, scales = \"free_y\", independent = \"y\") +\n  labs(\n    x = latex2exp::TeX(\"Calibration (ICI), $\\\\times 10^{3}$, log scale\"),\n    y = \"KL Divergence\"\n  ) +\n  scale_x_log10(labels = formatter1000) + scale_y_log10() +\n  scale_colour_manual(\"Sample\", values = colour_samples) +\n  scale_linetype_discrete(\"Recalibration\") +\n  theme_paper() +\n  theme(legend.key.width = unit(1.5, \"cm\"))\n\n\nggsave(\n  p_ici, file = \"../figs/xgb-kl-calib-ici-leaves-all.pdf\",\n  width = 13, height = 8\n)\np_ici\n\n\n\n\n\nFigure 5.18: KL Divergence and Calibration (ICI) across increasing boosting iterations (log scales)\n\n\n\n\n\n\n\n\n\n\n\n\n\n5.5.4 Tables\nWe examine the average values of various metrics across 100 replications for the “best” model selected according to different criteria: AUC* for the model with hyperparameters chosen to maximize AUC on the validation set, ICI* for the model with hyperparameters selected to minimize ICI, Brier* for minimizing the Brier Score, KL* for minimizing the KL divergence between the distribution of scores on the validation set and the true probability distribution, smallest for the model with only 2 boosting iterations, largest for the model with 400 boosting iterations, and mediocre ICI for the model chosen to illustrate the effects of score recalibration when initial calibration is mediocre.\n\n\nDisplay the R codes to produce the table\nmodels_interest_xgb &lt;- models_of_interest_metrics |&gt; \n  group_by(scenario, recalib, sample, result_type) |&gt; \n  summarise(\n    AUC_lower = quantile(AUC, probs = 2.5/100),\n    AUC_upper = quantile(AUC, probs = 97.5/100),\n    AUC_sd = sd(AUC),\n    AUC = mean(AUC),\n    brier_lower = quantile(brier, probs = 2.5/100),\n    brier_upper = quantile(brier, probs = 97.5/100),\n    brier_sd = sd(brier),\n    brier = mean(brier),\n    ici_lower = quantile(ici, probs = 2.5/100),\n    ici_upper = quantile(ici, probs = 97.5/100),\n    ici_sd = sd(ici),\n    ici = mean(ici),\n    KL_20_true_probas_lower = quantile(KL_20_true_probas, probs = 2.5/100),\n    KL_20_true_probas_upper = quantile(KL_20_true_probas, probs = 97.5/100),\n    KL_20_true_probas_sd = sd(KL_20_true_probas),\n    KL_20_true_probas = mean(KL_20_true_probas),\n    quant_ratio_sd = sd(inter_quantile_10_90),\n    quant_ratio = mean(inter_quantile_10_90),\n    .groups = \"drop\"\n  ) |&gt; \n  mutate(\n    model = \"xgb\",\n    sample = str_to_lower(as.character(sample)),\n    dgp = case_when(\n      scenario %in% 1:4 ~ 1,\n      scenario %in% 5:8 ~ 2,\n      scenario %in% 9:12 ~ 3,\n      scenario %in% 13:16 ~ 4\n    ),\n    no_noise = c(0, 10, 50, 100)[(scenario-1)%%4 + 1],\n    no_noise = factor(\n      no_noise,\n      levels = c(no_noise),\n      labels = str_c(no_noise, \" noise variables\")\n    )\n  )\n\n# Sanity check\n# metrics_xgb_all |&gt; count(scenario, recalib, ind, sample, nb_iter) |&gt;\n#   filter(n != max(repns_vector))\n\ntable_models_interest_mean &lt;- \n  models_interest_xgb |&gt; \n  filter(sample == \"test\") |&gt; \n  select(\n    dgp, no_noise, recalib, sample, result_type, \n    brier, ici, kl = KL_20_true_probas,\n  ) |&gt; \n  filter(\n    result_type %in% c(\"AUC*\", \"KL*\", \"High ICI\", \"Smallest\", \"Largest\")\n  ) |&gt; \n  mutate(value_type = \"mean\")\n\n\ntable_models_interest_sd &lt;- \n  models_interest_xgb |&gt; \n  filter(sample == \"test\") |&gt; \n  select(\n    dgp, no_noise, recalib, sample, result_type, \n    brier = brier_sd, ici = ici_sd, \n    kl = KL_20_true_probas_sd,\n  ) |&gt; \n  filter(\n    result_type %in% c(\"AUC*\", \"KL*\", \"High ICI\", \"Smallest\", \"Largest\")\n  ) |&gt; \n  mutate(value_type = \"sd\")\n\ndigits &lt;- 2\n\ntable_models_interest_mean |&gt; \n  bind_rows(table_models_interest_sd) |&gt; \n  pivot_longer(cols = c(brier, ici, kl)) |&gt; \n  pivot_wider(names_from = \"value_type\", values_from = \"value\") |&gt; \n  mutate(value = str_c(round(`mean`, digits), \" (\", round(`sd`, digits), \")\")) |&gt; \n  select(-mean, -sd, -sample) |&gt; \n  pivot_wider(names_from = c(recalib, name), values_from = value) |&gt; \n  knitr::kable(\n    align = str_c(\"cl\", str_c(rep(\"c\", 3*3), collapse = \"\"), collapse = \"\"),\n    escape = FALSE, booktabs = T, digits = 3, format = \"markdown\",\n    col.names = c(\n      \"DGP\", \"Noise\", \"Optim.\",\n      rep(c(\"BS\", \"ICI\", \"KL\"), 3)\n    )\n  ) |&gt; \n  kableExtra::collapse_rows(columns = 1:2, valign = \"top\") |&gt; \n  kableExtra::add_header_above(\n    c(\" \" = 3,\n      \"None\" = 3,\n      \"Platt\" = 3,\n      \"Isotonic\" = 3\n    )\n  ) |&gt; \n  kableExtra::scroll_box(fixed_thead = TRUE, height = \"500px\")\n\n\n\n\nTable 5.2: Performance and calibration metrics (Brier Score, Integrated Calibration Index, Kullback-Leibler Divergence) computed on the test set, on scores returned by the model (column ‘None’), on scores recalibrated using Platt scaling (column ‘Platt’), or Isotonic regression (coliumn ‘Isotonic’)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNone\n\n\nPlatt\n\n\nIsotonic\n\n\n\nDGP\nNoise\nOptim.\nBS\nICI\nKL\nBS\nICI\nKL\nBS\nICI\nKL\n\n\n\n\n1\n0 noise variables\nSmallest\n0.23 (0)\n0.12 (0.01)\n1.88 (0.06)\n0.21 (0)\n0.01 (0)\n2.04 (0.05)\n0.21 (0)\n0.01 (0)\n2.04 (0.06)\n\n\nLargest\n0.21 (0)\n0.03 (0.01)\n0.05 (0.01)\n0.21 (0)\n0.01 (0)\n0.1 (0.01)\n0.21 (0)\n0.01 (0)\n0.31 (0.11)\n\n\nAUC*\n0.2 (0)\n0.01 (0.01)\n0.05 (0.04)\n0.2 (0)\n0.02 (0)\n0.13 (0.03)\n0.2 (0)\n0.01 (0)\n0.3 (0.1)\n\n\nKL*\n0.2 (0)\n0.01 (0)\n0.02 (0.01)\n0.2 (0)\n0.02 (0)\n0.11 (0.02)\n0.2 (0)\n0.01 (0)\n0.31 (0.1)\n\n\nHigh ICI\n0.22 (0)\n0.06 (0.01)\n0.17 (0.1)\n0.21 (0)\n0.02 (0.01)\n0.2 (0.06)\n0.21 (0)\n0.01 (0)\n0.36 (0.12)\n\n\n10 noise variables\nSmallest\n0.23 (0)\n0.12 (0.01)\n1.88 (0.06)\n0.21 (0)\n0.01 (0)\n2.04 (0.05)\n0.21 (0)\n0.01 (0)\n2.04 (0.06)\n\n\nLargest\n0.21 (0)\n0.04 (0.01)\n0.04 (0.01)\n0.21 (0)\n0.02 (0)\n0.15 (0.03)\n0.21 (0)\n0.01 (0)\n0.31 (0.12)\n\n\nAUC*\n0.2 (0)\n0.01 (0.01)\n0.07 (0.03)\n0.2 (0)\n0.02 (0)\n0.14 (0.03)\n0.2 (0)\n0.01 (0)\n0.3 (0.11)\n\n\nKL*\n0.2 (0)\n0.02 (0)\n0.01 (0)\n0.2 (0)\n0.02 (0)\n0.11 (0.02)\n0.2 (0)\n0.01 (0)\n0.3 (0.11)\n\n\nHigh ICI\n0.23 (0)\n0.11 (0.01)\n0.43 (0.18)\n0.22 (0)\n0.02 (0)\n0.37 (0.03)\n0.22 (0)\n0.01 (0)\n0.35 (0.12)\n\n\n50 noise variables\nSmallest\n0.23 (0)\n0.12 (0.01)\n1.88 (0.06)\n0.21 (0)\n0.01 (0)\n2.04 (0.05)\n0.21 (0)\n0.01 (0)\n2.04 (0.06)\n\n\nLargest\n0.21 (0)\n0.05 (0.01)\n0.05 (0.01)\n0.21 (0)\n0.02 (0)\n0.19 (0.02)\n0.21 (0)\n0.01 (0)\n0.33 (0.11)\n\n\nAUC*\n0.2 (0)\n0.02 (0)\n0.07 (0.03)\n0.2 (0)\n0.02 (0)\n0.14 (0.03)\n0.2 (0)\n0.01 (0)\n0.31 (0.1)\n\n\nKL*\n0.21 (0)\n0.02 (0.01)\n0.01 (0)\n0.21 (0)\n0.02 (0)\n0.12 (0.02)\n0.21 (0)\n0.01 (0)\n0.31 (0.1)\n\n\nHigh ICI\n0.23 (0)\n0.13 (0.01)\n0.69 (0.09)\n0.22 (0)\n0.03 (0.01)\n0.43 (0.04)\n0.22 (0)\n0.01 (0)\n0.34 (0.12)\n\n\n100 noise variables\nSmallest\n0.23 (0)\n0.12 (0.01)\n1.88 (0.06)\n0.21 (0)\n0.01 (0)\n2.04 (0.05)\n0.21 (0)\n0.01 (0)\n2.04 (0.06)\n\n\nLargest\n0.21 (0)\n0.05 (0.01)\n0.05 (0.01)\n0.21 (0)\n0.02 (0)\n0.2 (0.01)\n0.21 (0)\n0.01 (0)\n0.32 (0.1)\n\n\nAUC*\n0.2 (0)\n0.02 (0.01)\n0.09 (0.03)\n0.2 (0)\n0.02 (0)\n0.15 (0.03)\n0.2 (0)\n0.01 (0)\n0.32 (0.11)\n\n\nKL*\n0.21 (0)\n0.02 (0.01)\n0.01 (0)\n0.21 (0)\n0.02 (0)\n0.12 (0.02)\n0.21 (0)\n0.01 (0)\n0.3 (0.11)\n\n\nHigh ICI\n0.24 (0)\n0.14 (0.01)\n0.8 (0.06)\n0.22 (0)\n0.03 (0)\n0.44 (0.03)\n0.22 (0)\n0.01 (0)\n0.34 (0.11)\n\n\n2\n0 noise variables\nSmallest\n0.19 (0)\n0.23 (0.01)\n3.21 (0.2)\n0.13 (0)\n0.03 (0.01)\n1.89 (0.09)\n0.13 (0)\n0.01 (0)\n1.72 (0.28)\n\n\nLargest\n0.12 (0)\n0.02 (0)\n0.02 (0.01)\n0.12 (0)\n0.04 (0)\n0.88 (0.03)\n0.12 (0)\n0.01 (0)\n0.21 (0.06)\n\n\nAUC*\n0.12 (0)\n0.01 (0)\n0.03 (0.03)\n0.12 (0)\n0.04 (0)\n0.78 (0.22)\n0.12 (0)\n0.01 (0)\n0.21 (0.07)\n\n\nKL*\n0.12 (0)\n0.01 (0)\n0.01 (0)\n0.12 (0)\n0.04 (0)\n0.86 (0.14)\n0.12 (0)\n0.01 (0)\n0.21 (0.07)\n\n\nHigh ICI\n0.13 (0)\n0.05 (0)\n0.13 (0.13)\n0.13 (0)\n0.05 (0.01)\n0.84 (0.08)\n0.13 (0)\n0.01 (0)\n0.24 (0.07)\n\n\n10 noise variables\nSmallest\n0.19 (0)\n0.23 (0.01)\n3.21 (0.2)\n0.13 (0)\n0.03 (0.01)\n1.89 (0.09)\n0.13 (0)\n0.01 (0)\n1.72 (0.28)\n\n\nLargest\n0.13 (0)\n0.03 (0)\n0.03 (0.01)\n0.13 (0)\n0.05 (0)\n0.87 (0.04)\n0.12 (0)\n0.01 (0)\n0.22 (0.08)\n\n\nAUC*\n0.12 (0)\n0.01 (0.01)\n0.04 (0.07)\n0.12 (0)\n0.04 (0)\n0.76 (0.23)\n0.12 (0)\n0.01 (0)\n0.21 (0.07)\n\n\nKL*\n0.12 (0)\n0.01 (0)\n0.01 (0)\n0.12 (0)\n0.04 (0)\n0.89 (0.08)\n0.12 (0)\n0.01 (0)\n0.21 (0.07)\n\n\nHigh ICI\n0.14 (0)\n0.08 (0)\n0.29 (0.14)\n0.13 (0)\n0.06 (0.01)\n1.23 (0.36)\n0.13 (0)\n0.01 (0)\n0.26 (0.09)\n\n\n50 noise variables\nSmallest\n0.19 (0)\n0.23 (0.01)\n3.21 (0.2)\n0.13 (0)\n0.03 (0.01)\n1.89 (0.09)\n0.13 (0)\n0.01 (0)\n1.72 (0.28)\n\n\nLargest\n0.13 (0)\n0.03 (0)\n0.04 (0.01)\n0.13 (0)\n0.05 (0)\n0.86 (0.04)\n0.13 (0)\n0.01 (0)\n0.22 (0.07)\n\n\nAUC*\n0.12 (0)\n0.01 (0)\n0.04 (0.02)\n0.12 (0)\n0.04 (0)\n0.73 (0.23)\n0.12 (0)\n0.01 (0)\n0.2 (0.07)\n\n\nKL*\n0.12 (0)\n0.01 (0)\n0.01 (0)\n0.12 (0)\n0.04 (0)\n0.89 (0.05)\n0.12 (0)\n0.01 (0)\n0.21 (0.07)\n\n\nHigh ICI\n0.14 (0)\n0.09 (0)\n0.43 (0.14)\n0.13 (0)\n0.06 (0.01)\n1.79 (0.16)\n0.13 (0)\n0.01 (0)\n0.23 (0.08)\n\n\n100 noise variables\nSmallest\n0.19 (0)\n0.23 (0.01)\n3.21 (0.2)\n0.13 (0)\n0.03 (0)\n1.89 (0.09)\n0.13 (0)\n0.01 (0)\n1.72 (0.28)\n\n\nLargest\n0.13 (0)\n0.04 (0)\n0.04 (0.01)\n0.13 (0)\n0.05 (0)\n0.86 (0.03)\n0.13 (0)\n0.01 (0)\n0.22 (0.07)\n\n\nAUC*\n0.12 (0)\n0.01 (0)\n0.04 (0.02)\n0.12 (0)\n0.04 (0)\n0.73 (0.22)\n0.12 (0)\n0.01 (0)\n0.21 (0.07)\n\n\nKL*\n0.12 (0)\n0.01 (0)\n0.01 (0)\n0.12 (0)\n0.04 (0)\n0.89 (0.03)\n0.12 (0)\n0.01 (0)\n0.21 (0.07)\n\n\nHigh ICI\n0.14 (0)\n0.09 (0)\n0.48 (0.08)\n0.13 (0)\n0.07 (0)\n1.85 (0.11)\n0.13 (0)\n0.01 (0)\n0.23 (0.07)\n\n\n3\n0 noise variables\nSmallest\n0.24 (0)\n0.07 (0.01)\n1.83 (0.1)\n0.23 (0)\n0.01 (0)\n1.65 (0.21)\n0.23 (0)\n0.01 (0)\n1.67 (0.21)\n\n\nLargest\n0.23 (0)\n0.04 (0)\n0.11 (0.03)\n0.23 (0)\n0.01 (0)\n0.09 (0.02)\n0.23 (0)\n0.01 (0)\n0.29 (0.1)\n\n\nAUC*\n0.22 (0)\n0.01 (0)\n0.01 (0.01)\n0.22 (0)\n0.01 (0)\n0.04 (0.01)\n0.22 (0)\n0.01 (0)\n0.28 (0.11)\n\n\nKL*\n0.22 (0)\n0.01 (0)\n0.01 (0)\n0.22 (0)\n0.01 (0)\n0.05 (0.01)\n0.22 (0)\n0.01 (0)\n0.29 (0.11)\n\n\nHigh ICI\n0.25 (0)\n0.11 (0)\n0.63 (0.09)\n0.23 (0)\n0.01 (0)\n0.27 (0.05)\n0.23 (0)\n0.01 (0)\n0.37 (0.11)\n\n\n10 noise variables\nSmallest\n0.24 (0)\n0.07 (0.01)\n1.83 (0.1)\n0.23 (0)\n0.01 (0)\n1.65 (0.21)\n0.23 (0)\n0.01 (0)\n1.67 (0.21)\n\n\nLargest\n0.23 (0)\n0.05 (0)\n0.11 (0.02)\n0.23 (0)\n0.01 (0)\n0.12 (0.03)\n0.23 (0)\n0.01 (0)\n0.31 (0.1)\n\n\nAUC*\n0.22 (0)\n0.01 (0)\n0.03 (0.02)\n0.22 (0)\n0.01 (0)\n0.05 (0.01)\n0.22 (0)\n0.01 (0)\n0.29 (0.1)\n\n\nKL*\n0.22 (0)\n0.01 (0.01)\n0 (0)\n0.22 (0)\n0.01 (0)\n0.06 (0.02)\n0.22 (0)\n0.01 (0)\n0.28 (0.1)\n\n\nHigh ICI\n0.25 (0)\n0.13 (0.01)\n0.93 (0.11)\n0.23 (0)\n0.01 (0)\n0.37 (0.03)\n0.23 (0)\n0.01 (0)\n0.38 (0.11)\n\n\n50 noise variables\nSmallest\n0.24 (0)\n0.07 (0.01)\n1.83 (0.1)\n0.23 (0)\n0.01 (0)\n1.65 (0.21)\n0.23 (0)\n0.01 (0)\n1.67 (0.21)\n\n\nLargest\n0.23 (0)\n0.06 (0.01)\n0.12 (0.02)\n0.23 (0)\n0.01 (0)\n0.16 (0.03)\n0.23 (0)\n0.01 (0)\n0.34 (0.1)\n\n\nAUC*\n0.22 (0)\n0.01 (0.01)\n0.05 (0.03)\n0.22 (0)\n0.01 (0)\n0.05 (0.01)\n0.22 (0)\n0.01 (0)\n0.27 (0.12)\n\n\nKL*\n0.22 (0)\n0.02 (0.01)\n0 (0)\n0.22 (0)\n0.01 (0)\n0.07 (0.02)\n0.22 (0)\n0.01 (0)\n0.28 (0.1)\n\n\nHigh ICI\n0.26 (0)\n0.14 (0.01)\n1.29 (0.17)\n0.23 (0)\n0.02 (0)\n0.4 (0.03)\n0.23 (0)\n0.01 (0)\n0.42 (0.11)\n\n\n100 noise variables\nSmallest\n0.24 (0)\n0.07 (0.01)\n1.83 (0.1)\n0.23 (0)\n0.01 (0)\n1.65 (0.21)\n0.23 (0)\n0.01 (0)\n1.67 (0.21)\n\n\nLargest\n0.23 (0)\n0.06 (0)\n0.13 (0.03)\n0.23 (0)\n0.01 (0)\n0.18 (0.03)\n0.23 (0)\n0.01 (0)\n0.35 (0.12)\n\n\nAUC*\n0.22 (0)\n0.01 (0.01)\n0.07 (0.03)\n0.22 (0)\n0.01 (0)\n0.05 (0.02)\n0.22 (0)\n0.01 (0)\n0.29 (0.11)\n\n\nKL*\n0.22 (0)\n0.02 (0)\n0 (0)\n0.22 (0)\n0.01 (0)\n0.08 (0.02)\n0.22 (0)\n0.01 (0)\n0.3 (0.12)\n\n\nHigh ICI\n0.26 (0)\n0.15 (0.01)\n1.45 (0.18)\n0.23 (0)\n0.02 (0)\n0.42 (0.04)\n0.23 (0)\n0.01 (0)\n0.43 (0.11)\n\n\n4\n0 noise variables\nSmallest\n0.24 (0)\n0.08 (0.01)\n2.37 (0.3)\n0.23 (0)\n0.01 (0)\n2.06 (0.11)\n0.23 (0)\n0.01 (0)\n2.06 (0.11)\n\n\nLargest\n0.21 (0)\n0.03 (0)\n0.02 (0.01)\n0.21 (0)\n0.01 (0)\n0.12 (0.02)\n0.21 (0)\n0.01 (0)\n0.32 (0.12)\n\n\nAUC*\n0.2 (0)\n0.01 (0)\n0.04 (0.02)\n0.2 (0)\n0.02 (0)\n0.13 (0.02)\n0.2 (0)\n0.01 (0)\n0.31 (0.1)\n\n\nKL*\n0.21 (0)\n0.02 (0)\n0.01 (0)\n0.21 (0)\n0.01 (0)\n0.12 (0.01)\n0.21 (0)\n0.01 (0)\n0.29 (0.1)\n\n\nHigh ICI\n0.22 (0)\n0.07 (0.01)\n0.18 (0.2)\n0.22 (0)\n0.02 (0)\n0.24 (0.13)\n0.22 (0)\n0.01 (0)\n0.36 (0.19)\n\n\n10 noise variables\nSmallest\n0.24 (0)\n0.08 (0.01)\n2.37 (0.3)\n0.23 (0)\n0.01 (0)\n2.06 (0.11)\n0.23 (0)\n0.01 (0)\n2.06 (0.11)\n\n\nLargest\n0.21 (0)\n0.04 (0.01)\n0.02 (0.01)\n0.21 (0)\n0.01 (0)\n0.17 (0.02)\n0.21 (0)\n0.01 (0)\n0.31 (0.11)\n\n\nAUC*\n0.21 (0)\n0.01 (0)\n0.09 (0.03)\n0.21 (0)\n0.02 (0)\n0.14 (0.02)\n0.21 (0)\n0.01 (0)\n0.3 (0.11)\n\n\nKL*\n0.21 (0)\n0.03 (0.01)\n0.01 (0)\n0.21 (0)\n0.01 (0)\n0.16 (0.02)\n0.21 (0)\n0.01 (0)\n0.31 (0.09)\n\n\nHigh ICI\n0.23 (0)\n0.11 (0)\n0.32 (0.28)\n0.22 (0)\n0.02 (0)\n0.4 (0.13)\n0.22 (0)\n0.01 (0)\n0.43 (0.21)\n\n\n50 noise variables\nSmallest\n0.24 (0)\n0.08 (0.01)\n2.37 (0.3)\n0.23 (0)\n0.01 (0)\n2.06 (0.11)\n0.23 (0)\n0.01 (0)\n2.06 (0.11)\n\n\nLargest\n0.22 (0)\n0.04 (0.01)\n0.02 (0.01)\n0.21 (0)\n0.01 (0)\n0.21 (0.02)\n0.21 (0)\n0.01 (0)\n0.33 (0.1)\n\n\nAUC*\n0.21 (0)\n0.02 (0.01)\n0.13 (0.03)\n0.21 (0)\n0.02 (0)\n0.15 (0.02)\n0.21 (0)\n0.01 (0)\n0.29 (0.11)\n\n\nKL*\n0.21 (0)\n0.03 (0.01)\n0.02 (0)\n0.21 (0)\n0.01 (0)\n0.19 (0.02)\n0.21 (0)\n0.01 (0)\n0.35 (0.1)\n\n\nHigh ICI\n0.24 (0)\n0.13 (0.01)\n0.42 (0.05)\n0.22 (0)\n0.02 (0)\n0.42 (0.04)\n0.22 (0)\n0.01 (0)\n0.38 (0.11)\n\n\n100 noise variables\nSmallest\n0.24 (0)\n0.08 (0.01)\n2.37 (0.3)\n0.23 (0)\n0.01 (0)\n2.06 (0.11)\n0.23 (0)\n0.01 (0)\n2.06 (0.11)\n\n\nLargest\n0.22 (0)\n0.05 (0.01)\n0.02 (0.01)\n0.22 (0)\n0.01 (0)\n0.22 (0.02)\n0.22 (0)\n0.01 (0)\n0.36 (0.11)\n\n\nAUC*\n0.21 (0)\n0.02 (0.01)\n0.14 (0.03)\n0.21 (0)\n0.02 (0)\n0.15 (0.02)\n0.21 (0)\n0.01 (0)\n0.33 (0.1)\n\n\nKL*\n0.22 (0)\n0.04 (0.01)\n0.02 (0)\n0.21 (0)\n0.01 (0)\n0.2 (0.02)\n0.22 (0)\n0.01 (0)\n0.37 (0.11)\n\n\nHigh ICI\n0.24 (0)\n0.13 (0.01)\n0.49 (0.05)\n0.22 (0)\n0.03 (0)\n0.47 (0.06)\n0.22 (0)\n0.01 (0)\n0.4 (0.1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5.5.5 Before vs. After Recalibration\nLet us visualize how the KL divergence and the ICI of a selected model changes after the scores are recalibrated, either using Platt scaling or isotonic regression.\n\n\nCode\ntable_models_interest_mean &lt;- \n  table_models_interest_mean |&gt; \n  mutate(\n    no_noise = fct_recode(no_noise, `0 noise variable` = \"0 noise variables\"),\n    dgp = str_c(\"DGP \", dgp)\n  )\n\ninitial_points &lt;- table_models_interest_mean |&gt; \n  filter(recalib == \"None\")\n\npoints_after_c &lt;- initial_points |&gt; \n  select(dgp, no_noise, result_type, ici, kl) |&gt; \n  left_join(\n    table_models_interest_mean |&gt; \n      filter(recalib %in% c(\"Platt\", \"Isotonic\")) |&gt; \n      select(dgp, no_noise, recalib, result_type, ici, kl) |&gt; \n      rename(ici_end = ici, kl_end = kl),\n    relationship = \"many-to-many\" # (Platt and Isotonic)\n  )\n\n\nJoining with `by = join_by(dgp, no_noise, result_type)`\n\n\nCode\np_kl_vs_ici_recalib &lt;- \n  ggplot() +\n  geom_point(\n    data = initial_points,\n    mapping = aes(x = ici, y = kl, shape = result_type),\n    size = 2\n  ) +\n  geom_segment(\n    data = points_after_c,\n    mapping = aes(\n      x = ici, y = kl, xend = ici_end, yend = kl_end,\n      colour = recalib, linetype = recalib\n    ),\n    arrow = arrow(length=unit(.2, 'cm'))\n  ) +\n  facet_grid(dgp~no_noise, scales = \"free\") +\n  scale_shape_manual(\n    NULL,\n    values = c(\n      # \"Smallest\" = 1,\n      \"AUC*\" = 19,\n      \"KL*\" = 15,\n      \"High ICI\" = 17\n    )\n  ) +\n  labs(x = \"ICI\", y = \"KL Divergence\") +\n  scale_colour_manual(\"Recalibration\", values = colour_recalib) +\n  scale_linetype_discrete(\"Recalibration\") +\n  theme_paper()\n\nggsave(p_kl_vs_ici_recalib, file = \"../figs/kl_vs_ici_recalib.pdf\", width = 12, height = 8)\np_kl_vs_ici_recalib\n\n\n\n\n\nFigure 5.19: Average KL divergence and ICI before and after recalibration of the estimated scores for DGP 1, for selected models.\n\n\n\n\n\n\n\n\n\n\n\n\nOjeda, Francisco M., Max L. Jansen, Alexandre Thiéry, Stefan Blankenberg, Christian Weimar, Matthias Schmid, and Andreas Ziegler. 2023. “Calibrating Machine Learning Approaches for Probability Estimation: A Comprehensive Comparison.” Statistics in Medicine 42 (29): 5451–78. https://doi.org/10.1002/sim.9921.",
    "crumbs": [
      "III. Simulated Data",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Extreme Gradient Boosting</span>"
    ]
  },
  {
    "objectID": "example_spambase_beta.html",
    "href": "example_spambase_beta.html",
    "title": "6  Prior Distribution: Example",
    "section": "",
    "text": "6.1 Raw Data\nTo illustrate the process, we use the spambase dataset (available on UCI Machine Learning Repository). The dataset contains 4,601 rows. The target variable, is_spam will be explained using the 57 continuous predictors.\nThe dataset can be downloaded as follows:\nif (!dir.exists(\"../data\")) dir.create(\"../data\")\ndownload.file(\n  url = \"https://archive.ics.uci.edu/static/public/94/spambase.zip\", \n  destfile = \"../data/spambase.zip\"\n)\nThe names of the columns are given in the spambase.names file in that archive.\n# This chunk is not run\ninfo_data &lt;- scan(\n  unz(\"../data/spambase.zip\", \"spambase.names\"), what = \"character\", sep = \"\\n\"\n)\n# Print the names for this dataset (not very convenient...)\nstr_extract(info_data[31:length(info_data)], \"^(.*):\") |&gt; \n  str_remove(\":$\") |&gt; \n  (\\(.x) str_c('\"', .x, '\",'))() |&gt; \n  cat()\nThen, we can import the dataset:\ndataset &lt;- read_csv(\n  file = unz(\"../data/spambase.zip\", \"spambase.data\"),\n  col_names = c(\n    \"word_freq_make\", \"word_freq_address\", \"word_freq_all\", \"word_freq_3d\",\n    \"word_freq_our\", \"word_freq_over\", \"word_freq_remove\", \"word_freq_internet\",\n    \"word_freq_order\", \"word_freq_mail\", \"word_freq_receive\", \"word_freq_will\",\n    \"word_freq_people\", \"word_freq_report\", \"word_freq_addresses\", \n    \"word_freq_free\", \"word_freq_business\", \"word_freq_email\", \"word_freq_you\",\n    \"word_freq_credit\", \"word_freq_your\", \"word_freq_font\", \"word_freq_000\",\n    \"word_freq_money\", \"word_freq_hp\", \"word_freq_hpl\", \"word_freq_george\",\n    \"word_freq_650\", \"word_freq_lab\", \"word_freq_labs\", \"word_freq_telnet\",\n    \"word_freq_857\", \"word_freq_data\", \"word_freq_415\", \"word_freq_85\",\n    \"word_freq_technology\", \"word_freq_1999\", \"word_freq_parts\", \"word_freq_pm\",\n    \"word_freq_direct\", \"word_freq_cs\", \"word_freq_meeting\", \n    \"word_freq_original\", \"word_freq_project\", \"word_freq_re\", \"word_freq_edu\",\n    \"word_freq_table\", \"word_freq_conference\", \"char_freq_;\", \"char_freq_(\",\n    \"char_freq_[\", \"char_freq_!\", \"char_freq_$\", \"char_freq_#\",\n    \"capital_run_length_average\", \"capital_run_length_longest\",\n    \"capital_run_length_total\", \"is_spam\"\n  )\n)\n\nRows: 4601 Columns: 58\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (58): word_freq_make, word_freq_address, word_freq_all, word_freq_3d, wo...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nThe target variable is is_spam.\ntarget_name &lt;- \"is_spam\"",
    "crumbs": [
      "IV. Real Data",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Prior Distribution: Example</span>"
    ]
  },
  {
    "objectID": "example_spambase_beta.html#data-pre-processing",
    "href": "example_spambase_beta.html#data-pre-processing",
    "title": "6  Prior Distribution: Example",
    "section": "6.2 Data Pre-processing",
    "text": "6.2 Data Pre-processing\nWe start by splitting the dataset into train and test sets in order calculate the prior distribution.\nWith the current dataset:\n\ndata &lt;- split_train_test(data = dataset, prop_train = .8, seed = 1234)\nnames(data)\n\n[1] \"train\" \"test\" \n\n\nSome of the models we use need the data to be numerical. We thus use the function encode_dataset() that transforms the categorical columns into sets of dummy variables. For each categorical variable, we remove one of the levels to avoid colinearity in the predictor matrix. This step is made using the convenient functions from the {recipes} package. In addition, the spline function from the {gam} package does not support variables with names that do not respect the R naming conventions. We thus rename all the variables and keep track of the changes.\nLet us use the encode_dataset() function to rename the columns here. As there is no categorical variable among the predictors, no dummy variable will be created.\n\ndata_dmy &lt;- encode_dataset(\n  data_train = data$train,\n  data_test = data$test,\n  target_name = target_name,\n  intercept = FALSE\n)",
    "crumbs": [
      "IV. Real Data",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Prior Distribution: Example</span>"
    ]
  },
  {
    "objectID": "example_spambase_beta.html#estimation-functions-gamsel",
    "href": "example_spambase_beta.html#estimation-functions-gamsel",
    "title": "6  Prior Distribution: Example",
    "section": "6.3 Estimation Functions: GAMSEL",
    "text": "6.3 Estimation Functions: GAMSEL\nWe first estimate the probability that the event occurs (the email is a spam) using a Generalized Additive Model with model selection. We use the helper function train_gamsel(), which can be used in a very simple way. This function first splits the target variable and the predictors in distinct objects. Then, we check that all variables obtained after using the encode_dataset() function are coded as numeric: the estimation function from {gamsel} does not allow integer variables. The formula to fit the GAMSEL model is then built (We need to create a vector that gives the maximum spline basis function to use for each variable. For dummy variables, this needs to be set to 1. For other variables, let us use either 6 or the minimum number of distinct values minus 1.). Then, we fit the model. The penalty parameter \\(\\lambda\\) is selected by 10-fold cross validation. We use the value of lambda which gives the minimum cross validation metric. Note that we could also use the largest value of lambda such that the error is within 1 standard error of the minimum (using lambda = gamsel_cv$lambda.1se). Lastly, we get the predicted scores.\n\n\nThe train_gamsel() function.\n#' Train a GAMSEL model\n#'\n#' @param data_train train set\n#' @param data_test test set\n#' @param target_name name of the target (response) variable\n#' @param degrees degree for the splines\n#' @param return_model if TRUE, the estimated model is returned\n#'\n#' @returns list with estimated scores on train set (`scores_train`) and on\n#'  test set (`scores_test`)\ntrain_gamsel &lt;- function(data_train,\n                         data_test,\n                         target_name,\n                         degrees = 6,\n                         return_model = FALSE) {\n  # Encode dataset so that categorical variables become dummy variables\n  data_dmy &lt;- encode_dataset(\n    data_train = data_train,\n    data_test = data_test,\n    target_name = target_name,\n    intercept = FALSE\n  )\n  # Estimation\n  X_dmy_train &lt;- data_dmy$train |&gt; dplyr::select(-!!target_name)\n  X_dmy_train &lt;- X_dmy_train |&gt; mutate(across(everything(), as.numeric))\n  X_dmy_test &lt;- data_dmy$test |&gt; dplyr::select(-!!target_name)\n  X_dmy_test &lt;- X_dmy_test |&gt; mutate(across(everything(), as.numeric))\n  y_train &lt;- data_dmy$train |&gt; dplyr::pull(!!target_name)\n  y_test &lt;- data_dmy$test |&gt; dplyr::pull(!!target_name)\n\n  deg &lt;- rep(NA, ncol(X_dmy_train))\n  col_names_X &lt;- colnames(X_dmy_train)\n  nb_val &lt;- map_dbl(\n    col_names_X, ~X_dmy_train |&gt; pull(.x) |&gt; unique() |&gt; length()\n  )\n  for (i_var_name in 1:ncol(X_dmy_train)) {\n    var_name &lt;- col_names_X[i_var_name]\n    if (var_name %in% data_dmy$categ_names) {\n      deg[i_var_name] &lt;- 1\n    } else {\n      deg[i_var_name] &lt;- min(nb_val[i_var_name]-1, degrees)\n    }\n  }\n  gamsel_cv &lt;- gamsel::cv.gamsel(\n    x = as.data.frame(X_dmy_train), y = y_train, family = \"binomial\",\n    degrees = deg\n  )\n  gamsel_out &lt;- gamsel::gamsel(\n    x = as.data.frame(X_dmy_train), y = y_train, family = \"binomial\",\n    degrees = deg,\n    lambda = gamsel_cv$lambda.min\n  )\n  # Scores on train and test set\n  scores_train &lt;- predict(\n    gamsel_out, newdata = as.data.frame(X_dmy_train), type = \"response\")[, 1]\n  scores_test &lt;- predict(\n    gamsel_out, newdata = as.data.frame(X_dmy_test), type = \"response\")[, 1]\n  scores_test[which(is.na(scores_test))] &lt;-\n    1/(1 + exp(-predict(gamsel_out,\n                        newdata = as.data.frame(X_dmy_test[which(is.na(scores_test)),]))\n               [, 1]))\n\n  if (return_model == TRUE) {\n    res &lt;- list(\n      scores_train = scores_train,\n      scores_test = scores_test,\n      fit = fit)\n  } else {\n    list(scores_train = scores_train, scores_test = scores_test, fit = NULL)\n  }\n}\n\n\n\nscores_gamsel &lt;- train_gamsel(\n    data_train = data$train, data_test = data$test, target_name = target_name,\n    degrees = 6\n  )",
    "crumbs": [
      "IV. Real Data",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Prior Distribution: Example</span>"
    ]
  },
  {
    "objectID": "example_spambase_beta.html#fitting-a-beta-distribution",
    "href": "example_spambase_beta.html#fitting-a-beta-distribution",
    "title": "6  Prior Distribution: Example",
    "section": "6.4 Fitting a Beta Distribution",
    "text": "6.4 Fitting a Beta Distribution\nOnce the scores from the models have been estimated, we fit a Beta distribution to them. This will provide a prior distribution of the true probabilities in the exercise.\nTo avoid crashing the ML estimation of the two parameters of the Beta distribution, let us make sure that any score is in \\((0,1)\\) and not exactly equal to 0 or 1.\n\nx_gamsel &lt;- (scores_gamsel$scores_test * (1 - 1e-6)) + 1e-6 / 2\n\nTo estimate the two parameters of the Beta distribution, we apply the function, fit_beta_scores() that calls the fitdistr() function from {MASS}.\n\n\nThe fit_beta_scores() function.\n#' Maximum-likelihood fitting of Beta distribution on scores\n#'\n#' @param scores vector of estimated scores\n#' @param shape1 non-negative first parameter of the Beta distribution\n#' @param shape1 non-negative second parameter of the Beta distribution\n#'\n#' @returns An object of class `fitdistr`, a list with four components\n#'  (see: MASS::fitdistr())\n#'  - `estimate`: the parameter estimates\n#'  - `sd`: the estimated standard errors\n#'  - `vcov`: the estimated variance-covariance matrix\n#'  - `loglik`: the log-likelihood\nfit_beta_scores &lt;- function(scores, shape1 = 1, shape2 = 1) {\n  # Fit a beta distribution\n  mle_fit &lt;- MASS::fitdistr(\n    scores, \"beta\", start = list(shape1 = 1, shape2 = 1)\n  )\n  mle_fit\n}\n\n\n\n(mle_gamsel &lt;- fit_beta_scores(scores = x_gamsel[!is.na(x_gamsel)]))\n\n     shape1       shape2  \n  0.42746386   0.51025950 \n (0.01747823) (0.02194235)",
    "crumbs": [
      "IV. Real Data",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Prior Distribution: Example</span>"
    ]
  },
  {
    "objectID": "example_spambase_beta.html#wrapper-functions",
    "href": "example_spambase_beta.html#wrapper-functions",
    "title": "6  Prior Distribution: Example",
    "section": "6.5 Wrapper Functions",
    "text": "6.5 Wrapper Functions\nFor convenience, we use a wrapper function, get_beta_fit() that takes a dataset as an input, the name of the target variable and possibly a seed. From these arguments, the function splits the dataset into a training and a test set. It then fits the GAMSEL model to the train set, and fit a Beta distribution on the scores estimated in the test set. This function returns a list with 2 elements: the estimated scores of the GAMSEL, the the parameters of the Beta distribution estimated using the scores of this model.\n\n\nThe get_beta_fit() function\n#' Estimation of a GLM-logistic, a GAM and a GAMSEL model on a classification\n#' task. Then, on estimated scores from the test set, fits a Beta distribution.\n#'\n#' @param dataset dataset with response variable and predictors\n#' @param target_name name of the target (response) variable\n#' @param seed desired seed (default to `NULL`)\n#'\n#' @returns A list with the following elements:\n#'  - `scores_glm`: scores on train and test set (in a list) from the GLM\n#'  - `scores_gam`: scores on train and test set (in a list) from the GAM\n#'  - `scores_gamsel`: scores on train and test set (in a list) from the GAMSEL\n#'  - `mle_glm`: An object of class \"fitdistr\" for the GLM model\n#'    (see fit_beta_scores())\n#'  - `mle_gamsel`: An object of class \"fitdistr\" for the GAM model\n#'    (see fit_beta_scores())\n#'  - `mle_gamsel`: An object of class \"fitdistr\" for the GAMSEL model\n#'    (see fit_beta_scores())\nget_beta_fit &lt;- function(dataset,\n                         target_name,\n                         seed = NULL) {\n  # Split data into train/test\n  data &lt;- split_train_test(data = dataset, prop_train = .8, seed = seed)\n\n  # Train a GAMSEL model\n  scores_gamsel &lt;- train_gamsel(\n    data_train = data$train, data_test = data$test, target_name = target_name,\n    degrees = 6\n  )\n  # Add a little noise to the estimated scores to avoid being in [0,1] and be\n  # in (0,1) instead.\n  x_gamsel &lt;- (scores_gamsel$scores_test * (1 - 1e-6)) + 1e-6 / 2\n  # Fit a Beta distribution on these scores\n  mle_gamsel &lt;- fit_beta_scores(scores = x_gamsel[!is.nan(x_gamsel)])\n\n  list(\n    scores_gamsel = scores_gamsel,\n    mle_gamsel = mle_gamsel\n  )\n}\n\n\nThese two functions can be called as follows:\n\nresul &lt;- get_beta_fit(dataset = dataset, target_name = \"is_spam\", seed = 1234)\n\nWe also use the function, plot_hist_scores_beta() to plot the distribution of scores obtained with the GAMSEL model and the density functions of the Beta distribution whose parameters were estimated based on the scores of the GAMSEL model.\n\nplot_hist_scores_beta(resul, \"spambase\")",
    "crumbs": [
      "IV. Real Data",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Prior Distribution: Example</span>"
    ]
  },
  {
    "objectID": "book_real_beta.html",
    "href": "book_real_beta.html",
    "title": "7  Datasets and Priors",
    "section": "",
    "text": "7.1 Datasets\nAll the datasets used here are from the UCI Machine Learning Repository.",
    "crumbs": [
      "IV. Real Data",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Datasets and Priors</span>"
    ]
  },
  {
    "objectID": "book_real_beta.html#datasets",
    "href": "book_real_beta.html#datasets",
    "title": "7  Datasets and Priors",
    "section": "",
    "text": "7.1.1 Abalone\n\nURL to the data: https://archive.ics.uci.edu/dataset/1/abalone\nDescription: Predict the age of abalone from physical measurements.\nNumber of instances: 4,177\nFeatures: 8\nReference: Nash et al. (1995)\n\n\nname &lt;- \"abalone\"\n\nThe dataset needs to be download.\n\n\nCode to download the data\nif (!dir.exists(\"../data\")) dir.create(\"../data\")\ndownload.file(\n  url = str_c(\"https://archive.ics.uci.edu/static/public/1/\", name, \".zip\"), \n  destfile = str_c(\"../data/\", name, \".zip\")\n)\n\n\nThen, we can import the dataset:\n\n\nCode to import the data\ntb_abalone &lt;- read_csv(\n  file = unz(str_c(\"../data/\", name, \".zip\"), str_c(name, \".data\")), \n  col_names = c(\n    \"Sex\", \"Length\", \"Diameter\", \"Height\", \"Whole_weight\", \n  \"Shucked_weight\", \"Viscera_weight\", \"Shell_weight\", \"Rings\"),\n  show_col_types = FALSE\n)\n\n\nThe target variable is sex. Let us turn it in a \\(\\{0,1\\}\\) variable.\n\ntb_abalone &lt;- tb_abalone |&gt; \n  mutate(Sex = ifelse(Sex == \"M\", 1, 0)) \ntarget_name &lt;- \"Sex\"\n\nLet us call the get_beta_fit() from Chapter 6 to get our priors.\n\npriors_abalone &lt;- get_beta_fit(\n  dataset = tb_abalone, target_name = target_name, seed = 1234\n)\n\nLet us save the results and the dataset:\n\nsave(priors_abalone, file = \"../output/real-data/priors_abalone.rda\")\nsave(tb_abalone, file = \"../output/real-data/tb_abalone.rda\")\n\n\nplot_hist_scores_beta(priors_abalone, \"abalone\")\n\n\n\n\nFigure 7.1: Distribution of estimated probabilities by the GAMSEL model and Beta distribution fitted to the scores of each of the three models, for the Abalone dataset.\n\n\n\n\n\n\n\n\n\n\n7.1.2 Adult\n\nURL to the data: https://archive.ics.uci.edu/dataset/2/adult\nDescription: Predict whether income exceeds $50K/yr based on census data. Also known as “Census Income” dataset.\nNumber of instances: 48,842\nFeatures: 14\nReference: Becker and Kohavi (1996)\n\n\nname &lt;- \"adult\"\n\nThe dataset needs to be download.\n\n\nCode to download the data\nif (!dir.exists(\"../data\")) dir.create(\"../data\")\ndownload.file(\n  url = str_c(\"https://archive.ics.uci.edu/static/public/2/\", name, \".zip\"), \n  destfile = str_c(\"../data/\", name, \".zip\")\n)\n\ninfo_data &lt;- scan(\n  unz(str_c(\"../data/\", name, \".zip\"), str_c(name, \".names\")), \n  what = \"character\", sep = \"\\n\"\n)\n# Print the names for this dataset (not very convenient...)\nstr_extract(info_data[94:length(info_data)], \"^(.*):\") |&gt; \n  str_remove(\":$\") |&gt; \n  (\\(.x) str_c('\"', .x, '\",'))() |&gt; \n  cat()\n\n\nThen, we can import the dataset:\n\n\nCode to import the data\ntb_adult &lt;- read_csv(\n  file = unz(str_c(\"../data/\", name, \".zip\"), str_c(name, \".data\")), \n  col_names = c(\n    \"age\", \"workclass\", \"fnlwgt\", \"education\", \"education-num\",\n    \"marital-status\", \"occupation\", \"relationship\", \"race\", \"sex\",\n    \"capital-gain\", \"capital-loss\", \"hours-per-week\", \"native-country\",\n    \"income\"\n  ),\n  show_col_types = FALSE\n)\n\n\nThe target variable is income. Let us turn it in a \\(\\{0,1\\}\\) variable and call it high_income.\n\ntb_adult &lt;- tb_adult |&gt; \n  mutate(high_income = ifelse(income == \"&gt;50K\", 1, 0)) |&gt; \n  dplyr::select(-income)\ntarget_name &lt;- \"high_income\"\n\nLet us call the get_beta_fit() from Chapter 6 to get our priors.\n\npriors_adult &lt;- get_beta_fit(\n  dataset = tb_adult, target_name = target_name, seed = 1234\n)\n\nLet us save the results and the dataset:\n\nsave(priors_adult, file = \"../output/real-data/priors_adult.rda\")\nsave(tb_adult, file = \"../output/real-data/tb_adult.rda\")\n\n\nplot_hist_scores_beta(priors_adult, \"adult\")\n\n\n\n\nFigure 7.2: Distribution of estimated probabilities by the GAMSEL model and Beta distribution fitted to the scores of each of the three models, for the Adult dataset.\n\n\n\n\n\n\n\n\n\n\n7.1.3 Bank Marketing\n\nURL to the data: https://archive.ics.uci.edu/dataset/222/bank+marketing\nDescription: The data is related with direct marketing campaigns (phone calls) of a Portuguese banking institution. The classification goal is to predict if the client will subscribe a term deposit (variable y).\nNumber of instances: 45,211\nFeatures: 16\nReference: Moro, Rita, and Cortez (2012)\n\n\nname &lt;- \"bank\"\n\nThe dataset needs to be download.\n\n\nCode to download the data\nif (!dir.exists(\"../data\")) dir.create(\"../data\")\ndownload.file(\n  url = \"https://archive.ics.uci.edu/static/public/222/bank+marketing.zip\", \n  destfile = str_c(\"data/\", name, \".zip\")\n)\n\n\nThen, we can import the dataset:\n\n\nCode to import the data\ndir.create(\"../data/bank/\")\nsystem(\"unzip ../data/bank.zip -d ../data/bank/\")\nsystem(\"unzip ../data/bank/bank.zip -d ../data/bank/\")\ntb_bank &lt;- read_csv2(\n  file = unz(str_c(\"../data/bank/\", name, \".zip\"), str_c(\"bank-full.csv\")), \n  skip = 1,\n  col_names = c(\n    \"age\", \"job\", \"marital\", \"education\", \"default\", \"balance\", \"housing\", \n    \"loan\", \"contact\", \"day\", \"month\", \"duration\", \"campaign\", \"pdays\", \n    \"previous\", \"poutcome\", \"y\"\n  ),\n  show_col_types = FALSE\n)\n\n\nℹ Using \"','\" as decimal and \"'.'\" as grouping mark. Use `read_delim()` for more control.\n\n\nCode to import the data\nsystem(\"rm -rf ../data/bank/\")\n\n\nThe target variable is y (whether the client will subscribe a term deposit). Let us turn it in a \\(\\{0,1\\}\\) variable.\n\ntb_bank &lt;- tb_bank |&gt; \n  mutate(y = ifelse(y == \"yes\", 1, 0)) \ntarget_name &lt;- \"y\"\n\nLet us call the get_beta_fit() from Chapter 6 to get our priors.\n\npriors_bank &lt;- get_beta_fit(\n  dataset = tb_bank, target_name = target_name, seed = 1234\n)\n\nLet us save the results and the dataset:\n\nsave(priors_bank, file = \"../output/real-data/priors_bank.rda\")\nsave(tb_bank, file = \"../output/real-data/tb_bank.rda\")\n\n\nplot_hist_scores_beta(priors_bank, \"bank\")\n\n\n\n\nFigure 7.3: Distribution of estimated probabilities by the GAMSEL model and Beta distribution fitted to the scores of each of the three models, for the Bank Marketing dataset.\n\n\n\n\n\n\n\n\n\n\n7.1.4 Default of Credit Card Clients\n\nURL to the data: https://archive.ics.uci.edu/dataset/350/default+of+credit+card+clients\nDescription: This research aimed at the case of customers’ default payments in Taiwan and compares the predictive accuracy of probability of default among six data mining methods.\nNumber of instances: 30,000\nFeatures: 23\nReference: Yeh (2016)\n\n\nname &lt;- \"default\"\n\nThe dataset needs to be download.\n\n\nCode to download the data\nif (!dir.exists(\"../data\")) dir.create(\"../data\")\ndownload.file(\n  url = str_c(\"https://archive.ics.uci.edu/static/public/350/\",\n              \"default+of+credit+card+clients.zip\"\n  ), \n  destfile = str_c(\"../data/\", name, \".zip\")\n)\n\n\nThen, we can import the dataset:\n\n\nCode to import the data\ndir.create(\"../data/default/\")\nsystem(\"unzip ../data/default.zip -d ../data/default/\")\ntb_default &lt;- readxl::read_excel(\n  path = \"../data/default/default of credit card clients.xls\",\n  skip = 1\n) |&gt; \n  select(-ID)\nsystem(\"rm -rf ../data/default\")\n\n\nThe target variable is defalut (1 if default, 0 otherwise).\n\ntb_default &lt;- \n  tb_default |&gt; \n  mutate(\n    across(all_of(c(\n      \"SEX\", \"EDUCATION\", \"MARRIAGE\", \n      \"PAY_0\", \"PAY_2\", \"PAY_3\", \"PAY_4\", \"PAY_5\", \"PAY_6\")), as.factor)\n  ) |&gt; \n  mutate(\n    across(all_of(c(\n      \"EDUCATION\", \"PAY_0\", \"PAY_2\", \"PAY_3\", \"PAY_4\", \"PAY_5\", \"PAY_6\"\n    )), ~fct_lump(.x, prop = .05)\n    )\n  ) |&gt; \n  rename(default = `default payment next month`)\ntarget_name &lt;- \"default\"\n\nLet us call the get_beta_fit() from Chapter 6 to get our priors.\n\npriors_default &lt;- get_beta_fit(\n  dataset = tb_default, target_name = target_name, seed = 1234\n)\n\nLet us save the results and the dataset:\n\nsave(priors_default, file = \"../output/real-data/priors_default.rda\")\nsave(tb_default, file = \"../output/real-data/tb_default.rda\")\n\n\nplot_hist_scores_beta(priors_default, \"default\")\n\n\n\n\nFigure 7.4: Distribution of estimated probabilities by the GAMSEL model and Beta distribution fitted to the scores of each of the three models, for the Default of Credit Card Clients dataset.\n\n\n\n\n\n\n\n\n\n\n7.1.5 Dry Bean\n\nURL to the data: https://archive.ics.uci.edu/dataset/602/dry+bean+dataset\nDescription: Images of 13,611 grains of 7 different registered dry beans were taken with a high-resolution camera. A total of 16 features; 12 dimensions and 4 shape forms, were obtained from the grains.\nNumber of instances: 13,611\nFeatures: 16\nReferences: “Dry Bean” (2020)\n\n\nname &lt;- \"drybean\"\n\nThe dataset needs to be download.\n\n\nCode to download the data\nif (!dir.exists(\"../data\")) dir.create(\"../data\")\ndownload.file(\n  url = \"https://archive.ics.uci.edu/static/public/602/dry+bean+dataset.zip\", \n  destfile = str_c(\"../data/\", name, \".zip\")\n)\n\n\nThen, we can import the dataset:\n\n\nCode to import the data\ndir.create(\"../data/drybean/\")\nsystem(\"unzip ../data/drybean.zip -d ../data/drybean/\")\ntb_drybean &lt;- readxl::read_excel(\n  path = \"../data/drybean/DryBeanDataset/Dry_Bean_Dataset.xlsx\"\n)\nsystem(\"rm -rf ../data/drybean\")\n\n\nThe target variable is sex. Let us turn it in a \\(\\{0,1\\}\\) variable.\n\ntb_drybean &lt;- tb_drybean |&gt; \n  mutate(is_dermason = ifelse(Class == \"DERMASON\", 1, 0)) |&gt; \n  select(-Class)\ntarget_name &lt;- \"is_dermason\"\n\nLet us call the get_beta_fit() from Chapter 6 to get our priors.\n\npriors_drybean &lt;- get_beta_fit(\n  dataset = tb_drybean, target_name = target_name, seed = 1234\n)\n\nLet us save the results and the dataset:\n\nsave(priors_drybean, file = \"../output/real-data/priors_drybean.rda\")\nsave(tb_drybean, file = \"../output/real-data/tb_drybean.rda\")\n\n\nplot_hist_scores_beta(priors_drybean, \"drybean\")\n\n\n\n\nFigure 7.5: Distribution of estimated probabilities by the GAMSEL model and Beta distribution fitted to the scores of each of the three models, for the Dry Bean dataset.\n\n\n\n\n\n\n\n\n\n\n7.1.6 In-Vehicle Coupon Recommendation\n\nURL to the data: https://archive.ics.uci.edu/dataset/603/in+vehicle+coupon+recommendation\nDescription: This data studies whether a person will accept the coupon recommended to him in different driving scenarios.\nNumber of instances: 12,684\nFeatures: 25\nReferences: “In-Vehicle Coupon Recommendation” (2020)\n\n\nname &lt;- \"coupon\"\n\nThe dataset needs to be download.\n\n\nCode to download the data\nif (!dir.exists(\"../data\")) dir.create(\"../data\")\ndownload.file(\n  url = str_c(\"https://archive.ics.uci.edu/static/public/603/\", \n              \"in+vehicle+coupon+recommendation.zip\"), \n  destfile = str_c(\"../data/\", name, \".zip\")\n)\n\n\nThen, we can import the dataset:\n\n\nCode to import the data\ntb_coupon &lt;- read_csv(\n  file = unz(str_c(\"../data/\", name, \".zip\"), \"in-vehicle-coupon-recommendation.csv\"),\n  show_col_types = FALSE\n)\n\n\nThe target variable is y (1 if the person accepted the coupon, 0 otherwise).\n\ntb_coupon &lt;- \n  tb_coupon |&gt; \n  mutate(\n    temperature = as.factor(temperature),\n    has_children = as.factor(has_children),\n    toCoupon_GEQ15min = as.factor(toCoupon_GEQ15min),\n    toCoupon_GEQ25min = as.factor(toCoupon_GEQ25min),\n    direction_same = as.factor(direction_same)\n  ) |&gt; \n  select(-toCoupon_GEQ5min, -direction_opp, -car) |&gt; \n  rename(y = Y)\n\ntb_coupon &lt;- na.omit(tb_coupon)\n\ntarget_name &lt;- \"y\"\n\nLet us call the get_beta_fit() from Chapter 6 to get our priors.\n\npriors_coupon &lt;- get_beta_fit(\n  dataset = tb_coupon, target_name = target_name, seed = 1234\n)\n\nLet us save the results and the dataset:\n\nsave(priors_coupon, file = \"../output/real-data/priors_coupon.rda\")\nsave(tb_coupon, file = \"../output/real-data/tb_coupon.rda\")\n\n\nplot_hist_scores_beta(priors_coupon, \"coupon\")\n\n\n\n\nFigure 7.6: Distribution of estimated probabilities by the GAMSEL model and Beta distribution fitted to the scores of each of the three models, for the In-Vehicle Coupon Recommendation dataset.\n\n\n\n\n\n\n\n\n\n\n7.1.7 Mushroom\n\nURL to the data: https://archive.ics.uci.edu/dataset/73/mushroom\nDescription: From Audobon Society Field Guide; mushrooms described in terms of physical characteristics; classification: poisonous or edible.\nNumber of instances: 8,124\nFeatures: 22\nReferences: “Mushroom” (1987)\n\n\nname &lt;- \"mushroom\"\n\nThe dataset needs to be download.\n\n\nCode to download the data\nif (!dir.exists(\"../data\")) dir.create(\"../data\")\ndownload.file(\n  url = str_c(\"https://archive.ics.uci.edu/static/public/73/mushroom.zip\"), \n  destfile = str_c(\"../data/\", name, \".zip\")\n)\n\n\nThen, we can import the dataset:\n\n\nCode to import the data\ntb_mushroom &lt;- read_csv(\n  file = unz(str_c(\"../data/\", name, \".zip\"), \"agaricus-lepiota.data\"), \n  col_names = c(\n    \"edible\",\n    \"cap_shape\", \"cap_surface\", \"cap_color\", \"bruises\", \"odor\", \n    \"gill_attachment\", \"gill_spacing\", \"gill_size\", \"gill_color\", \n    \"stalk_shape\", \"stalk_root\", \"stalk_surface_above_ring\",\n    \"stalk_surface_below_ring\", \"stalk_color_above_ring\", \n    \"stalk_color_below_ring\", \"veil_type\", \"veil_color\", \"ring_number\", \n    \"ring_type\", \"spore_print_color\", \"population\", \"habitat\"\n  ),\n  show_col_types = FALSE\n)\n\n\nThe target variable is edible. Let us turn it in a \\(\\{0,1\\}\\) variable.\n\ntb_mushroom &lt;- tb_mushroom |&gt; \n  mutate(bruises = ifelse(bruises == TRUE, \"yes\", \"no\")) |&gt; \n  mutate(edible = ifelse(edible == \"e\", 1, 0)) |&gt; \n  select(-veil_type)\ntarget_name &lt;- \"edible\"\n\nLet us call the get_beta_fit() from Chapter 6 to get our priors.\n\npriors_mushroom &lt;- get_beta_fit(\n  dataset = tb_mushroom, target_name = target_name, seed = 1234\n)\n\nLet us save the results and the dataset:\n\nsave(priors_mushroom, file = \"../output/real-data/priors_mushroom.rda\")\nsave(tb_mushroom, file = \"../output/real-data/tb_mushroom.rda\")\n\n\nplot_hist_scores_beta(priors_mushroom, \"mushroom\")\n\n\n\n\nFigure 7.7: Distribution of estimated probabilities by the GAMSEL model and Beta distribution fitted to the scores of each of the three models, for the Mushroom dataset.\n\n\n\n\n\n\n\n\n\n\n7.1.8 Occupancy Detection\n\nURL to the data: https://archive.ics.uci.edu/dataset/357/occupancy+detection\nDescription: Predict the age of occupancy from physical measurements.\nNumber of instances: 20,560\nFeatures: 6\nReferences: Candanedo (2016)\n\n\nname &lt;- \"occupancy\"\n\nThe dataset needs to be download.\n\n\nCode to download the data\nif (!dir.exists(\"../data\")) dir.create(\"../data\")\ndownload.file(\n  url = str_c(\"https://archive.ics.uci.edu/static/public/357/\",\n              \"occupancy+detection.zip\"), \n  destfile = str_c(\"../data/\", name, \".zip\")\n)\n\n\nThen, we can import the dataset:\n\n\nCode to import the data\ntb_occupancy &lt;- read_csv(\n  file = unz(str_c(\"../data/\", name, \".zip\"), \"datatraining.txt\"), \n  col_names = c(\n    \"id\", \"date\",\"Temperature\",\"Humidity\",\"Light\",\"CO2\",\n    \"HumidityRatio\",\"Occupancy\"\n  ),\n  show_col_types = FALSE, skip = 1\n) |&gt; \n  bind_rows(\n    read_csv(\n      file = unz(str_c(\"../data/\", name, \".zip\"), \"datatest.txt\"), \n      col_names = c(\n        \"id\", \"date\",\"Temperature\",\"Humidity\",\"Light\",\"CO2\",\n        \"HumidityRatio\",\"Occupancy\"\n      ),\n      show_col_types = FALSE, skip = 1,\n    )\n  ) |&gt; \n  bind_rows(\n    read_csv(\n      file = unz(str_c(\"../data/\", name, \".zip\"), \"datatest2.txt\"), \n      show_col_types = FALSE, skip = 1,\n      col_names = c(\n        \"id\", \"date\",\"Temperature\",\"Humidity\",\"Light\",\"CO2\",\n        \"HumidityRatio\",\"Occupancy\"\n      ),\n    )\n  ) |&gt; \n  select(-id)\n\n\nThe target variable is Occupancy.\n\ntb_occupancy &lt;- tb_occupancy |&gt; \n  select(-date)\ntarget_name &lt;- \"Occupancy\"\n\nLet us call the get_beta_fit() from Chapter 6 to get our priors.\n\npriors_occupancy &lt;- get_beta_fit(\n  dataset = tb_occupancy, target_name = target_name, seed = 1234\n)\n\nLet us save the results and the dataset:\n\nsave(priors_occupancy, file = \"../output/real-data/priors_occupancy.rda\")\nsave(tb_occupancy, file = \"../output/real-data/tb_occupancy.rda\")\n\n\nplot_hist_scores_beta(priors_occupancy, \"occupancy\")\n\n\n\n\nFigure 7.8: Distribution of estimated probabilities by the GAMSEL model and Beta distribution fitted to the scores of each of the three models, for the Occupancy Detection dataset.\n\n\n\n\n\n\n\n\n\n\n7.1.9 Wine Quality\n\nURL to the data: https://archive.ics.uci.edu/dataset/186/wine+quality\nDescription: Two datasets are included, related to red and white vinho verde wine samples, from the north of Portugal. The goal is to model wine quality based on physicochemical tests (see [Cortez et al., 2009], http://www3.dsi.uminho.pt/pcortez/wine/).\nNumber of instances: 4,898\nFeatures: 11\nReferences: Cortez et al. (2009)\n\n\nname &lt;- \"winequality\"\n\nThe dataset needs to be download.\n\n\nCode to download the data\nif (!dir.exists(\"../data\")) dir.create(\"../data\")\ndownload.file(\n  url = str_c(\"https://archive.ics.uci.edu/static/public/186/\",\n              \"wine+quality.zip\"), \n  destfile = str_c(\"../data/\", name, \".zip\")\n)\n\n\nThen, we can import the dataset:\n\n\nCode to import the data\nred_wine &lt;- read_csv2(\n  file = unz(str_c(\"../data/\", name, \".zip\"), \"winequality-red.csv\"),\n  show_col_types = FALSE) |&gt;\n  mutate(wine_type = \"red\")\n\n\nℹ Using \"','\" as decimal and \"'.'\" as grouping mark. Use `read_delim()` for more control.\n\n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\n\n\nCode to import the data\nwhite_wine &lt;- read_csv2(\n  file = unz(str_c(\"../data/\", name, \".zip\"), \"winequality-white.csv\"),\n  show_col_types = FALSE) |&gt; \n  mutate(wine_type = \"white\") |&gt; \n  mutate(`residual sugar` = as.numeric(`residual sugar`))\n\n\nℹ Using \"','\" as decimal and \"'.'\" as grouping mark. Use `read_delim()` for more control.\n\n\nThe target variable is quality. Let us use it to define a \\(\\{0,1\\}\\) variable. We define the variable high_quality which equals 1 if the quality is larger or equal than 6.\n\ntb_winequality &lt;- red_wine |&gt; bind_rows(white_wine) |&gt; \n  mutate(high_quality = ifelse(quality &gt;= 6, 1, 0)) |&gt; \n  mutate(across(all_of(c(\n    \"density\", \"chlorides\", \"volatile acidity\", \"sulphates\", \"citric acid\"\n    )), ~as.numeric(.x))) |&gt; \n  select(-quality)\ntb_winequality &lt;- na.omit(tb_winequality)\ntarget_name &lt;- \"high_quality\"\n\nLet us call the get_beta_fit() from Chapter 6 to get our priors.\n\npriors_winequality &lt;- get_beta_fit(\n  dataset = tb_winequality, target_name = target_name, seed = 1234\n)\n\nLet us save the results and the dataset:\n\nsave(priors_winequality, file = \"../output/real-data/priors_winequality.rda\")\nsave(tb_winequality, file = \"../output/real-data/tb_winequality.rda\")\n\n\nplot_hist_scores_beta(priors_winequality, \"winequality\")\n\n\n\n\nFigure 7.9: Distribution of estimated probabilities by the GAMSEL model and Beta distribution fitted to the scores of each of the three models, for the Wine Quality dataset.\n\n\n\n\n\n\n\n\n\n\n7.1.10 Spambase\n\nURL to the data: https://archive.ics.uci.edu/dataset/94/spambase\nDescription: Classifying Email as Spam or Non-Spam\nNumber of instances: 4,601\nFeatures: 57\nReferences: Hopkins et al. (1999)\n\n\nname &lt;- \"spambase\"\n\nThe dataset needs to be download.\n\n\nCode to download the data\nif (!dir.exists(\"../data\")) dir.create(\"../data\")\ndownload.file(\n  url = \"https://archive.ics.uci.edu/static/public/94/spambase.zip\", \n  destfile = \"../data/spambase.zip\"\n)\n\ninfo_data &lt;- scan(\n  unz(\"../data/spambase.zip\", \"spambase.names\"), what = \"character\", sep = \"\\n\"\n)\n# Print the names for this dataset (not very convenient...)\nstr_extract(info_data[31:length(info_data)], \"^(.*):\") |&gt; \n  str_remove(\":$\") |&gt; \n  (\\(.x) str_c('\"', .x, '\",'))() |&gt; \n  cat()\n\n\nThen, we can import the dataset:\n\n\nCode to import the data\ntb_spambase &lt;- read_csv(\n  file = unz(str_c(\"../data/\", name, \".zip\"), str_c(name, \".data\")),\n  col_names = c(\n    \"word_freq_make\", \"word_freq_address\", \"word_freq_all\", \"word_freq_3d\",\n    \"word_freq_our\", \"word_freq_over\", \"word_freq_remove\", \"word_freq_internet\",\n    \"word_freq_order\", \"word_freq_mail\", \"word_freq_receive\", \"word_freq_will\",\n    \"word_freq_people\", \"word_freq_report\", \"word_freq_addresses\",\n    \"word_freq_free\", \"word_freq_business\", \"word_freq_email\", \"word_freq_you\",\n    \"word_freq_credit\", \"word_freq_your\", \"word_freq_font\", \"word_freq_000\",\n    \"word_freq_money\", \"word_freq_hp\", \"word_freq_hpl\", \"word_freq_george\",\n    \"word_freq_650\", \"word_freq_lab\", \"word_freq_labs\", \"word_freq_telnet\",\n    \"word_freq_857\", \"word_freq_data\", \"word_freq_415\", \"word_freq_85\",\n    \"word_freq_technology\", \"word_freq_1999\", \"word_freq_parts\", \"word_freq_pm\",\n    \"word_freq_direct\", \"word_freq_cs\", \"word_freq_meeting\",\n    \"word_freq_original\", \"word_freq_project\", \"word_freq_re\", \"word_freq_edu\",\n    \"word_freq_table\", \"word_freq_conference\", \"char_freq_;\", \"char_freq_(\",\n    \"char_freq_[\", \"char_freq_!\", \"char_freq_$\", \"char_freq_#\",\n    \"capital_run_length_average\", \"capital_run_length_longest\",\n    \"capital_run_length_total\", \"is_spam\"\n  ),\n  show_col_types = FALSE\n)\n\n\nThe target variable:\n\ntarget_name &lt;- \"is_spam\"\n\nLet us call the get_beta_fit() from Chapter 6 to get our priors.\n\npriors_spambase &lt;- get_beta_fit(\n  dataset = tb_spambase, target_name = target_name, seed = 1234\n)\n\nLet us save the results and the dataset:\n\nsave(priors_spambase, file = str_c(\"../output/real-data/priors_spambase.rda\"))\nsave(tb_spambase, file = \"../output/real-data/tb_spambase.rda\")\n\n\nplot_hist_scores_beta(priors_spambase, \"spambase\")\n\n\n\n\nFigure 7.10: Distribution of estimated probabilities by the GAMSEL model and Beta distribution fitted to the scores of each of the three models, for the spambase dataset.",
    "crumbs": [
      "IV. Real Data",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Datasets and Priors</span>"
    ]
  },
  {
    "objectID": "book_real_beta.html#summary",
    "href": "book_real_beta.html#summary",
    "title": "7  Datasets and Priors",
    "section": "7.2 Summary",
    "text": "7.2 Summary\n\n\nCodes to get the key characteristics of the datasets\ndatasets &lt;- tribble(\n  ~name, ~target_name, ~reference,\n  \"abalone\", \"Sex\", \"@misc_abalone_1\",\n  \"adult\", \"high_income\", \"@misc_adult_2\",\n  \"bank\", \"y\", \"@misc_bank_marketing_222\",\n  \"default\", \"default\", \"@misc_default_of_credit_card_clients_350\",\n  \"drybean\", \"is_dermason\", \"@misc_dry_bean_602\",\n  \"coupon\", \"y\", \"@misc_vehicle_coupon_recommendation_603\",\n  \"mushroom\", \"edible\", \"@misc_mushroom_73\",\n  \"occupancy\", \"Occupancy\", \"@misc_occupancy_detection__357\",\n  \"winequality\", \"high_quality\", \"@misc_wine_quality_186\",\n  \"spambase\", \"is_spam\", \"@misc_spambase_94\"\n)\n\ndataset_info &lt;- vector(mode = \"list\", length = nrow(datasets))\nfor (i in 1:nrow(datasets)) {\n  name &lt;- datasets$name[i]\n  target_name &lt;- datasets$target_name[i]\n  current_data &lt;- get(str_c('tb_', name))\n  current_target &lt;- current_data |&gt; pull(!!target_name)\n  current_ref &lt;- datasets$reference[i]\n  n &lt;- nrow(current_data)\n  n_col &lt;- ncol(current_data)\n  n_numeric &lt;- current_data |&gt; select(-!!target_name) |&gt; \n    select(where(is.numeric)) |&gt; \n    ncol()\n  dataset_info[[i]] &lt;- tibble(\n    Dataset = name, \n    n = n, \n    `# features` = n_col-1,\n    `# numeric features` = n_numeric,\n    `Prop. target = 1` = round(sum(current_target == 1) / n, 2),\n    Reference = current_ref\n  )\n}\n\ndataset_info &lt;- list_rbind(dataset_info)\nknitr::kable(dataset_info, booktabs = TRUE, format.args = list(big.mark = \",\"))\n\n\n\n\nTable 7.1: Key characteristics of the datasets.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDataset\nn\n# features\n# numeric features\nProp. target = 1\nReference\n\n\n\n\nabalone\n4,177\n8\n8\n0.37\nNash et al. (1995)\n\n\nadult\n32,561\n14\n6\n0.24\nBecker and Kohavi (1996)\n\n\nbank\n45,211\n16\n7\n0.12\nMoro, Rita, and Cortez (2012)\n\n\ndefault\n30,000\n23\n14\n0.22\nYeh (2016)\n\n\ndrybean\n13,611\n16\n16\n0.26\n“Dry Bean” (2020)\n\n\ncoupon\n12,079\n22\n0\n0.57\n“In-Vehicle Coupon Recommendation” (2020)\n\n\nmushroom\n8,124\n21\n0\n0.52\n“Mushroom” (1987)\n\n\noccupancy\n20,560\n5\n5\n0.23\nCandanedo (2016)\n\n\nwinequality\n6,495\n12\n11\n0.63\nCortez et al. (2009)\n\n\nspambase\n4,601\n57\n57\n0.39\nHopkins et al. (1999)\n\n\n\n\n\n\n\n\n\n\n\n\nBecker, Barry, and Ronny Kohavi. 1996. “Adult.” UCI Machine Learning Repository.\n\n\nCandanedo, Luis. 2016. “Occupancy Detection .” UCI Machine Learning Repository.\n\n\nCortez, Paulo, A. Cerdeira, F. Almeida, T. Matos, and J. Reis. 2009. “Wine Quality.” UCI Machine Learning Repository.\n\n\n“Dry Bean.” 2020. UCI Machine Learning Repository.\n\n\nHopkins, Mark, Erik Reeber, George Forman, and Jaap Suermondt. 1999. “Spambase.” UCI Machine Learning Repository.\n\n\n“In-Vehicle Coupon Recommendation.” 2020. UCI Machine Learning Repository.\n\n\nMoro, S., P. Rita, and P. Cortez. 2012. “Bank Marketing.” UCI Machine Learning Repository.\n\n\n“Mushroom.” 1987. UCI Machine Learning Repository.\n\n\nNash, Warwick, Tracy Sellers, Simon Talbot, Andrew Cawthorn, and Wes Ford. 1995. “Abalone.” UCI Machine Learning Repository.\n\n\nYeh, I-Cheng. 2016. “Default of Credit Card Clients.” UCI Machine Learning Repository.",
    "crumbs": [
      "IV. Real Data",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Datasets and Priors</span>"
    ]
  },
  {
    "objectID": "example_spambase_xgb.html",
    "href": "example_spambase_xgb.html",
    "title": "8  Recalibration: Example",
    "section": "",
    "text": "8.1 Setup",
    "crumbs": [
      "IV. Real Data",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Recalibration: Example</span>"
    ]
  },
  {
    "objectID": "example_spambase_xgb.html#setup",
    "href": "example_spambase_xgb.html#setup",
    "title": "8  Recalibration: Example",
    "section": "",
    "text": "8.1.1 XGBoost\nTo train the models, we rely on the {xgboost} R package.\n\nlibrary(xgboost)\n\n\n\n8.1.2 Recalibration\nHere, we define a function to recalibrate predicted scores using either Platt scaling or isotonic regression. The recalibration algorithm is first trained on the calibration set and then applied to both the calibration and test sets.\n\n#' Recalibrates scores using a calibration\n#' \n#' @param obs_calib vector of observed events in the calibration set\n#' @param scores_calib vector of predicted probabilities in the calibration set\n#' @param obs_test vector of observed events in the test set\n#' @param scores_test vector of predicted probabilities in the test set\n#' @param method recalibration method (`\"platt\"` for Platt scaling, \n#'   `\"isotonic\"` for isotonic regression)\n#' @returns list of two elements: recalibrated scores on the calibration set,\n#'   recalibrated scores on the test set\nrecalibrate &lt;- function(obs_calib,\n                        obs_test,\n                        pred_calib,\n                        pred_test,\n                        method = c(\"platt\", \"isotonic\")) {\n  data_calib &lt;- tibble(d = obs_calib, scores = pred_calib)\n  data_test &lt;- tibble(d = obs_test, scores = pred_test)\n  \n  if (method == \"platt\") {\n    lr &lt;- glm(d ~ scores, family = binomial(link = 'logit'), data = data_calib)\n    score_c_calib &lt;- predict(lr, newdata = data_calib, type = \"response\")\n    score_c_test &lt;- predict(lr, newdata = data_test, type = \"response\")\n  } else if (method == \"isotonic\") {\n    iso &lt;- isoreg(x = data_calib$scores, y = data_calib$d)\n    fit_iso &lt;- as.stepfun(iso)\n    score_c_calib &lt;- fit_iso(data_calib$scores)\n    score_c_test &lt;- fit_iso(data_test$scores)\n    \n  } else {\n    stop(\"Unrecognized method: platt or isotonic only\")\n  }\n  # Format results in tibbles:\n  # For calibration set\n  tb_score_c_calib &lt;- tibble(\n    d = obs_calib,\n    p_u = pred_calib,\n    p_c = score_c_calib\n  )\n  # For test set\n  tb_score_c_test &lt;- tibble(\n    d = obs_test,\n    p_u = pred_test,\n    p_c = score_c_test\n  )\n  \n  list(\n    tb_score_c_calib = tb_score_c_calib,\n    tb_score_c_test = tb_score_c_test\n  )\n  \n}\n\n\n\n8.1.3 Dispersion Metrics\nWe modify our dispersion_metrics() function to replace the vector of simulated true probabilities with the parameters of a Beta distribution. This adjustment allows us to compute the divergence between the model-estimated scores and the specified Beta distribution.\n\n\nFunction dispersion_metrics_beta()\n#' Computes the dispersion and divergence metrics for a vector of scores and\n#' a Beta distribution\n#'\n#' @param shape_1 first parameter of the beta distribution\n#' @param shape_2 second parameter of the beta distribution\n#' @param scores predicted scores\n#'\n#' @returns\n#' \\itemize{\n#'   \\item \\code{inter_quantile_25_75}: Difference of inter-quantile between 25% and 75%\n#'   \\item \\code{inter_quantile_10_90}: Difference of inter-quantile between 10% and 90%\n#'   \\item \\code{KL_10_true_probas}: KL of of predicted probabilities w.r. to true probabilities with 10 bins\n#'   \\item \\code{KL_10_scores}: KL of of true probabilities w.r. to predicted probabilities with 10 bins\n#'   \\item \\code{KL_20_true_probas}: KL of of predicted probabilities w.r. to true probabilities with 20 bins\n#'   \\item \\code{KL_20_scores}: KL of of true probabilities w.r. to predicted probabilities with 20 bins\n#'   \\item \\code{ind_cov}: Difference between the variance of true probabilities and the covariance between true probabilities and predicted scores\n#' }\ndispersion_metrics_beta &lt;- function(shape_1 = 1, shape_2 = 1, scores){\n\n  # Inter-quantiles\n  inter_q_80 &lt;- diff(quantile(scores, c(.9, .1))) /\n    diff(qbeta(c(.9, .1), shape_1, shape_2))\n  inter_q_50 &lt;- diff(quantile(scores, c(.75,.25))) /\n    diff(qbeta(c(.75,.25), shape_1, shape_1))\n\n  # KL divergences\n  m &lt;- 10 # Number of bins\n  h_phat &lt;- hist(scores, breaks = seq(0, 1, length = m + 1), plot = FALSE)\n  h_p &lt;- list(breaks = h_phat$breaks, mids = h_phat$mids)\n  h_p$density = diff(pbeta(h_p$breaks, shape_1, shape_2))\n  h_p$counts =  h_p$density*length(scores)\n\n  # Densities\n  h1 &lt;- rbind(h_phat$density / m, h_p$density / m) # Reference : true probabilities\n  h2 &lt;- rbind(h_p$density / m, h_phat$density / m) # Reference : predicted scores\n  KL_10_true_probas &lt;- distance(\n    h1, method = \"kullback-leibler\", unit = \"log2\", mute.message = TRUE)\n  KL_10_scores &lt;- distance(\n    h2, method = \"kullback-leibler\", unit = \"log2\", mute.message = TRUE)\n\n\n  m &lt;- 20 # Number of bins\n  h_phat &lt;- hist(scores, breaks = seq(0, 1, length = m + 1), plot = FALSE)\n  h_p &lt;- list(breaks = h_phat$breaks, mids = h_phat$mids)\n  h_p$density = diff(pbeta(h_p$breaks, shape_1, shape_2))\n  h_p$counts =  h_p$density * length(scores)\n  # Densities\n  h1 &lt;- rbind(h_phat$density / m, h_p$density) # Reference : true probabilities\n  h2 &lt;- rbind(h_p$density, h_phat$density / m) # Reference : predicted scores\n  KL_20_true_probas &lt;- distance(\n    h1, method = \"kullback-leibler\", unit = \"log2\", mute.message = TRUE)\n  KL_20_scores &lt;- distance(\n    h2, method = \"kullback-leibler\", unit = \"log2\", mute.message = TRUE)\n\n  # Indicator of the difference between variance and covariance\n  var_p &lt;- shape_1 * shape_2 / ((shape_1 + shape_2)^2 * (shape_1 + shape_2 + 1))\n  cov_p_phat &lt;- cov(\n    qbeta(\n      rank(scores, ties.method = \"average\") / (1 + length(scores)),\n      shape_1,\n      shape_2),\n    scores\n  )\n  ind_cov &lt;- abs(cov_p_phat - var_p)\n\n  # Collection\n  dispersion_metrics &lt;- tibble(\n    \"inter_quantile_25_75\" = as.numeric(inter_q_50),\n    \"inter_quantile_10_90\" = as.numeric(inter_q_80),\n    \"KL_10_true_probas\" = as.numeric(KL_10_true_probas),\n    \"KL_10_scores\" = as.numeric(KL_10_scores),\n    \"KL_20_true_probas\" = as.numeric(KL_20_true_probas),\n    \"KL_20_scores\" = as.numeric(KL_20_scores),\n    \"ind_cov\" = ind_cov\n  )\n\n  dispersion_metrics\n}\n\ndisp_metrics_dataset &lt;- function(prior, scores) {\n  # GAMSEL prior\n  shape_1 &lt;- prior$mle_gamsel$estimate[\"shape1\"]\n  shape_2 &lt;- prior$mle_gamsel$estimate[\"shape2\"]\n\n  # Divergence metrics\n  dist_prior_gamsel &lt;- dispersion_metrics_beta(\n    shape_1 = shape_1, shape_2 = shape_2, scores = scores\n  )\n\n  dist_prior_gamsel |&gt;\n    mutate(\n      prior = \"gamsel\", shape_1 = shape_1, shape_2 = shape_2\n      )\n}\n\n\n\n\n8.1.4 Estimation Functions: Extreme Gradient Boosting\nWe train XGB models on the dataset “spambase”, varying the maximum tree depth (with values of 2, 4, or 6) and the number of boosting iterations (ranging from 2 to 500). At each iteration, we compute metrics based on scores from both the train, calibration, validation and test samples, as well as the recalibrated scores.\nThis function allows us to calculate the performance, dispersion and calibration metrics on all sets of the dataset at a given iteration for a single XGBoost algorithm:\n\n\nFunction get_metrics_xgb_iter()\n#' Get the metrics based on scores estimated at a given boosting iteration\n#'\n#' @param scores scores estimated a boosting iteration `nb_iter` (list with\n#'   train and test scores, returned by `predict_score_iter()`)\n#' @param data_train train set\n#' @param data_valid validation set\n#' @param data_calib calibration set\n#' @param data_test test set\n#' @param target_name name of the target variable\n#' @param ind index of the grid search\n#' @param nb_iter boosting iteration to consider\n#' @param params hyperparameters to consider\n#'\n#' @returns A list with 4 elements:\n#'  - `tb_metrics`: performance / calibration metrics\n#'  - `tb_disp_metrics`: disp and div metrics\n#'  - `tb_prop_scores`: table with P(q1 &lt; score &lt; q2)\n#'  - `scores_hist`: histogram of scores\nget_metrics_xgb_iter &lt;- function(scores,\n                                 prior,\n                                 data_train,\n                                 data_valid,\n                                 data_calib,\n                                 data_test,\n                                 target_name,\n                                 ind,\n                                 nb_iter,\n                                 params) {\n\n  scores_train &lt;- scores$scores_train\n  scores_valid &lt;- scores$scores_valid\n  scores_calib &lt;- scores$scores_calib\n  scores_test &lt;- scores$scores_test\n  \n  # Recalibration\n  # Platt scaling\n  res_recalibration_platt &lt;- recalibrate(\n    obs_calib = data_calib |&gt; dplyr::pull(!!target_name), \n    obs_test = data_test |&gt; dplyr::pull(!!target_name), \n    pred_calib = scores_calib, \n    pred_test = scores_test, \n    method = \"platt\"\n  )\n  scores_c_platt_calib &lt;- res_recalibration_platt$tb_score_c_calib$p_c\n  scores_c_platt_test &lt;- res_recalibration_platt$tb_score_c_test$p_c\n  \n  # Isotonic regression\n  res_recalibration_iso &lt;- recalibrate(\n    obs_calib = data_calib |&gt; dplyr::pull(!!target_name), \n    obs_test = data_test |&gt; dplyr::pull(!!target_name), \n    pred_calib = scores_calib, \n    pred_test = scores_test, \n    method = \"isotonic\"\n  )\n  scores_c_iso_calib &lt;- res_recalibration_iso$tb_score_c_calib$p_c\n  scores_c_iso_test &lt;- res_recalibration_iso$tb_score_c_test$p_c\n\n  ## Metrics----\n  ## Histogram of scores----\n  breaks &lt;- seq(0, 1, by = .05)\n  scores_train_hist &lt;- hist(scores_train, breaks = breaks, plot = FALSE)\n  scores_calib_hist &lt;- hist(scores_calib, breaks = breaks, plot = FALSE)\n  scores_valid_hist &lt;- hist(scores_valid, breaks = breaks, plot = FALSE)\n  scores_test_hist &lt;- hist(scores_test, breaks = breaks, plot = FALSE)\n  scores_c_platt_calib_hist &lt;- hist(scores_c_platt_calib, breaks = breaks, plot = FALSE)\n  scores_c_platt_test_hist &lt;- hist(scores_c_platt_test, breaks = breaks, plot = FALSE)\n  scores_c_iso_calib_hist &lt;- hist(scores_c_iso_calib, breaks = breaks, plot = FALSE)\n  scores_c_iso_test_hist &lt;- hist(scores_c_iso_test, breaks = breaks, plot = FALSE)\n  \n  scores_hist &lt;- list(\n    train = scores_train_hist,\n    valid = scores_valid_hist,\n    calib = scores_calib_hist,\n    test = scores_test_hist,\n    calib_c_platt = scores_c_platt_calib_hist,\n    test_c_platt = scores_c_platt_test_hist,\n    calib_c_iso = scores_c_iso_calib_hist,\n    test_c_iso = scores_c_iso_test_hist,\n    ind = ind,\n    nb_iter = nb_iter,\n    max_depth = params$max_depth\n    \n  )\n  \n  ## Estimation of P(q1 &lt; score &lt; q2)----\n  prop_btw_q_h &lt;- function(s, sample_name, recalib_name) {\n    map(\n      c(.1, .2, .3, .4),\n      ~prop_btw_quantiles(s = s, q1 = .x)\n    ) |&gt;\n      list_rbind() |&gt;\n      mutate(sample = sample_name, recalib = recalib_name)\n  }\n  \n  proq_scores_train &lt;- prop_btw_q_h(\n    scores_train, sample_name = \"train\", recalib_name = \"none\"\n  )\n  proq_scores_valid &lt;- prop_btw_q_h(\n    scores_valid, sample_name = \"valid\", recalib_name = \"none\"\n  )\n  proq_scores_calib &lt;- prop_btw_q_h(\n    scores_calib, sample_name = \"calib\", recalib_name = \"none\"\n  )\n  proq_scores_test &lt;- prop_btw_q_h(\n    scores_test, sample_name = \"test\", recalib_name = \"none\"\n  )\n  proq_scores_c_platt_calib &lt;- prop_btw_q_h(\n    scores_c_platt_calib, sample_name = \"calib\", recalib_name = \"platt\"\n  )\n  proq_scores_c_platt_test &lt;- prop_btw_q_h(\n    scores_c_platt_test, sample_name = \"test\", recalib_name = \"platt\"\n  )\n  proq_scores_c_iso_calib &lt;- prop_btw_q_h(\n    scores_c_iso_calib, sample_name = \"calib\", recalib_name = \"isotonic\"\n  )\n  proq_scores_c_iso_test &lt;- prop_btw_q_h(\n    scores_c_iso_test, sample_name = \"test\", recalib_name = \"isotonic\"\n  )\n\n  ## Dispersion Metrics----\n  disp_train &lt;- disp_metrics_dataset(\n    prior = prior, scores = scores_train\n  ) |&gt; \n    mutate(sample = \"train\", recalib = \"none\")\n  \n  disp_valid &lt;- disp_metrics_dataset(\n    prior = prior, scores = scores_valid\n  ) |&gt;\n    mutate(sample = \"valid\", recalib = \"none\")\n  \n  disp_calib &lt;- disp_metrics_dataset(\n    prior = prior, scores = scores_calib\n  ) |&gt; \n    mutate(sample = \"calib\", recalib = \"none\")\n  \n  disp_test &lt;- disp_metrics_dataset(\n    prior = prior, scores = scores_test\n  ) |&gt; \n    mutate(sample = \"test\", recalib = \"none\")\n  \n  disp_c_platt_calib &lt;- disp_metrics_dataset(\n    prior = prior, scores = scores_c_platt_calib\n  ) |&gt;\n    mutate(sample = \"calib\", recalib = \"platt\")\n  \n  disp_c_platt_test &lt;- disp_metrics_dataset(\n    prior = prior, scores = scores_c_platt_test\n  ) |&gt;\n    mutate(sample = \"test\", recalib = \"platt\")\n  \n  disp_c_iso_calib &lt;- disp_metrics_dataset(\n    prior = prior, scores = scores_c_iso_calib\n  ) |&gt;\n    mutate(sample = \"calib\", recalib = \"isotonic\")\n  \n  disp_c_iso_test &lt;- disp_metrics_dataset(\n    prior = prior, scores = scores_c_iso_test\n  ) |&gt;\n    mutate(sample = \"test\", recalib = \"isotonic\")\n\n  # Performance and Calibration Metrics----\n  # We add very small noise to predicted scores\n  # otherwise the local regression may crash\n  scores_train_noise &lt;- scores_train +\n    runif(n = length(scores_train), min = 0, max = 0.01)\n  scores_train_noise[scores_train_noise &gt; 1] &lt;- 1\n  metrics_train &lt;- compute_metrics(\n    obs = data_train |&gt; pull(!!target_name),\n    scores = scores_train_noise, true_probas = NULL\n  ) |&gt; mutate(sample = \"train\", recalib = \"none\")\n\n  scores_valid_noise &lt;- scores_valid +\n    runif(n = length(scores_valid), min = 0, max = 0.01)\n  scores_valid_noise[scores_valid_noise &gt; 1] &lt;- 1\n  metrics_valid &lt;- compute_metrics(\n    obs = data_valid |&gt; pull(!!target_name),\n    scores = scores_valid_noise, true_probas = NULL\n  ) |&gt; mutate(sample = \"valid\", recalib = \"none\")\n  \n  scores_calib_noise &lt;- scores_calib +\n    runif(n = length(scores_calib), min = 0, max = 0.01)\n  scores_calib_noise[scores_calib_noise &gt; 1] &lt;- 1\n  metrics_calib &lt;- compute_metrics(\n    obs = data_calib |&gt; pull(!!target_name),\n    scores = scores_calib_noise, true_probas = NULL\n  ) |&gt; mutate(sample = \"calib\", recalib = \"none\")\n  \n  scores_test_noise &lt;- scores_test +\n    runif(n = length(scores_test), min = 0, max = 0.01)\n  scores_test_noise[scores_test_noise &gt; 1] &lt;- 1\n  metrics_test &lt;- compute_metrics(\n    obs = data_test |&gt; pull(!!target_name),\n    scores = scores_test_noise, true_probas = NULL\n  ) |&gt; mutate(sample = \"test\", recalib = \"none\")\n  \n  # With recalibrated scores (platt)\n  scores_c_platt_calib_noise &lt;- scores_c_platt_calib +\n    runif(n = length(scores_calib), min = 0, max = 0.01)\n  scores_c_platt_calib_noise[scores_c_platt_calib_noise &gt; 1] &lt;- 1\n  metrics_c_platt_calib &lt;- compute_metrics(\n    obs = data_calib |&gt; pull(!!target_name),\n    scores = scores_c_platt_calib_noise, true_probas = NULL\n  ) |&gt; mutate(sample = \"calib\", recalib = \"platt\")\n  \n  scores_c_platt_test_noise &lt;- scores_c_platt_test +\n    runif(n = length(scores_test), min = 0, max = 0.01)\n  scores_c_platt_test_noise[scores_c_platt_test_noise &gt; 1] &lt;- 1\n  metrics_c_platt_test &lt;- compute_metrics(\n    obs = data_test |&gt; pull(!!target_name),\n    scores = scores_c_platt_test_noise, true_probas = NULL\n  ) |&gt; mutate(sample = \"test\", recalib = \"platt\")\n  \n  # With recalibrated scores (isotonic)\n  scores_c_iso_calib_noise &lt;- scores_c_iso_calib +\n    runif(n = length(scores_c_iso_calib), min = 0, max = 0.01)\n  scores_c_iso_calib_noise[scores_c_iso_calib_noise &gt; 1] &lt;- 1\n  metrics_c_iso_calib &lt;- compute_metrics(\n    obs = data_calib |&gt; pull(!!target_name),\n    scores = scores_c_iso_calib_noise, true_probas = NULL\n  ) |&gt; mutate(sample = \"calib\", recalib = \"isotonic\")\n  \n  scores_c_iso_test_noise &lt;- scores_c_iso_test +\n    runif(n = length(scores_c_iso_test), min = 0, max = 0.01)\n  scores_c_iso_test_noise[scores_c_iso_test_noise &gt; 1] &lt;- 1\n  metrics_c_iso_test &lt;- compute_metrics(\n    obs = data_test |&gt; pull(!!target_name),\n    scores = scores_c_iso_test_noise, true_probas = NULL\n  ) |&gt; mutate(sample = \"test\", recalib = \"isotonic\")\n  \n  tb_metrics &lt;- metrics_train |&gt;\n    bind_rows(metrics_valid) |&gt;\n    bind_rows(metrics_calib) |&gt;\n    bind_rows(metrics_test) |&gt;\n    bind_rows(metrics_c_platt_calib) |&gt;\n    bind_rows(metrics_c_platt_test) |&gt;\n    bind_rows(metrics_c_iso_calib) |&gt;\n    bind_rows(metrics_c_iso_test) |&gt;\n    mutate(ind = ind, nb_iter = nb_iter, max_depth = params$max_depth) |&gt;\n    select(-c(mse, mae))\n  \n  tb_disp_metrics &lt;- disp_train |&gt;\n    bind_rows(disp_valid) |&gt; \n    bind_rows(disp_calib) |&gt; \n    bind_rows(disp_test) |&gt; \n    bind_rows(disp_c_platt_calib) |&gt; \n    bind_rows(disp_c_platt_test) |&gt; \n    bind_rows(disp_c_iso_calib) |&gt; \n    bind_rows(disp_c_iso_test) |&gt;\n    mutate(ind = ind, nb_iter = nb_iter, max_depth = params$max_depth)\n  \n  tb_metrics &lt;- tb_metrics |&gt; left_join(\n    tb_disp_metrics, by = c(\"sample\", \"recalib\", \"ind\", \"nb_iter\", \"max_depth\")\n  )\n\n  tb_prop_scores &lt;- proq_scores_train |&gt;\n    bind_rows(proq_scores_valid) |&gt;\n    bind_rows(proq_scores_calib) |&gt;\n    bind_rows(proq_scores_test) |&gt;\n    bind_rows(proq_scores_c_platt_calib) |&gt;\n    bind_rows(proq_scores_c_platt_test) |&gt;\n    bind_rows(proq_scores_c_iso_calib) |&gt;\n    bind_rows(proq_scores_c_iso_test) |&gt;\n    mutate(ind = ind, nb_iter = nb_iter, max_depth = params$max_depth)\n  \n   list(\n    ind = ind,                         # index for grid\n    nb_iter = nb_iter,                 # number of boosting iterations\n    max_depth = params$max_depth,      # max depth of used trees\n    tb_metrics = tb_metrics,           # # table with performance/calib/divergence\n    tb_prop_scores = tb_prop_scores,   # table with P(q1 &lt; score &lt; q2)\n    scores_hist = scores_hist          # histogram of scores\n  )\n}\n\n\nWe then define a function to predict a XGBoost algorithm on train, validation, calibration and test samples given an xgboost object (fit). We are able to obtain the predicted scores at a specified iteration during the training of the algorithm:\n\n\nFunction predict_score_iter()\n#' Predicts the scores at a given iteration of the XGB model\n#'\n#' @param fit_xgb estimated XGB model\n#' @param tb_train_xgb train set\n#' @param tb_valid_xgb validation set\n#' @param tb_test_xgb test set\n#' @param ind index of the grid search\n#' @param nb_iter boosting iteration to consider\n#'\n#' @returns A list with three elements: `scores_train`, `scores_valid`, and\n#'  `scores_train` which contain the estimated scores on the train and on the \n#'  test score, resp.\npredict_score_iter &lt;- function(fit_xgb,\n                               tb_train_xgb,\n                               tb_valid_xgb,\n                               tb_calib_xgb,\n                               tb_test_xgb,\n                               nb_iter) {\n\n  ## Predicted scores----\n  scores_train &lt;- predict(fit_xgb, tb_train_xgb, iterationrange = c(1, nb_iter))\n  scores_valid &lt;- predict(fit_xgb, tb_valid_xgb, iterationrange = c(1, nb_iter))\n  scores_calib &lt;- predict(fit_xgb, tb_calib_xgb, iterationrange = c(1, nb_iter))\n  scores_test &lt;- predict(fit_xgb, tb_test_xgb, iterationrange = c(1, nb_iter))\n\n  list(\n    scores_train = scores_train,\n    scores_valid = scores_valid,\n    scores_calib = scores_calib,\n    scores_test = scores_test\n  )\n}\n\n\nThis function calculates the different metrics on train, validation, calibration and test samples for a given XGBoost algorithm at all iterations:\n\n\nFunction simul_xgb_helper()\n#' Fit an XGB and returns metrics based on scores. The divergence metrics are\n#' obtained using the prior distributions.\n#'\n#' @param data_train train set\n#' @param data_valid validation set\n#' @param data_test test set\n#' @param target_name name of the target variable\n#' @param parms tibble with hyperparameters for the current estimation\n#' @param prior prior obtained with `get_beta_fit()`\n#'\n#' @returns A list with 4 elements:\n#'  - `tb_metrics`: performance / calibration metrics\n#'  - `tb_disp_metrics`: disp and div metrics\n#'  - `tb_prop_scores`: table with P(q1 &lt; score &lt; q2)\n#'  - `scores_hist`: histogram of scores\nsimul_xgb_helper &lt;- function(data_train,\n                             data_valid,\n                             data_calib,\n                             data_test,\n                             target_name,\n                             params,\n                             prior) {\n\n  ## Format data for xgboost----\n  tb_train_xgb &lt;- xgb.DMatrix(\n    data = data_train |&gt; dplyr::select(-!!target_name) |&gt; as.matrix(),\n    label = data_train |&gt; dplyr::pull(!!target_name) |&gt; as.matrix()\n  )\n  tb_valid_xgb &lt;- xgb.DMatrix(\n    data = data_valid |&gt; dplyr::select(-!!target_name) |&gt; as.matrix(),\n    label = data_valid |&gt; dplyr::pull(!!target_name) |&gt; as.matrix()\n  )\n  tb_calib_xgb &lt;- xgb.DMatrix(\n    data = data_calib |&gt; dplyr::select(-!!target_name) |&gt; as.matrix(),\n    label = data_calib |&gt; dplyr::pull(!!target_name) |&gt; as.matrix()\n  )\n  tb_test_xgb &lt;- xgb.DMatrix(\n    data = data_test |&gt; dplyr::select(-!!target_name) |&gt; as.matrix(),\n    label = data_test |&gt; dplyr::pull(!!target_name) |&gt; as.matrix()\n  )\n  # Parameters for the algorithm\n  param &lt;- list(\n    max_depth = params$max_depth, #Note: root node is indexed 0\n    eta = params$eta,\n    nthread = 1,\n    objective = \"binary:logistic\",\n    eval_metric = \"auc\"\n  )\n  watchlist &lt;- list(train = tb_train_xgb, eval = tb_valid_xgb)\n  ## Estimation----\n  fit_xgb &lt;- xgb.train(\n    param, tb_train_xgb,\n    nrounds = params$nb_iter_total,\n    watchlist,\n    verbose = 0\n  )\n\n  # First, we estimate the scores at each boosting iteration\n  # As the xgb.Dmatrix objects cannot be easily serialised, we first estimate\n  # these scores in a classical way, without parallelism...\n  scores_iter &lt;- vector(mode = \"list\", length = params$nb_iter_total)\n  for (i_iter in 1:params$nb_iter_total) {\n    scores_iter[[i_iter]] &lt;- predict_score_iter(\n      fit_xgb = fit_xgb,\n      tb_train_xgb = tb_train_xgb,\n      tb_valid_xgb = tb_valid_xgb,\n      tb_calib_xgb = tb_calib_xgb,\n      tb_test_xgb = tb_test_xgb,\n      nb_iter = i_iter)\n  }\n\n  # Then, to compute the metrics, as it is a bit slower, we can use parallelism\n\n  ncl &lt;- detectCores() - 1\n  (cl &lt;- makeCluster(ncl))\n  clusterEvalQ(cl, {\n    library(tidyverse)\n    library(locfit)\n    library(philentropy)\n  }) |&gt;\n    invisible()\n\n  clusterExport(cl, c(\n    \"scores_iter\", \"prior\", \"data_train\", \"data_valid\", \"data_calib\", \"data_test\", \"params\", \n    \"target_name\"\n  ), envir = environment())\n  clusterExport(cl, c(\n    \"get_metrics_xgb_iter\",\n    \"brier_score\",\n    \"compute_metrics\",\n    \"disp_metrics_dataset\", \"dispersion_metrics_beta\",\n    \"recalibrate\", \"prop_btw_quantiles\"\n  ))\n\n  metrics_iter &lt;-\n    pbapply::pblapply(\n      X = seq_len(params$nb_iter_total),\n      FUN = function(i_iter) {\n        get_metrics_xgb_iter(\n          scores = scores_iter[[i_iter]],\n          prior = prior,\n          data_train = data_train,\n          data_valid = data_valid,\n          data_calib = data_calib,\n          data_test = data_test,\n          target_name = target_name,\n          ind = params$ind,\n          nb_iter = i_iter,\n          params = params\n        )\n      },\n      cl = cl\n    )\n  stopCluster(cl)\n  \n  # Merge tibbles from each iteration into a single one\n  tb_metrics &lt;-\n    map(metrics_iter, \"tb_metrics\") |&gt;\n    list_rbind()\n  tb_prop_scores &lt;-\n    map(metrics_iter, \"tb_prop_scores\") |&gt;\n    list_rbind()\n  scores_hist &lt;- map(metrics_iter, \"scores_hist\")\n\n  list(\n    tb_metrics = tb_metrics,\n    tb_prop_scores = tb_prop_scores,\n    scores_hist = scores_hist\n  )\n}",
    "crumbs": [
      "IV. Real Data",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Recalibration: Example</span>"
    ]
  },
  {
    "objectID": "example_spambase_xgb.html#import-data-and-prior",
    "href": "example_spambase_xgb.html#import-data-and-prior",
    "title": "8  Recalibration: Example",
    "section": "8.2 Import Data and Prior",
    "text": "8.2 Import Data and Prior\nWe load the dataset “spambase” and the associated Beta prior on both train and test sets.\n\nload(\"../output/real-data/tb_spambase.rda\") # dataset\nload(\"../output/real-data/priors_spambase.rda\") # scores_gamsel\n\nprior &lt;- priors_spambase\ndata &lt;- tb_spambase\n\nThe target variable is is_spam.\n\nname &lt;- \"spambase\"\ntarget_name &lt;- \"is_spam\"",
    "crumbs": [
      "IV. Real Data",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Recalibration: Example</span>"
    ]
  },
  {
    "objectID": "example_spambase_xgb.html#grid",
    "href": "example_spambase_xgb.html#grid",
    "title": "8  Recalibration: Example",
    "section": "8.3 Grid",
    "text": "8.3 Grid\nWe consider the following grid:\n\ngrid &lt;- expand_grid(\n  max_depth = c(2, 4, 6),\n  nb_iter_total = 500,\n  eta = 0.3\n) |&gt;\n  mutate(ind = row_number())\n\nThe different configurations are reported in Table 9.1.\n\nDT::datatable(grid)\n\n\n\nTable 8.1: Grid Search Values\n\n\n\n\n\n\n\n\n\n\nWe define a function, simul_xgb_real() to train the model on a dataset for all different values of the hyperparameters of the grid (and for all iterations of each algorithm, defined by a row in the grid).\n\n\nFunction simul_xgb_real()\n#' Train an XGB on a dataset for a binary task for various\n#' hyperparameters and computes metrics based on scores and on a set of prior\n#' distributions of the underlying probability\n#'\n#' @param data dataset\n#' @param target_name name of the target variable\n#' @param prior prior obtained with `get_beta_fit()`\n#' @param seed desired seed (default to `NULL`)\n#'\n#' @returns A list with two elements:\n#'  - `res`: results for each estimated model of the grid. Each element is a\n#'  list with the following elements:\n#'      - `tb_metrics`: performance / calibration metrics\n#'      - `tb_disp_metrics`: disp and div metrics\n#'      - `tb_prop_scores`: table with P(q1 &lt; score &lt; q2)\n#'      - `scores_hist`: histogram of scores.\n#'  - `grid`: the grid search.\nsimul_xgb_real &lt;- function(data,\n                           target_name,\n                           prior,\n                           seed = NULL) {\n\n  if (!is.null(seed)) set.seed(seed)\n\n  # Split data into train and test set\n  data_splitted &lt;- split_train_test(data = data, prop_train = .5, seed = seed)\n  data_encoded &lt;- encode_dataset(\n    data_train = data_splitted$train,\n    data_test = data_splitted$test,\n    target_name = target_name,\n    intercept = FALSE\n  )\n\n  # Further split train into two samples (train/valid)\n  data_splitted_train &lt;- \n    split_train_test(data = data_encoded$train, prop_train = .8, seed = seed)\n  \n  # Further split test into two samples (calib/test)\n  data_splitted_test &lt;- \n    split_train_test(data = data_encoded$test, prop_train = .8, seed = seed)\n  \n  res_grid &lt;- vector(mode = \"list\", length = nrow(grid))\n  for (i_grid in 1:nrow(grid)) {\n    res_grid[[i_grid]] &lt;- simul_xgb_helper(\n      data_train = data_splitted_train$train,\n      data_valid = data_splitted_train$test,\n      data_calib = data_splitted_test$train,\n      data_test = data_splitted_test$test,\n      target_name = target_name,\n      params = grid |&gt; dplyr::slice(i_grid),\n      prior = prior\n    )\n  }\n  \n  # The metrics computed for all set of hyperparameters (identified with `ind`)\n  # and for each number of boosting iterations (`nb_iter`)\n  metrics_simul &lt;- map(res_grid, \"tb_metrics\") |&gt; \n    list_rbind()\n  \n  # P(q_1&lt;s(x)&lt;q_2)\n  prop_scores_simul &lt;- map(res_grid, \"tb_prop_scores\") |&gt; \n    list_rbind()\n  \n  # Histogram of estimated scores\n  scores_hist &lt;- map(res_grid, \"scores_hist\")\n\n  list(\n    metrics_simul = metrics_simul,\n    scores_hist = scores_hist,\n    prop_scores_simul = prop_scores_simul\n  )\n}",
    "crumbs": [
      "IV. Real Data",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Recalibration: Example</span>"
    ]
  },
  {
    "objectID": "example_spambase_xgb.html#example-spambase-dataset",
    "href": "example_spambase_xgb.html#example-spambase-dataset",
    "title": "8  Recalibration: Example",
    "section": "8.4 Example: “spambase” dataset",
    "text": "8.4 Example: “spambase” dataset\nWe start to split the dataset “spambase” into train/validation/calibration/test sets.\n\ndata_splitted &lt;- split_train_test(data = data, prop_train = .7, seed = 1234)\ndata_encoded &lt;- encode_dataset(\n  data_train = data_splitted$train,\n  data_test = data_splitted$test,\n  target_name = target_name,\n  intercept = FALSE\n  )\n\n# Further split train into two samples (train/valid)\ndata_splitted_train &lt;- \n  split_train_test(data = data_encoded$train, prop_train = .8, seed = 1234)\n  \n# Further split test into two samples (calib/test)\ndata_splitted_test &lt;- \n  split_train_test(data = data_encoded$test, prop_train = .6, seed = 1234)\n\n# The different sets\ndata_train = data_splitted_train$train\ndata_valid = data_splitted_train$test\ndata_calib = data_splitted_test$train\ndata_test = data_splitted_test$test\n\nWe try to fit and predict XGBoost with one set of hyperparameters. Here, we don’t need to use the validation set because we have specified the hyperparameters. In practice, hyperparameters will be selected based on different metrics (calibration, performance, dispersion) calculated on the validation set.\nLet’s take the first row of the grid:\n\ni_grid &lt;- 1\nparams &lt;- grid |&gt; dplyr::slice(i_grid)\n\n# Parameters for the algorithm\nparam &lt;- list(\n  max_depth = params$max_depth, #Note: root node is indexed 0\n  eta = params$eta,\n  nthread = 1,\n  objective = \"binary:logistic\",\n  eval_metric = \"auc\"\n)\n\nNext, we transform the different sets of the dataset in the right format to apply XGBoost:\n\n## Format data for xgboost----\ntb_train_xgb &lt;- xgb.DMatrix(\n  data = data_train |&gt; dplyr::select(-!!target_name) |&gt; as.matrix(),\n  label = data_train |&gt; dplyr::pull(!!target_name) |&gt; as.matrix()\n)\ntb_valid_xgb &lt;- xgb.DMatrix(\n  data = data_valid |&gt; dplyr::select(-!!target_name) |&gt; as.matrix(),\n  label = data_valid |&gt; dplyr::pull(!!target_name) |&gt; as.matrix()\n)\ntb_calib_xgb &lt;- xgb.DMatrix(\n  data = data_calib |&gt; dplyr::select(-!!target_name) |&gt; as.matrix(),\n  label = data_calib |&gt; dplyr::pull(!!target_name) |&gt; as.matrix()\n)\ntb_test_xgb &lt;- xgb.DMatrix(\n  data = data_test |&gt; dplyr::select(-!!target_name) |&gt; as.matrix(),\n  label = data_test |&gt; dplyr::pull(!!target_name) |&gt; as.matrix()\n)\n  \nwatchlist &lt;- list(train = tb_train_xgb, eval = tb_valid_xgb)\n\nWe fit the specified algorithm on the train set:\n\n## Estimation----\nfit_xgb &lt;- xgb.train(\n  param, tb_train_xgb,\n  nrounds = params$nb_iter_total,\n  watchlist,\n  verbose = 0\n)\n\nThen, we estimate the scores at each boosting iteration to specify it for the final model, by using the function predict_score_iter(). We obtain a list of 400 elements, corresponding to the predicted scores at each iteration (total of 400 iterations for a specified XGBoost algorithm):\n\n# As the xgb.Dmatrix objects cannot be easily serialised, we first estimate\n# these scores in a classical way, without parallelism...\nscores_iter &lt;- vector(mode = \"list\", length = params$nb_iter_total)\nfor (i_iter in 1:params$nb_iter_total) {\n  scores_iter[[i_iter]] &lt;- predict_score_iter(\n    fit_xgb = fit_xgb,\n    tb_train_xgb = tb_train_xgb,\n    tb_valid_xgb = tb_valid_xgb,\n    tb_calib_xgb = tb_calib_xgb,\n    tb_test_xgb = tb_test_xgb,\n    nb_iter = i_iter\n  )\n}\n\nWe are going to calculate different metrics for a single number of iterations (here 1 iteration only):\n\ni_iter &lt;- 1\nscores &lt;- scores_iter[[i_iter]]\nscores_train &lt;- scores$scores_train\nscores_valid &lt;- scores$scores_valid\nscores_calib &lt;- scores$scores_calib\nscores_test &lt;- scores$scores_test\n\nWe recalibrate those predicted scores (calibration and test sets only) using either Platt scaling or isotonic regression:\n\n# Recalibration\n# Platt scaling\nres_recalibration_platt &lt;- recalibrate(\n  obs_calib = data_calib |&gt; dplyr::pull(!!target_name),\n  obs_test = data_test |&gt; dplyr::pull(!!target_name), \n  pred_calib = scores_calib, \n  pred_test = scores_test, \n  method = \"platt\"\n)\nscores_c_platt_calib &lt;- res_recalibration_platt$tb_score_c_calib$p_c\nscores_c_platt_test &lt;- res_recalibration_platt$tb_score_c_test$p_c\n  \n# Isotonic regression\nres_recalibration_iso &lt;- recalibrate(\n  obs_calib = data_calib |&gt; dplyr::pull(!!target_name), \n  obs_test = data_test |&gt; dplyr::pull(!!target_name), \n  pred_calib = scores_calib, \n  pred_test = scores_test, \n  method = \"isotonic\"\n)\nscores_c_iso_calib &lt;- res_recalibration_iso$tb_score_c_calib$p_c\nscores_c_iso_test &lt;- res_recalibration_iso$tb_score_c_test$p_c\n\nHere are the histograms of predicted scores:\n\n## Histogram of scores----\nbreaks &lt;- seq(0, 1, by = .05)\nscores_train_hist &lt;- hist(scores_train, breaks = breaks, plot = FALSE)\nscores_calib_hist &lt;- hist(scores_calib, breaks = breaks, plot = FALSE)\nscores_valid_hist &lt;- hist(scores_valid, breaks = breaks, plot = FALSE)\nscores_test_hist &lt;- hist(scores_test, breaks = breaks, plot = FALSE)\nscores_c_platt_calib_hist &lt;- hist(scores_c_platt_calib, breaks = breaks, plot = FALSE)\nscores_c_platt_test_hist &lt;- hist(scores_c_platt_test, breaks = breaks, plot = FALSE)\nscores_c_iso_calib_hist &lt;- hist(scores_c_iso_calib, breaks = breaks, plot = FALSE)\nscores_c_iso_test_hist &lt;- hist(scores_c_iso_test, breaks = breaks, plot = FALSE)\n\nind &lt;- params$ind\nnb_iter &lt;- i_iter\nscores_hist &lt;- list(\n  train = scores_train_hist,\n  valid = scores_valid_hist,\n  calib = scores_calib_hist,\n  test = scores_test_hist,\n  calib_c_platt = scores_c_platt_calib_hist,\n  test_c_platt = scores_c_platt_test_hist,\n  calib_c_iso = scores_c_iso_calib_hist,\n  test_c_iso = scores_c_iso_test_hist,\n  ind = ind,\n  nb_iter = nb_iter,\n  max_depth = params$max_depth\n)\n\nWe can also calculate the interquantile distances of recalibrated and non-recalibrated predicted scores:\n\n ## Estimation of P(q1 &lt; score &lt; q2)----\nprop_btw_q_h &lt;- function(s, sample_name, recalib_name) {\n  map(\n    c(.1, .2, .3, .4),\n    ~prop_btw_quantiles(s = s, q1 = .x)\n    ) |&gt;\n    list_rbind() |&gt;\n    mutate(sample = sample_name, recalib = recalib_name)\n}\n  \nproq_scores_train &lt;- prop_btw_q_h(\n  scores_train, sample_name = \"train\", recalib_name = \"none\"\n)\nproq_scores_valid &lt;- prop_btw_q_h(\n  scores_valid, sample_name = \"valid\", recalib_name = \"none\"\n)\nproq_scores_calib &lt;- prop_btw_q_h(\n  scores_calib, sample_name = \"calib\", recalib_name = \"none\"\n)\nproq_scores_test &lt;- prop_btw_q_h(\n  scores_test, sample_name = \"test\", recalib_name = \"none\"\n)\nproq_scores_c_platt_calib &lt;- prop_btw_q_h(\n  scores_c_platt_calib, sample_name = \"calib\", recalib_name = \"platt\"\n)\nproq_scores_c_platt_test &lt;- prop_btw_q_h(\n  scores_c_platt_test, sample_name = \"test\", recalib_name = \"platt\"\n)\nproq_scores_c_iso_calib &lt;- prop_btw_q_h(\n  scores_c_iso_calib, sample_name = \"calib\", recalib_name = \"isotonic\"\n)\nproq_scores_c_iso_test &lt;- prop_btw_q_h(\n  scores_c_iso_test, sample_name = \"test\", recalib_name = \"isotonic\"\n)\n\nAnd the Kullback-Leibler divergence between predicted scores and Beta prior (obtained with GAMSEL model):\n\n ## Dispersion Metrics----\ndisp_train &lt;- disp_metrics_dataset(\n  prior = prior, scores = scores_train\n  ) |&gt; \n  mutate(sample = \"train\", recalib = \"none\")\n  \ndisp_valid &lt;- disp_metrics_dataset(\n  prior = prior, scores = scores_valid\n  ) |&gt;\n  mutate(sample = \"valid\", recalib = \"none\")\n  \ndisp_calib &lt;- disp_metrics_dataset(\n  prior = prior, scores = scores_calib\n  ) |&gt; \n  mutate(sample = \"calib\", recalib = \"none\")\n  \ndisp_test &lt;- disp_metrics_dataset(\n  prior = prior, scores = scores_test\n  ) |&gt; \n  mutate(sample = \"test\", recalib = \"none\")\n\ndisp_c_platt_calib &lt;- disp_metrics_dataset(\n  prior = prior, scores = scores_c_platt_calib\n  ) |&gt;\n  mutate(sample = \"calib\", recalib = \"platt\")\n\ndisp_c_platt_test &lt;- disp_metrics_dataset(\n  prior = prior, scores = scores_c_platt_test\n  ) |&gt;\n  mutate(sample = \"test\", recalib = \"platt\")\n\ndisp_c_iso_calib &lt;- disp_metrics_dataset(\n  prior = prior, scores = scores_c_iso_calib\n  ) |&gt;\n  mutate(sample = \"calib\", recalib = \"isotonic\")\n  \ndisp_c_iso_test &lt;- disp_metrics_dataset(\n  prior = prior, scores = scores_c_iso_test\n  ) |&gt;\n  mutate(sample = \"test\", recalib = \"isotonic\")\n\nFinally, the performance metrics:\n\n# Performance and Calibration Metrics----\n# We add very small noise to predicted scores\n# otherwise the local regression may crash\nscores_train_noise &lt;- scores_train +\n  runif(n = length(scores_train), min = 0, max = 0.01)\nscores_train_noise[scores_train_noise &gt; 1] &lt;- 1\nmetrics_train &lt;- compute_metrics(\n  obs = data_train |&gt; pull(!!target_name),\n  scores = scores_train_noise, true_probas = NULL\n  ) |&gt; mutate(sample = \"train\", recalib = \"none\")\n\nscores_valid_noise &lt;- scores_valid +\n  runif(n = length(scores_valid), min = 0, max = 0.01)\nscores_valid_noise[scores_valid_noise &gt; 1] &lt;- 1\nmetrics_valid &lt;- compute_metrics(\n  obs = data_valid |&gt; pull(!!target_name),\n  scores = scores_valid_noise, true_probas = NULL\n  ) |&gt; mutate(sample = \"valid\", recalib = \"none\")\n  \nscores_calib_noise &lt;- scores_calib +\n  runif(n = length(scores_calib), min = 0, max = 0.01)\nscores_calib_noise[scores_calib_noise &gt; 1] &lt;- 1\nmetrics_calib &lt;- compute_metrics(\n  obs = data_calib |&gt; pull(!!target_name),\n  scores = scores_calib_noise, true_probas = NULL\n  ) |&gt; mutate(sample = \"calib\", recalib = \"none\")\n  \nscores_test_noise &lt;- scores_test +\n  runif(n = length(scores_test), min = 0, max = 0.01)\nscores_test_noise[scores_test_noise &gt; 1] &lt;- 1\nmetrics_test &lt;- compute_metrics(\n  obs = data_test |&gt; pull(!!target_name),\n  scores = scores_test_noise, true_probas = NULL\n  ) |&gt; mutate(sample = \"test\", recalib = \"none\")\n  \n# With recalibrated scores (platt)\nscores_c_platt_calib_noise &lt;- scores_c_platt_calib +\n  runif(n = length(scores_calib), min = 0, max = 0.01)\nscores_c_platt_calib_noise[scores_c_platt_calib_noise &gt; 1] &lt;- 1\nmetrics_c_platt_calib &lt;- compute_metrics(\n  obs = data_calib |&gt; pull(!!target_name),\n  scores = scores_c_platt_calib_noise, true_probas = NULL\n  ) |&gt; mutate(sample = \"calib\", recalib = \"platt\")\n  \nscores_c_platt_test_noise &lt;- scores_c_platt_test +\n  runif(n = length(scores_test), min = 0, max = 0.01)\nscores_c_platt_test_noise[scores_c_platt_test_noise &gt; 1] &lt;- 1\nmetrics_c_platt_test &lt;- compute_metrics(\n  obs = data_test |&gt; pull(!!target_name),\n  scores = scores_c_platt_test_noise, true_probas = NULL\n  ) |&gt; mutate(sample = \"test\", recalib = \"platt\")\n  \n# With recalibrated scores (isotonic)\nscores_c_iso_calib_noise &lt;- scores_c_iso_calib +\n  runif(n = length(scores_c_iso_calib), min = 0, max = 0.01)\nscores_c_iso_calib_noise[scores_c_iso_calib_noise &gt; 1] &lt;- 1\nmetrics_c_iso_calib &lt;- compute_metrics(\n  obs = data_calib |&gt; pull(!!target_name),\n  scores = scores_c_iso_calib_noise, true_probas = NULL\n  ) |&gt; mutate(sample = \"calib\", recalib = \"isotonic\")\n  \nscores_c_iso_test_noise &lt;- scores_c_iso_test +\n  runif(n = length(scores_c_iso_test), min = 0, max = 0.01)\nscores_c_iso_test_noise[scores_c_iso_test_noise &gt; 1] &lt;- 1\nmetrics_c_iso_test &lt;- compute_metrics(\n  obs = data_test |&gt; pull(!!target_name),\n  scores = scores_c_iso_test_noise, true_probas = NULL\n  ) |&gt; mutate(sample = \"test\", recalib = \"isotonic\")\n\nWe wrap-up all metrics (calibration, dispersion and performance):\n\ntb_metrics &lt;- metrics_train |&gt;\n  bind_rows(metrics_valid) |&gt;\n  bind_rows(metrics_calib) |&gt;\n  bind_rows(metrics_test) |&gt;\n  bind_rows(metrics_c_platt_calib) |&gt;\n  bind_rows(metrics_c_platt_test) |&gt;\n  bind_rows(metrics_c_iso_calib) |&gt;\n  bind_rows(metrics_c_iso_test) |&gt;\n  mutate(ind = ind, nb_iter = nb_iter, max_depth = params$max_depth) |&gt;\n  select(-c(mse, mae))\n\ntb_disp_metrics &lt;- disp_train |&gt;\n  bind_rows(disp_valid) |&gt; \n  bind_rows(disp_calib) |&gt; \n  bind_rows(disp_test) |&gt; \n  bind_rows(disp_c_platt_calib) |&gt; \n  bind_rows(disp_c_platt_test) |&gt; \n  bind_rows(disp_c_iso_calib) |&gt; \n  bind_rows(disp_c_iso_test) |&gt;\n  mutate(ind = ind, nb_iter = nb_iter, max_depth = params$max_depth)\n\ntb_metrics &lt;- tb_metrics |&gt; left_join(\n  tb_disp_metrics, by = c(\"sample\", \"recalib\", \"ind\", \"nb_iter\", \"max_depth\")\n)\n\ntb_prop_scores &lt;- proq_scores_train |&gt;\n  bind_rows(proq_scores_valid) |&gt;\n  bind_rows(proq_scores_calib) |&gt;\n  bind_rows(proq_scores_test) |&gt;\n  bind_rows(proq_scores_c_platt_calib) |&gt;\n  bind_rows(proq_scores_c_platt_test) |&gt;\n  bind_rows(proq_scores_c_iso_calib) |&gt;\n  bind_rows(proq_scores_c_iso_test) |&gt;\n  mutate(ind = ind, nb_iter = nb_iter, max_depth = params$max_depth)\n\nmetrics_iter_1 &lt;- list(\n  ind = ind,\n  nb_iter = nb_iter,\n  max_depth = params$max_depth,\n  tb_metrics = tb_metrics,\n  tb_prop_scores = tb_prop_scores,\n  scores_hist = scores_hist\n  )\nmetrics_iter &lt;- list(metrics_iter_1)\n\ntb_metrics &lt;-\n  map(metrics_iter, \"tb_metrics\") |&gt;\n  list_rbind()\ntb_prop_scores &lt;-\n  map(metrics_iter, \"tb_prop_scores\") |&gt;\n  list_rbind()\nscores_hist &lt;- map(metrics_iter, \"scores_hist\")\n\nres_grid &lt;- vector(mode = \"list\", length = 1)\nres_grid[[i_grid]] &lt;- list(\n  tb_metrics = tb_metrics,\n  tb_prop_scores = tb_prop_scores,\n  scores_hist = scores_hist\n  )\n\nWe can also the reproduce the previous steps using the different functions defined above:\n\n# Results for a single iteration\nresul_iter_1 &lt;- get_metrics_xgb_iter(\n  scores = scores,\n  prior = prior,\n  data_train = data_train,\n  data_valid = data_valid,\n  data_calib = data_calib,\n  data_test = data_test,\n  target_name = target_name,\n  ind = ind,\n  nb_iter = i_iter,\n  params = params\n)",
    "crumbs": [
      "IV. Real Data",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Recalibration: Example</span>"
    ]
  },
  {
    "objectID": "example_spambase_xgb.html#final-results-spambase-dataset",
    "href": "example_spambase_xgb.html#final-results-spambase-dataset",
    "title": "8  Recalibration: Example",
    "section": "8.5 Final results: “spambase” dataset",
    "text": "8.5 Final results: “spambase” dataset\nWe calculate the performance/calibration/dispersion metrics for all iterations and all hyperparameters of the grid:\n\nxgb_resul &lt;- simul_xgb_real(\n  data,\n  target_name,\n  prior,\n  seed = 1234\n)\n\nWe save the results for the dataset “spambase”:\n\nsave(xgb_resul, file = str_c(\"../output/real-data/xgb_resul_\", name, \".rda\"))\n\nWe can now load the results:\n\nload(str_c(\"../output/real-data/xgb_resul_\", name, \".rda\"))",
    "crumbs": [
      "IV. Real Data",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Recalibration: Example</span>"
    ]
  },
  {
    "objectID": "example_spambase_xgb.html#results",
    "href": "example_spambase_xgb.html#results",
    "title": "8  Recalibration: Example",
    "section": "8.6 Results",
    "text": "8.6 Results\n\nmetrics_xgb_all &lt;- xgb_resul$metrics_simul |&gt;\n  mutate(\n    sample = factor(\n      sample,\n      levels = c(\"train\", \"valid\", \"calib\", \"test\"),\n      labels = c(\"Train\",\"Validation\", \"Calibration\" ,\"Test\")\n    ),\n    recalib = factor(\n      recalib,\n      levels = c(\"none\", \"platt\", \"isotonic\"),\n      labels = c(\"None\", \"Platt\", \"Isotonic\")\n    )\n  )\n\nFor each replication, we made some hyperparameters vary. Let us identify some models of interest:\n\nsmallest: model with the lowest number of boosting iteration\nlargest: model with the highest number of boosting iteration\nlargest_auc: model with the highest AUC on validation set\nlowest_mse: model with the lowest MSE on validation set\nlowest_ici: model with the lowest ICI on validation set\nlowest_kl: model with the lowest KL Divergence on validation set\nmediocre_ici: model with a rather high ICI (uncalibrated model)\n\n\n\nCode\n# Identify the smallest tree on the validation set, when the scores are not\n# recalibrated\nsmallest_xgb &lt;-\n  metrics_xgb_all |&gt;\n  filter(sample == \"Validation\", recalib == \"None\") |&gt;\n  arrange(nb_iter) |&gt;\n  slice_head(n = 1) |&gt;\n  select(ind, nb_iter, recalib) |&gt;\n  mutate(result_type = \"smallest\")\n\n# Identify the largest tree\nlargest_xgb &lt;-\n  metrics_xgb_all |&gt;\n  filter(sample == \"Validation\", recalib == \"None\") |&gt;\n  arrange(desc(nb_iter)) |&gt;\n  slice_head(n = 1) |&gt;\n  select(ind, nb_iter, recalib) |&gt;\n  mutate(result_type = \"largest\")\n\n# Identify tree with highest AUC on test set\nhighest_auc_xgb &lt;-\n  metrics_xgb_all |&gt;\n  filter(sample == \"Validation\", recalib == \"None\") |&gt;\n  arrange(desc(AUC)) |&gt;\n  slice_head(n = 1) |&gt;\n  select(ind, nb_iter, recalib) |&gt;\n  mutate(result_type = \"largest_auc\")\n\n# Identify tree with lowest brier\nlowest_brier_xgb &lt;-\n  metrics_xgb_all |&gt;\n  filter(sample == \"Validation\", recalib == \"None\") |&gt;\n  arrange(brier) |&gt;\n  slice_head(n = 1) |&gt;\n  select(ind, nb_iter, recalib) |&gt;\n  mutate(result_type = \"lowest_brier\")\n\n# Identify tree with lowest ICI\nlowest_ici_xgb &lt;-\n  metrics_xgb_all |&gt;\n  filter(sample == \"Validation\", recalib == \"None\") |&gt;\n  arrange(ici) |&gt;\n  slice_head(n = 1) |&gt;\n  select(ind, nb_iter, recalib) |&gt;\n  mutate(result_type = \"lowest_ici\")\n\n# Identify tree with lowest KL\nlowest_kl_xgb &lt;-\n  metrics_xgb_all |&gt;\n  filter(sample == \"Validation\", recalib == \"None\") |&gt;\n  arrange(KL_20_true_probas) |&gt;\n  slice_head(n = 1) |&gt;\n  select(ind, nb_iter, recalib) |&gt;\n  mutate(result_type = \"lowest_kl\")\n\nmediocre_ici_xgb &lt;- \n  metrics_xgb_all |&gt;\n  filter(sample == \"Validation\", recalib == \"None\") |&gt;\n  arrange(desc(ici)) |&gt;\n  #slice_head(n = 1) |&gt;\n  filter(nb_iter == 10 & ind == 1) |&gt;\n  select(ind, nb_iter, recalib) |&gt;\n  mutate(result_type = \"mediocre_ici\")\n\n# Merge these\nmodels_of_interest_xgb &lt;-\n  smallest_xgb |&gt;\n  bind_rows(largest_xgb) |&gt;\n  bind_rows(highest_auc_xgb) |&gt;\n  bind_rows(lowest_brier_xgb) |&gt;\n  bind_rows(lowest_ici_xgb) |&gt;\n  bind_rows(lowest_kl_xgb) |&gt; \n  bind_rows(mediocre_ici_xgb)\n\nmodels_of_interest_metrics &lt;- NULL\nfor (recalibration_method in c(\"None\", \"Platt\", \"Isotonic\")) {\n  # Add metrics now\n  models_of_interest_metrics &lt;-\n    models_of_interest_metrics |&gt;\n    bind_rows(\n      models_of_interest_xgb |&gt; select(-recalib) |&gt;\n        left_join(\n          metrics_xgb_all |&gt;\n            filter(\n              recalib == recalibration_method,\n              sample %in% c(\"Validation\", \"Test\")\n            ),\n          by = c(\"ind\", \"nb_iter\"),\n          relationship = \"many-to-many\" # (calib, test)\n        )\n    )\n}\n\n\nmodels_of_interest_metrics &lt;-\n  models_of_interest_metrics |&gt;\n  mutate(\n    result_type = factor(\n      result_type,\n      levels = c(\n        \"smallest\", \"largest\", \"lowest_mse\", \"largest_auc\",\n        \"lowest_brier\", \"lowest_ici\", \"lowest_kl\", \"mediocre_ici_xgb\"),\n      labels = c(\n        \"Smallest\", \"Largest\", \"MSE*\", \"AUC*\",\n        \"Brier*\", \"ICI*\", \"KL*\", \"High ICI\"\n      )\n    )\n  )\n\n\n\nmodels_of_interest_metrics |&gt; \n  mutate(\n    across(\n      c(\n      \"acc\", \"AUC\", \"brier\", \"ici\", \"log_loss\", \"inter_quantile_25_75\",\n      \"inter_quantile_10_90\", starts_with(\"KL_\"), \"ind_cov\", \n      starts_with(\"prior_\")\n      ), \n      ~round(.x, 3)\n    )\n  ) |&gt; \n  DT::datatable()\n\n\n\nTable 8.2: Metrics for spambase dataset\n\n\n\n\n\n\n\n\n\n\n\n8.6.1 Distribution of Scores\nLet us extract all the histogram information computed over the simulations and put that in a single object, scores_hist_all.\n\nscores_hist_all &lt;- xgb_resul$scores_hist\n\nWe then define a function, plot_bp_xgb() which plots the distribution of scores on the test set for the “spambase” dataset. We also define a helper function, plot_bp_interest(), which plots the histogram of the scores at a specific iteration number. We will then be able to plot the distributions at the beginning of the boosting iterations, at the end, at a point where the AUC was the highest on the validation set, and at a point where the KL divergence between the distribution of scores on the validation set and the distribution of the true probabilities was the lowest. We will plot the distributions of the scores returned by the classifier, as well as those obtained with the reclibrators.\n\n\nFunction plot_metrics()\nplot_bp_interest &lt;- function(metrics_interest,\n                             scores_hist_interest,\n                             label,\n                             recalib_method) {\n  subtitle &lt;- str_c(\n    \"Depth = \", metrics_interest$max_depth, \", \",\n    \"AUC = \", round(metrics_interest$AUC, 2), \", \\n\",\n    \"Brier = \", round(metrics_interest$brier, 2), \",\",\n    \"ICI = \", round(metrics_interest$ici, 2), \", \",\n    \"KL = \", round(metrics_interest$KL_20_true_probas, 2)\n  )\n\n  if (recalib_method == \"none\") {\n    plot(\n      main = str_c(label, \" (iter = \", metrics_interest$nb_iter,\")\"),\n      scores_hist_interest$test,\n      xlab = latex2exp::TeX(\"$\\\\hat{s}(x)$\"),\n      ylab = \"\"\n    )\n  } else if (recalib_method == \"platt\") {\n    plot(\n      main = str_c(label, \" (iter = \", metrics_interest$nb_iter,\")\"),\n      scores_hist_interest$test_c_platt,\n      xlab = latex2exp::TeX(\"$\\\\hat{s}(x)$\"),\n      ylab = \"\",\n      col = colour_recalib[[\"Platt\"]]\n    )\n  } else if (recalib_method == \"iso\") {\n    plot(\n      main = str_c(label, \" (iter = \", metrics_interest$nb_iter,\")\"),\n      scores_hist_interest$test_c_iso,\n      xlab = latex2exp::TeX(\"$\\\\hat{s}(x)$\"),\n      ylab = \"\",\n      col = colour_recalib[[\"Isotonic\"]]\n    )\n  }\n  mtext(side = 3, line = -0.25, adj = .5, subtitle, cex = .5)\n}\n\nplot_bp_xgb &lt;- function(paper_version = FALSE) {\n  # Focus on current scenario\n  #scores_hist_scenario &lt;- scores_hist_all[[scenario]]\n  # Focus on a particular replication\n  #scores_hist_repn &lt;- scores_hist_scenario[[repn]]\n  # # Focus on a value for max_depth\n  max_depth_val &lt;- map_dbl(scores_hist_all, ~.x[[1]]$max_depth)\n  # i_max_depth &lt;- which(max_depth_val == max_depth)\n  # scores_hist &lt;- scores_hist_repn[[i_max_depth]]\n  \n  # True Probabilities\n  #simu_data &lt;- simulate_data_wrapper(\n    #scenario = scenario,\n    #params_df = params_df,\n   # repn = repn # only one replication here\n  #)\n  true_prob &lt;- prior$scores_gamsel$scores_train\n  #true_prob &lt;-  simu_data$data$probs_train\n  \n  for (recalib_method in c(\"none\", \"platt\", \"iso\")) {\n    \n    i_method &lt;- match(recalib_method, c(\"none\", \"platt\", \"iso\"))\n    recalib_method_lab &lt;- c(\"None\", \"Platt\", \"Isotonic\")[i_method]\n    \n    # The metrics for the corresponding results, on the validation set\n    metrics_xgb_current_valid &lt;-\n      metrics_xgb_all |&gt;\n      filter(\n        sample == \"Validation\",\n        recalib == \"None\"\n      )\n    # and on the test set\n    metrics_xgb_current_test &lt;-\n      metrics_xgb_all |&gt;\n      filter(\n        sample == \"Test\",\n        recalib == recalib_method_lab\n      )\n    \n    if (paper_version == FALSE) {\n      hist(\n        true_prob,\n        breaks = seq(0, 1, by = .05),\n        xlab = \"p\", ylab = \"\",\n        main = \"Prior Probabilities\",\n        xlim = c(0, 1)\n      )\n      mtext(\n        side = 2, recalib_method_lab, line = 2.5, cex = 1,\n        font.lab = 2\n      )\n      # Iterations of interest----\n      ## Start of iterations\n      scores_hist_start &lt;- scores_hist_all[[1]][[1]]\n      metrics_start &lt;- metrics_xgb_current_test |&gt;\n        filter(\n          nb_iter == scores_hist_start$nb_iter,\n          max_depth == scores_hist_start$max_depth\n        )\n      \n      plot_bp_interest(\n        metrics_interest = metrics_start,\n        scores_hist_interest = scores_hist_start,\n        label = \"Start\",\n        recalib_method = recalib_method\n      )\n      \n      ## End of iterations\n      scores_hist_end &lt;- scores_hist_all[[1]][[length(scores_hist_all[[1]])]]\n      metrics_end &lt;- metrics_xgb_current_test |&gt;\n        filter(\n          nb_iter == scores_hist_end$nb_iter,\n          max_depth == scores_hist_start$max_depth\n        )\n      plot_bp_interest(\n        metrics_interest = metrics_end,\n        scores_hist_interest = scores_hist_end,\n        label = \"End\",\n        recalib_method = recalib_method\n      )\n    }\n    ## Iteration with max AUC on validation set\n    metrics_valid_auc_star &lt;-\n      metrics_xgb_current_valid |&gt; arrange(desc(AUC)) |&gt; dplyr::slice(1)\n    nb_iter_auc &lt;- metrics_valid_auc_star$nb_iter\n    max_depth_auc_star &lt;- metrics_valid_auc_star$max_depth\n    i_max_depth_auc_star &lt;- which(max_depth_val == max_depth_auc_star)\n    \n    metrics_max_auc &lt;-\n      metrics_xgb_current_test |&gt;\n      filter(nb_iter == !!nb_iter_auc, max_depth == max_depth_auc_star)\n    # Note: indexing at 0 here...\n    ind_auc &lt;- which(map_dbl(scores_hist_all[[i_max_depth_auc_star]], \"nb_iter\") == nb_iter_auc)\n    scores_hist_max_auc &lt;- scores_hist_all[[i_max_depth_auc_star]][[ind_auc]]\n    plot_bp_interest(\n      metrics_interest = metrics_max_auc,\n      scores_hist_interest = scores_hist_max_auc,\n      label = \"AUC*\",\n      recalib_method = recalib_method\n    )\n    if (paper_version == TRUE) {\n      mtext(\n        side = 2, recalib_method_lab, line = 2.5, cex = 1,\n        font.lab = 2\n      )\n    }\n    \n    ## Min Brier on validation set\n    metrics_valid_brier_star &lt;-\n      metrics_xgb_current_valid |&gt; arrange(brier) |&gt; dplyr::slice(1)\n    nb_iter_brier &lt;- metrics_valid_brier_star$nb_iter\n    max_depth_brier_star &lt;- metrics_valid_brier_star$max_depth\n    i_max_depth_brier_star &lt;- which(max_depth_val == max_depth_brier_star)\n    \n    metrics_min_brier &lt;-\n      metrics_xgb_current_test |&gt;\n      filter(nb_iter == !!nb_iter_brier, max_depth == max_depth_brier_star)\n    ind_brier &lt;- which(map_dbl(scores_hist_all[[i_max_depth_brier_star]], \"nb_iter\") == nb_iter_brier)\n    scores_hist_min_brier &lt;- scores_hist_all[[i_max_depth_brier_star]][[ind_brier]]\n    plot_bp_interest(\n      metrics_interest = metrics_min_brier,\n      scores_hist_interest = scores_hist_min_brier,\n      label = \"Brier*\",\n      recalib_method = recalib_method\n    )\n    \n    ## Min ICI on validation set\n    metrics_valid_ici_star &lt;-\n      metrics_xgb_current_valid |&gt; arrange(ici) |&gt; dplyr::slice(1)\n    nb_iter_ici &lt;-   metrics_valid_ici_star$nb_iter\n    max_depth_ici_star &lt;- metrics_valid_ici_star$max_depth\n    i_max_depth_ici_star &lt;- which(max_depth_val == max_depth_ici_star)\n    \n    metrics_min_ici &lt;-\n      metrics_xgb_current_test |&gt;\n      filter(nb_iter == !!nb_iter_ici, max_depth == max_depth_ici_star)\n    ind_ici &lt;- which(map_dbl(scores_hist_all[[i_max_depth_ici_star]], \"nb_iter\") == nb_iter_ici)\n    scores_hist_min_ici &lt;- scores_hist_all[[i_max_depth_ici_star]][[ind_ici]]\n    plot_bp_interest(\n      metrics_interest = metrics_min_ici,\n      scores_hist_interest = scores_hist_min_ici,\n      label = \"ICI*\",\n      recalib_method = recalib_method\n    )\n    \n    ## Min KL on validation set\n    metrics_valid_kl_star &lt;-\n      metrics_xgb_current_valid |&gt; arrange(KL_20_true_probas) |&gt; dplyr::slice(1)\n    nb_iter_kl &lt;- metrics_valid_kl_star$nb_iter\n    max_depth_kl_star &lt;- metrics_valid_kl_star$max_depth\n    i_max_depth_kl_star &lt;- which(max_depth_val == max_depth_kl_star)\n    \n    metrics_min_kl &lt;-\n      metrics_xgb_current_test |&gt;\n      filter(nb_iter == !!nb_iter_kl, max_depth == max_depth_kl_star)\n    ind_kl &lt;- which(map_dbl(scores_hist_all[[i_max_depth_kl_star]], \"nb_iter\") == nb_iter_kl)\n    scores_hist_min_kl &lt;- scores_hist_all[[i_max_depth_kl_star]][[ind_kl]]\n    plot_bp_interest(\n      metrics_interest = metrics_min_kl,\n      scores_hist_interest = scores_hist_min_kl,\n      label = \"KL*\",\n      recalib_method = recalib_method\n    )\n    \n    ## Mediocre ICI on validation set\n    identified_mici &lt;-  mediocre_ici_xgb\n    \n    metrics_valid_mici_star &lt;- metrics_xgb_current_valid |&gt; \n      filter(ind == identified_mici$ind, nb_iter == identified_mici$nb_iter)\n    nb_iter_mici &lt;- metrics_valid_mici_star$nb_iter\n    max_depth_mici_star &lt;- metrics_valid_mici_star$max_depth\n    i_max_depth_mici_star &lt;- which(max_depth_val == max_depth_mici_star)\n    \n    metrics_mici &lt;-\n      metrics_xgb_current_test |&gt;\n      filter(nb_iter == !!nb_iter_mici, max_depth == max_depth_mici_star)\n    ind_mici &lt;- which(map_dbl(scores_hist_all[[i_max_depth_mici_star]], \"nb_iter\") == nb_iter_mici)\n    scores_hist_mici &lt;- scores_hist_all[[i_max_depth_mici_star]][[ind_mici]]\n    plot_bp_interest(\n      metrics_interest = metrics_mici,\n      scores_hist_interest = scores_hist_mici,\n      label = \"Med. ICI\",\n      recalib_method = recalib_method\n    )\n  }\n}\n\n\n\n\nCode\npar(mfrow = c(3,8), mar = c(4.1, 4, 3.5, 1.5))\nplot_bp_xgb()\n\n\n\n\n\nFigure 8.1: Distribution of scores on the test set (spambase)\n\n\n\n\n\n\n\n\n\ntable_models_interest &lt;- \n  models_of_interest_metrics |&gt; \n  filter(sample == \"Test\") |&gt; \n  select(\n    recalib, sample, result_type, \n    brier, ici, kl = KL_20_true_probas\n  ) |&gt; \n  filter(\n    result_type %in% c(\"AUC*\", \"KL*\", \"Largest\")\n  )\n\n\ninitial_points &lt;- table_models_interest |&gt; \n  filter(recalib == \"None\")\n\npoints_after_c &lt;- initial_points |&gt; \n  select(result_type, ici, kl) |&gt; \n  left_join(\n    table_models_interest |&gt; \n      filter(recalib %in% c(\"Platt\", \"Isotonic\")) |&gt; \n      select(recalib, result_type, ici, kl) |&gt; \n      rename(ici_end = ici, kl_end = kl),\n    relationship = \"many-to-many\" # (Platt and Isotonic)\n  )\n\nJoining with `by = join_by(result_type)`\n\nggplot() +\n  geom_point(\n    data = initial_points,\n    mapping = aes(x = ici, y = kl, shape = result_type),\n    size = 2\n    ) +\n  geom_segment(\n    data = points_after_c,\n    mapping = aes(\n      x = ici, y = kl, xend = ici_end, yend = kl_end,\n      colour = recalib, linetype = recalib\n      ),\n    arrow = arrow(length=unit(.2, 'cm'))\n  ) +\n  scale_shape_manual(\n    NULL,\n    values = c(\n      # \"Smallest\" = 1,\n      \"Largest\" = 2,\n      \"AUC*\" = 19,\n      \"KL*\" = 15\n    )\n  ) +\n  labs(x = \"ICI\", y = \"KL Divergence\") +\n  scale_colour_manual(\"Recalibration\", values = colour_recalib) +\n  scale_linetype_discrete(\"Recalibration\") +\n  theme_paper()",
    "crumbs": [
      "IV. Real Data",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Recalibration: Example</span>"
    ]
  },
  {
    "objectID": "book_real_xgb.html",
    "href": "book_real_xgb.html",
    "title": "9  Estimations",
    "section": "",
    "text": "9.1 Functions",
    "crumbs": [
      "IV. Real Data",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Estimations</span>"
    ]
  },
  {
    "objectID": "book_real_xgb.html#functions",
    "href": "book_real_xgb.html#functions",
    "title": "9  Estimations",
    "section": "",
    "text": "9.1.1 Recalibration\n\n#' Recalibrates scores using a calibration\n#' \n#' @param obs_calib vector of observed events in the calibration set\n#' @param scores_calib vector of predicted probabilities in the calibration set\n#' @param obs_test vector of observed events in the test set\n#' @param scores_test vector of predicted probabilities in the test set\n#' @param method recalibration method (`\"platt\"` for Platt scaling, \n#'   `\"isotonic\"` for isotonic regression)\n#' @returns list of two elements: recalibrated scores on the calibration set,\n#'   recalibrated scores on the test set\nrecalibrate &lt;- function(obs_calib,\n                        obs_test,\n                        pred_calib,\n                        pred_test,\n                        method = c(\"platt\", \"isotonic\")) {\n  data_calib &lt;- tibble(d = obs_calib, scores = pred_calib)\n  data_test &lt;- tibble(d = obs_test, scores = pred_test)\n  \n  if (method == \"platt\") {\n    lr &lt;- glm(d ~ scores, family = binomial(link = 'logit'), data = data_calib)\n    score_c_calib &lt;- predict(lr, newdata = data_calib, type = \"response\")\n    score_c_test &lt;- predict(lr, newdata = data_test, type = \"response\")\n  } else if (method == \"isotonic\") {\n    iso &lt;- isoreg(x = data_calib$scores, y = data_calib$d)\n    fit_iso &lt;- as.stepfun(iso)\n    score_c_calib &lt;- fit_iso(data_calib$scores)\n    score_c_test &lt;- fit_iso(data_test$scores)\n    \n  } else {\n    stop(\"Unrecognized method: platt or isotonic only\")\n  }\n  # Format results in tibbles:\n  # For calibration set\n  tb_score_c_calib &lt;- tibble(\n    d = obs_calib,\n    p_u = pred_calib,\n    p_c = score_c_calib\n  )\n  # For test set\n  tb_score_c_test &lt;- tibble(\n    d = obs_test,\n    p_u = pred_test,\n    p_c = score_c_test\n  )\n  \n  list(\n    tb_score_c_calib = tb_score_c_calib,\n    tb_score_c_test = tb_score_c_test\n  )\n  \n}\n\n\n\n9.1.2 Dispersion Metrics with Beta prior\n\n\nFunction dispersion_metrics_beta()\n#' Computes the dispersion and divergence metrics for a vector of scores and\n#' a Beta distribution\n#'\n#' @param shape_1 first parameter of the beta distribution\n#' @param shape_2 second parameter of the beta distribution\n#' @param scores predicted scores\n#'\n#' @returns\n#' \\itemize{\n#'   \\item \\code{inter_quantile_25_75}: Difference of inter-quantile between 25% and 75%\n#'   \\item \\code{inter_quantile_10_90}: Difference of inter-quantile between 10% and 90%\n#'   \\item \\code{KL_10_true_probas}: KL of of predicted probabilities w.r. to true probabilities with 10 bins\n#'   \\item \\code{KL_10_scores}: KL of of true probabilities w.r. to predicted probabilities with 10 bins\n#'   \\item \\code{KL_20_true_probas}: KL of of predicted probabilities w.r. to true probabilities with 20 bins\n#'   \\item \\code{KL_20_scores}: KL of of true probabilities w.r. to predicted probabilities with 20 bins\n#'   \\item \\code{ind_cov}: Difference between the variance of true probabilities and the covariance between true probabilities and predicted scores\n#' }\ndispersion_metrics_beta &lt;- function(shape_1 = 1, shape_2 = 1, scores){\n\n  # Inter-quantiles\n  inter_q_80 &lt;- diff(quantile(scores, c(.9, .1))) /\n    diff(qbeta(c(.9, .1), shape_1, shape_2))\n  inter_q_50 &lt;- diff(quantile(scores, c(.75,.25))) /\n    diff(qbeta(c(.75,.25), shape_1, shape_1))\n\n  # KL divergences\n  m &lt;- 10 # Number of bins\n  h_phat &lt;- hist(scores, breaks = seq(0, 1, length = m + 1), plot = FALSE)\n  h_p &lt;- list(breaks = h_phat$breaks, mids = h_phat$mids)\n  h_p$density = diff(pbeta(h_p$breaks, shape_1, shape_2))\n  h_p$counts =  h_p$density*length(scores)\n\n  # Densities\n  h1 &lt;- rbind(h_phat$density / m, h_p$density / m) # Reference : true probabilities\n  h2 &lt;- rbind(h_p$density / m, h_phat$density / m) # Reference : predicted scores\n  KL_10_true_probas &lt;- distance(\n    h1, method = \"kullback-leibler\", unit = \"log2\", mute.message = TRUE)\n  KL_10_scores &lt;- distance(\n    h2, method = \"kullback-leibler\", unit = \"log2\", mute.message = TRUE)\n\n\n  m &lt;- 20 # Number of bins\n  h_phat &lt;- hist(scores, breaks = seq(0, 1, length = m + 1), plot = FALSE)\n  h_p &lt;- list(breaks = h_phat$breaks, mids = h_phat$mids)\n  h_p$density = diff(pbeta(h_p$breaks, shape_1, shape_2))\n  h_p$counts =  h_p$density * length(scores)\n  # Densities\n  h1 &lt;- rbind(h_phat$density / m, h_p$density) # Reference : true probabilities\n  h2 &lt;- rbind(h_p$density, h_phat$density / m) # Reference : predicted scores\n  KL_20_true_probas &lt;- distance(\n    h1, method = \"kullback-leibler\", unit = \"log2\", mute.message = TRUE)\n  KL_20_scores &lt;- distance(\n    h2, method = \"kullback-leibler\", unit = \"log2\", mute.message = TRUE)\n\n  # Indicator of the difference between variance and covariance\n  var_p &lt;- shape_1 * shape_2 / ((shape_1 + shape_2)^2 * (shape_1 + shape_2 + 1))\n  cov_p_phat &lt;- cov(\n    qbeta(\n      rank(scores, ties.method = \"average\") / (1 + length(scores)),\n      shape_1,\n      shape_2),\n    scores\n  )\n  ind_cov &lt;- abs(cov_p_phat - var_p)\n\n  # Collection\n  dispersion_metrics &lt;- tibble(\n    \"inter_quantile_25_75\" = as.numeric(inter_q_50),\n    \"inter_quantile_10_90\" = as.numeric(inter_q_80),\n    \"KL_10_true_probas\" = as.numeric(KL_10_true_probas),\n    \"KL_10_scores\" = as.numeric(KL_10_scores),\n    \"KL_20_true_probas\" = as.numeric(KL_20_true_probas),\n    \"KL_20_scores\" = as.numeric(KL_20_scores),\n    \"ind_cov\" = ind_cov\n  )\n\n  dispersion_metrics\n}\n\ndisp_metrics_dataset &lt;- function(prior, scores) {\n  # GAMSEL prior\n  shape_1 &lt;- prior$mle_gamsel$estimate[\"shape1\"]\n  shape_2 &lt;- prior$mle_gamsel$estimate[\"shape2\"]\n\n  # Divergence metrics\n  dist_prior_gamsel &lt;- dispersion_metrics_beta(\n    shape_1 = shape_1, shape_2 = shape_2, scores = scores\n  )\n\n  dist_prior_gamsel |&gt;\n    mutate(\n      prior = \"gamsel\", shape_1 = shape_1, shape_2 = shape_2\n      )\n}\n\n\n\n\n9.1.3 Extreme Gradient Boosting\n\n\nFunction get_metrics_xgb_iter()\n#' Get the metrics based on scores estimated at a given boosting iteration\n#'\n#' @param scores scores estimated a boosting iteration `nb_iter` (list with\n#'   train and test scores, returned by `predict_score_iter()`)\n#' @param data_train train set\n#' @param data_valid validation set\n#' @param data_calib calibration set\n#' @param data_test test set\n#' @param target_name name of the target variable\n#' @param ind index of the grid search\n#' @param nb_iter boosting iteration to consider\n#' @param params hyperparameters to consider\n#'\n#' @returns A list with 4 elements:\n#'  - `tb_metrics`: performance / calibration metrics\n#'  - `tb_disp_metrics`: disp and div metrics\n#'  - `tb_prop_scores`: table with P(q1 &lt; score &lt; q2)\n#'  - `scores_hist`: histogram of scores\nget_metrics_xgb_iter &lt;- function(scores,\n                                 prior,\n                                 data_train,\n                                 data_valid,\n                                 data_calib,\n                                 data_test,\n                                 target_name,\n                                 ind,\n                                 nb_iter,\n                                 params) {\n\n  scores_train &lt;- scores$scores_train\n  scores_valid &lt;- scores$scores_valid\n  scores_calib &lt;- scores$scores_calib\n  scores_test &lt;- scores$scores_test\n  \n  # Recalibration\n  # Platt scaling\n  res_recalibration_platt &lt;- recalibrate(\n    obs_calib = data_calib |&gt; dplyr::pull(!!target_name), \n    obs_test = data_test |&gt; dplyr::pull(!!target_name), \n    pred_calib = scores_calib, \n    pred_test = scores_test, \n    method = \"platt\"\n  )\n  scores_c_platt_calib &lt;- res_recalibration_platt$tb_score_c_calib$p_c\n  scores_c_platt_test &lt;- res_recalibration_platt$tb_score_c_test$p_c\n  \n  # Isotonic regression\n  res_recalibration_iso &lt;- recalibrate(\n    obs_calib = data_calib |&gt; dplyr::pull(!!target_name), \n    obs_test = data_test |&gt; dplyr::pull(!!target_name), \n    pred_calib = scores_calib, \n    pred_test = scores_test, \n    method = \"isotonic\"\n  )\n  scores_c_iso_calib &lt;- res_recalibration_iso$tb_score_c_calib$p_c\n  scores_c_iso_test &lt;- res_recalibration_iso$tb_score_c_test$p_c\n\n  ## Metrics----\n  ## Histogram of scores----\n  breaks &lt;- seq(0, 1, by = .05)\n  scores_train_hist &lt;- hist(scores_train, breaks = breaks, plot = FALSE)\n  scores_calib_hist &lt;- hist(scores_calib, breaks = breaks, plot = FALSE)\n  scores_valid_hist &lt;- hist(scores_valid, breaks = breaks, plot = FALSE)\n  scores_test_hist &lt;- hist(scores_test, breaks = breaks, plot = FALSE)\n  scores_c_platt_calib_hist &lt;- hist(scores_c_platt_calib, breaks = breaks, plot = FALSE)\n  scores_c_platt_test_hist &lt;- hist(scores_c_platt_test, breaks = breaks, plot = FALSE)\n  scores_c_iso_calib_hist &lt;- hist(scores_c_iso_calib, breaks = breaks, plot = FALSE)\n  scores_c_iso_test_hist &lt;- hist(scores_c_iso_test, breaks = breaks, plot = FALSE)\n  \n  scores_hist &lt;- list(\n    train = scores_train_hist,\n    valid = scores_valid_hist,\n    calib = scores_calib_hist,\n    test = scores_test_hist,\n    calib_c_platt = scores_c_platt_calib_hist,\n    test_c_platt = scores_c_platt_test_hist,\n    calib_c_iso = scores_c_iso_calib_hist,\n    test_c_iso = scores_c_iso_test_hist,\n    ind = ind,\n    nb_iter = nb_iter,\n    max_depth = params$max_depth\n    \n  )\n  \n  ## Estimation of P(q1 &lt; score &lt; q2)----\n  prop_btw_q_h &lt;- function(s, sample_name, recalib_name) {\n    map(\n      c(.1, .2, .3, .4),\n      ~prop_btw_quantiles(s = s, q1 = .x)\n    ) |&gt;\n      list_rbind() |&gt;\n      mutate(sample = sample_name, recalib = recalib_name)\n  }\n  \n  proq_scores_train &lt;- prop_btw_q_h(\n    scores_train, sample_name = \"train\", recalib_name = \"none\"\n  )\n  proq_scores_valid &lt;- prop_btw_q_h(\n    scores_valid, sample_name = \"valid\", recalib_name = \"none\"\n  )\n  proq_scores_calib &lt;- prop_btw_q_h(\n    scores_calib, sample_name = \"calib\", recalib_name = \"none\"\n  )\n  proq_scores_test &lt;- prop_btw_q_h(\n    scores_test, sample_name = \"test\", recalib_name = \"none\"\n  )\n  proq_scores_c_platt_calib &lt;- prop_btw_q_h(\n    scores_c_platt_calib, sample_name = \"calib\", recalib_name = \"platt\"\n  )\n  proq_scores_c_platt_test &lt;- prop_btw_q_h(\n    scores_c_platt_test, sample_name = \"test\", recalib_name = \"platt\"\n  )\n  proq_scores_c_iso_calib &lt;- prop_btw_q_h(\n    scores_c_iso_calib, sample_name = \"calib\", recalib_name = \"isotonic\"\n  )\n  proq_scores_c_iso_test &lt;- prop_btw_q_h(\n    scores_c_iso_test, sample_name = \"test\", recalib_name = \"isotonic\"\n  )\n\n  ## Dispersion Metrics----\n  disp_train &lt;- disp_metrics_dataset(\n    prior = prior, scores = scores_train\n  ) |&gt; \n    mutate(sample = \"train\", recalib = \"none\")\n  \n  disp_valid &lt;- disp_metrics_dataset(\n    prior = prior, scores = scores_valid\n  ) |&gt;\n    mutate(sample = \"valid\", recalib = \"none\")\n  \n  disp_calib &lt;- disp_metrics_dataset(\n    prior = prior, scores = scores_calib\n  ) |&gt; \n    mutate(sample = \"calib\", recalib = \"none\")\n  \n  disp_test &lt;- disp_metrics_dataset(\n    prior = prior, scores = scores_test\n  ) |&gt; \n    mutate(sample = \"test\", recalib = \"none\")\n  \n  disp_c_platt_calib &lt;- disp_metrics_dataset(\n    prior = prior, scores = scores_c_platt_calib\n  ) |&gt;\n    mutate(sample = \"calib\", recalib = \"platt\")\n  \n  disp_c_platt_test &lt;- disp_metrics_dataset(\n    prior = prior, scores = scores_c_platt_test\n  ) |&gt;\n    mutate(sample = \"test\", recalib = \"platt\")\n  \n  disp_c_iso_calib &lt;- disp_metrics_dataset(\n    prior = prior, scores = scores_c_iso_calib\n  ) |&gt;\n    mutate(sample = \"calib\", recalib = \"isotonic\")\n  \n  disp_c_iso_test &lt;- disp_metrics_dataset(\n    prior = prior, scores = scores_c_iso_test\n  ) |&gt;\n    mutate(sample = \"test\", recalib = \"isotonic\")\n\n  # Performance and Calibration Metrics----\n  # We add very small noise to predicted scores\n  # otherwise the local regression may crash\n  scores_train_noise &lt;- scores_train +\n    runif(n = length(scores_train), min = 0, max = 0.01)\n  scores_train_noise[scores_train_noise &gt; 1] &lt;- 1\n  metrics_train &lt;- compute_metrics(\n    obs = data_train |&gt; pull(!!target_name),\n    scores = scores_train_noise, true_probas = NULL\n  ) |&gt; mutate(sample = \"train\", recalib = \"none\")\n\n  scores_valid_noise &lt;- scores_valid +\n    runif(n = length(scores_valid), min = 0, max = 0.01)\n  scores_valid_noise[scores_valid_noise &gt; 1] &lt;- 1\n  metrics_valid &lt;- compute_metrics(\n    obs = data_valid |&gt; pull(!!target_name),\n    scores = scores_valid_noise, true_probas = NULL\n  ) |&gt; mutate(sample = \"valid\", recalib = \"none\")\n  \n  scores_calib_noise &lt;- scores_calib +\n    runif(n = length(scores_calib), min = 0, max = 0.01)\n  scores_calib_noise[scores_calib_noise &gt; 1] &lt;- 1\n  metrics_calib &lt;- compute_metrics(\n    obs = data_calib |&gt; pull(!!target_name),\n    scores = scores_calib_noise, true_probas = NULL\n  ) |&gt; mutate(sample = \"calib\", recalib = \"none\")\n  \n  scores_test_noise &lt;- scores_test +\n    runif(n = length(scores_test), min = 0, max = 0.01)\n  scores_test_noise[scores_test_noise &gt; 1] &lt;- 1\n  metrics_test &lt;- compute_metrics(\n    obs = data_test |&gt; pull(!!target_name),\n    scores = scores_test_noise, true_probas = NULL\n  ) |&gt; mutate(sample = \"test\", recalib = \"none\")\n  \n  # With recalibrated scores (platt)\n  scores_c_platt_calib_noise &lt;- scores_c_platt_calib +\n    runif(n = length(scores_calib), min = 0, max = 0.01)\n  scores_c_platt_calib_noise[scores_c_platt_calib_noise &gt; 1] &lt;- 1\n  metrics_c_platt_calib &lt;- compute_metrics(\n    obs = data_calib |&gt; pull(!!target_name),\n    scores = scores_c_platt_calib_noise, true_probas = NULL\n  ) |&gt; mutate(sample = \"calib\", recalib = \"platt\")\n  \n  scores_c_platt_test_noise &lt;- scores_c_platt_test +\n    runif(n = length(scores_test), min = 0, max = 0.01)\n  scores_c_platt_test_noise[scores_c_platt_test_noise &gt; 1] &lt;- 1\n  metrics_c_platt_test &lt;- compute_metrics(\n    obs = data_test |&gt; pull(!!target_name),\n    scores = scores_c_platt_test_noise, true_probas = NULL\n  ) |&gt; mutate(sample = \"test\", recalib = \"platt\")\n  \n  # With recalibrated scores (isotonic)\n  scores_c_iso_calib_noise &lt;- scores_c_iso_calib +\n    runif(n = length(scores_c_iso_calib), min = 0, max = 0.01)\n  scores_c_iso_calib_noise[scores_c_iso_calib_noise &gt; 1] &lt;- 1\n  metrics_c_iso_calib &lt;- compute_metrics(\n    obs = data_calib |&gt; pull(!!target_name),\n    scores = scores_c_iso_calib_noise, true_probas = NULL\n  ) |&gt; mutate(sample = \"calib\", recalib = \"isotonic\")\n  \n  scores_c_iso_test_noise &lt;- scores_c_iso_test +\n    runif(n = length(scores_c_iso_test), min = 0, max = 0.01)\n  scores_c_iso_test_noise[scores_c_iso_test_noise &gt; 1] &lt;- 1\n  metrics_c_iso_test &lt;- compute_metrics(\n    obs = data_test |&gt; pull(!!target_name),\n    scores = scores_c_iso_test_noise, true_probas = NULL\n  ) |&gt; mutate(sample = \"test\", recalib = \"isotonic\")\n  \n  tb_metrics &lt;- metrics_train |&gt;\n    bind_rows(metrics_valid) |&gt;\n    bind_rows(metrics_calib) |&gt;\n    bind_rows(metrics_test) |&gt;\n    bind_rows(metrics_c_platt_calib) |&gt;\n    bind_rows(metrics_c_platt_test) |&gt;\n    bind_rows(metrics_c_iso_calib) |&gt;\n    bind_rows(metrics_c_iso_test) |&gt;\n    mutate(ind = ind, nb_iter = nb_iter, max_depth = params$max_depth) |&gt;\n    select(-c(mse, mae))\n  \n  tb_disp_metrics &lt;- disp_train |&gt;\n    bind_rows(disp_valid) |&gt; \n    bind_rows(disp_calib) |&gt; \n    bind_rows(disp_test) |&gt; \n    bind_rows(disp_c_platt_calib) |&gt; \n    bind_rows(disp_c_platt_test) |&gt; \n    bind_rows(disp_c_iso_calib) |&gt; \n    bind_rows(disp_c_iso_test) |&gt;\n    mutate(ind = ind, nb_iter = nb_iter, max_depth = params$max_depth)\n  \n  tb_metrics &lt;- tb_metrics |&gt; left_join(\n    tb_disp_metrics, by = c(\"sample\", \"recalib\", \"ind\", \"nb_iter\", \"max_depth\")\n  )\n\n  tb_prop_scores &lt;- proq_scores_train |&gt;\n    bind_rows(proq_scores_valid) |&gt;\n    bind_rows(proq_scores_calib) |&gt;\n    bind_rows(proq_scores_test) |&gt;\n    bind_rows(proq_scores_c_platt_calib) |&gt;\n    bind_rows(proq_scores_c_platt_test) |&gt;\n    bind_rows(proq_scores_c_iso_calib) |&gt;\n    bind_rows(proq_scores_c_iso_test) |&gt;\n    mutate(ind = ind, nb_iter = nb_iter, max_depth = params$max_depth)\n  \n   list(\n    ind = ind,                         # index for grid\n    nb_iter = nb_iter,                 # number of boosting iterations\n    max_depth = params$max_depth,      # max depth of used trees\n    tb_metrics = tb_metrics,           # # table with performance/calib/divergence\n    tb_prop_scores = tb_prop_scores,   # table with P(q1 &lt; score &lt; q2)\n    scores_hist = scores_hist          # histogram of scores\n  )\n}\n\n\n\n\nFunction predict_score_iter()\n#' Predicts the scores at a given iteration of the XGB model\n#'\n#' @param fit_xgb estimated XGB model\n#' @param tb_train_xgb train set\n#' @param tb_valid_xgb validation set\n#' @param tb_test_xgb test set\n#' @param ind index of the grid search\n#' @param nb_iter boosting iteration to consider\n#'\n#' @returns A list with three elements: `scores_train`, `scores_valid`, and\n#'  `scores_train` which contain the estimated scores on the train and on the \n#'  test score, resp.\npredict_score_iter &lt;- function(fit_xgb,\n                               tb_train_xgb,\n                               tb_valid_xgb,\n                               tb_calib_xgb,\n                               tb_test_xgb,\n                               nb_iter) {\n\n  ## Predicted scores----\n  scores_train &lt;- predict(fit_xgb, tb_train_xgb, iterationrange = c(1, nb_iter))\n  scores_valid &lt;- predict(fit_xgb, tb_valid_xgb, iterationrange = c(1, nb_iter))\n  scores_calib &lt;- predict(fit_xgb, tb_calib_xgb, iterationrange = c(1, nb_iter))\n  scores_test &lt;- predict(fit_xgb, tb_test_xgb, iterationrange = c(1, nb_iter))\n\n  list(\n    scores_train = scores_train,\n    scores_valid = scores_valid,\n    scores_calib = scores_calib,\n    scores_test = scores_test\n  )\n}\n\n\n\n\nFunction simul_xgb_helper()\n#' Fit an XGB and returns metrics based on scores. The divergence metrics are\n#' obtained using the prior distributions.\n#'\n#' @param data_train train set\n#' @param data_valid validation set\n#' @param data_test test set\n#' @param target_name name of the target variable\n#' @param parms tibble with hyperparameters for the current estimation\n#' @param prior prior obtained with `get_beta_fit()`\n#'\n#' @returns A list with 4 elements:\n#'  - `tb_metrics`: performance / calibration metrics\n#'  - `tb_disp_metrics`: disp and div metrics\n#'  - `tb_prop_scores`: table with P(q1 &lt; score &lt; q2)\n#'  - `scores_hist`: histogram of scores\nsimul_xgb_helper &lt;- function(data_train,\n                             data_valid,\n                             data_calib,\n                             data_test,\n                             target_name,\n                             params,\n                             prior) {\n\n  ## Format data for xgboost----\n  tb_train_xgb &lt;- xgb.DMatrix(\n    data = data_train |&gt; dplyr::select(-!!target_name) |&gt; as.matrix(),\n    label = data_train |&gt; dplyr::pull(!!target_name) |&gt; as.matrix()\n  )\n  tb_valid_xgb &lt;- xgb.DMatrix(\n    data = data_valid |&gt; dplyr::select(-!!target_name) |&gt; as.matrix(),\n    label = data_valid |&gt; dplyr::pull(!!target_name) |&gt; as.matrix()\n  )\n  tb_calib_xgb &lt;- xgb.DMatrix(\n    data = data_calib |&gt; dplyr::select(-!!target_name) |&gt; as.matrix(),\n    label = data_calib |&gt; dplyr::pull(!!target_name) |&gt; as.matrix()\n  )\n  tb_test_xgb &lt;- xgb.DMatrix(\n    data = data_test |&gt; dplyr::select(-!!target_name) |&gt; as.matrix(),\n    label = data_test |&gt; dplyr::pull(!!target_name) |&gt; as.matrix()\n  )\n  # Parameters for the algorithm\n  param &lt;- list(\n    max_depth = params$max_depth, #Note: root node is indexed 0\n    eta = params$eta,\n    nthread = 1,\n    objective = \"binary:logistic\",\n    eval_metric = \"auc\"\n  )\n  watchlist &lt;- list(train = tb_train_xgb, eval = tb_valid_xgb)\n  ## Estimation----\n  fit_xgb &lt;- xgb.train(\n    param, tb_train_xgb,\n    nrounds = params$nb_iter_total,\n    watchlist,\n    verbose = 0\n  )\n\n  # First, we estimate the scores at each boosting iteration\n  # As the xgb.Dmatrix objects cannot be easily serialised, we first estimate\n  # these scores in a classical way, without parallelism...\n  scores_iter &lt;- vector(mode = \"list\", length = params$nb_iter_total)\n  for (i_iter in 1:params$nb_iter_total) {\n    scores_iter[[i_iter]] &lt;- predict_score_iter(\n      fit_xgb = fit_xgb,\n      tb_train_xgb = tb_train_xgb,\n      tb_valid_xgb = tb_valid_xgb,\n      tb_calib_xgb = tb_calib_xgb,\n      tb_test_xgb = tb_test_xgb,\n      nb_iter = i_iter)\n  }\n\n  # Then, to compute the metrics, as it is a bit slower, we can use parallelism\n\n  ncl &lt;- detectCores() - 1\n  (cl &lt;- makeCluster(ncl))\n  clusterEvalQ(cl, {\n    library(tidyverse)\n    library(locfit)\n    library(philentropy)\n  }) |&gt;\n    invisible()\n\n  clusterExport(cl, c(\n    \"scores_iter\", \"prior\", \"data_train\", \"data_valid\", \"data_calib\", \"data_test\", \"params\", \n    \"target_name\"\n  ), envir = environment())\n  clusterExport(cl, c(\n    \"get_metrics_xgb_iter\",\n    \"brier_score\",\n    \"compute_metrics\",\n    \"disp_metrics_dataset\", \"dispersion_metrics_beta\",\n    \"recalibrate\", \"prop_btw_quantiles\"\n  ))\n\n  metrics_iter &lt;-\n    pbapply::pblapply(\n      X = seq_len(params$nb_iter_total),\n      FUN = function(i_iter) {\n        get_metrics_xgb_iter(\n          scores = scores_iter[[i_iter]],\n          prior = prior,\n          data_train = data_train,\n          data_valid = data_valid,\n          data_calib = data_calib,\n          data_test = data_test,\n          target_name = target_name,\n          ind = params$ind,\n          nb_iter = i_iter,\n          params = params\n        )\n      },\n      cl = cl\n    )\n  stopCluster(cl)\n  \n  # Merge tibbles from each iteration into a single one\n  tb_metrics &lt;-\n    map(metrics_iter, \"tb_metrics\") |&gt;\n    list_rbind()\n  tb_prop_scores &lt;-\n    map(metrics_iter, \"tb_prop_scores\") |&gt;\n    list_rbind()\n  scores_hist &lt;- map(metrics_iter, \"scores_hist\")\n\n  list(\n    tb_metrics = tb_metrics,\n    tb_prop_scores = tb_prop_scores,\n    scores_hist = scores_hist\n  )\n}\n\n\n\n\nFunction simul_xgb_real()\n#' Train an XGB on a dataset for a binary task for various\n#' hyperparameters and computes metrics based on scores and on a set of prior\n#' distributions of the underlying probability\n#'\n#' @param data dataset\n#' @param target_name name of the target variable\n#' @param prior prior obtained with `get_beta_fit()`\n#' @param seed desired seed (default to `NULL`)\n#'\n#' @returns A list with two elements:\n#'  - `res`: results for each estimated model of the grid. Each element is a\n#'  list with the following elements:\n#'      - `tb_metrics`: performance / calibration metrics\n#'      - `tb_disp_metrics`: disp and div metrics\n#'      - `tb_prop_scores`: table with P(q1 &lt; score &lt; q2)\n#'      - `scores_hist`: histogram of scores.\n#'  - `grid`: the grid search.\nsimul_xgb_real &lt;- function(data,\n                           target_name,\n                           prior,\n                           seed = NULL) {\n\n  if (!is.null(seed)) set.seed(seed)\n\n  # Split data into train and test set\n  data_splitted &lt;- split_train_test(data = data, prop_train = .5, seed = seed)\n  data_encoded &lt;- encode_dataset(\n    data_train = data_splitted$train,\n    data_test = data_splitted$test,\n    target_name = target_name,\n    intercept = FALSE\n  )\n\n  # Further split train into two samples (train/valid)\n  data_splitted_train &lt;- \n    split_train_test(data = data_encoded$train, prop_train = .8, seed = seed)\n  \n  # Further split test into two samples (calib/test)\n  data_splitted_test &lt;- \n    split_train_test(data = data_encoded$test, prop_train = .8, seed = seed)\n  \n  res_grid &lt;- vector(mode = \"list\", length = nrow(grid))\n  for (i_grid in 1:nrow(grid)) {\n    res_grid[[i_grid]] &lt;- simul_xgb_helper(\n      data_train = data_splitted_train$train,\n      data_valid = data_splitted_train$test,\n      data_calib = data_splitted_test$train,\n      data_test = data_splitted_test$test,\n      target_name = target_name,\n      params = grid |&gt; dplyr::slice(i_grid),\n      prior = prior\n    )\n  }\n  \n  # The metrics computed for all set of hyperparameters (identified with `ind`)\n  # and for each number of boosting iterations (`nb_iter`)\n  metrics_simul &lt;- map(res_grid, \"tb_metrics\") |&gt; \n    list_rbind()\n  \n  # P(q_1&lt;s(x)&lt;q_2)\n  prop_scores_simul &lt;- map(res_grid, \"tb_prop_scores\") |&gt; \n    list_rbind()\n  \n  # Histogram of estimated scores\n  scores_hist &lt;- map(res_grid, \"scores_hist\")\n\n  list(\n    metrics_simul = metrics_simul,\n    scores_hist = scores_hist,\n    prop_scores_simul = prop_scores_simul\n  )\n}",
    "crumbs": [
      "IV. Real Data",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Estimations</span>"
    ]
  },
  {
    "objectID": "book_real_xgb.html#grid",
    "href": "book_real_xgb.html#grid",
    "title": "9  Estimations",
    "section": "9.2 Grid",
    "text": "9.2 Grid\nWe consider the following grid:\n\ngrid &lt;- expand_grid(\n  max_depth = c(2, 4, 6),\n  nb_iter_total = 500,\n  eta = 0.3\n) |&gt;\n  mutate(ind = row_number())\n\nThe different configurations are reported in Table 9.1.\n\nDT::datatable(grid)\n\n\n\nTable 9.1: Grid Search Values",
    "crumbs": [
      "IV. Real Data",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Estimations</span>"
    ]
  },
  {
    "objectID": "book_real_xgb.html#data",
    "href": "book_real_xgb.html#data",
    "title": "9  Estimations",
    "section": "9.3 Data",
    "text": "9.3 Data\nIn Chapter 7, we estimated the shapes of Beta distributions using fitted scores from a GAMSEL model applied to various datasets from the UCI Machine Learning Repository. We saved these estimated priors and the datasets in R data files, which can now be easily loaded.\nThe list of datasets and the name of the target variable:\n\ndatasets &lt;- tribble(\n  ~name, ~target_name,\n  \"abalone\", \"Sex\",\n  \"adult\", \"high_income\",\n  \"bank\", \"y\",\n  \"default\", \"default\",\n  \"drybean\", \"is_dermason\",\n  \"coupon\", \"y\",\n  \"mushroom\", \"edible\",\n  \"occupancy\", \"Occupancy\",\n  \"winequality\", \"high_quality\",\n  \"spambase\", \"is_spam\"\n)\n\n\nfor (name in datasets$name) {\n  # The data\n  load(str_c(\"../output/real-data/tb_\", name, \".rda\"))\n  # The Prior on the distribution of the scores\n  load(str_c(\"../output/real-data/priors_\", name, \".rda\"))\n}",
    "crumbs": [
      "IV. Real Data",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Estimations</span>"
    ]
  },
  {
    "objectID": "book_real_xgb.html#estimations-extreme-gradient-boosting",
    "href": "book_real_xgb.html#estimations-extreme-gradient-boosting",
    "title": "9  Estimations",
    "section": "9.4 Estimations: Extreme Gradient Boosting",
    "text": "9.4 Estimations: Extreme Gradient Boosting\nThe models are estimated in parallel. The number of available cores can be determined using the following command:\n\nlibrary(future)\nnb_cores &lt;- future::availableCores() - 1\n\nWe use the following loop to estimate all the models across each dataset:\n\nseed &lt;- 1234\nfor (name in datasets$name) {\n  plan(multisession, workers = nb_cores)\n  current_data &lt;- get(str_c(\"tb_\", name))\n  current_priors &lt;- get(str_c(\"priors_\", name))\n  current_target_name &lt;- datasets |&gt;\n    filter(name == !!name) |&gt; pull(target_name)\n  ## Extreme Gradient Boosting----\n  xgb_resul &lt;- simul_xgb_real(data = current_data,\n                              target_name = current_target_name,\n                              prior = current_priors,\n                              seed = seed)\n  save(xgb_resul, file = str_c(\"../output/real-data/xgb_resul_\", name, \".rda\"))\n}",
    "crumbs": [
      "IV. Real Data",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Estimations</span>"
    ]
  },
  {
    "objectID": "book_real_results.html",
    "href": "book_real_results.html",
    "title": "10  Real-data Results",
    "section": "",
    "text": "10.1 Data\nThe list of datasets and the name of the target variable:\ndatasets &lt;- tribble(\n  ~name, ~target_name,\n  \"abalone\", \"Sex\",\n  \"adult\", \"high_income\",\n  \"bank\", \"y\",\n  \"default\", \"default\",\n  \"drybean\", \"is_dermason\",\n  \"coupon\", \"y\",\n  \"mushroom\", \"edible\",\n  \"occupancy\", \"Occupancy\",\n  \"winequality\", \"high_quality\",\n  \"spambase\", \"is_spam\"\n)\nThe priors:\nfor (name in datasets$name) {\n  # The Prior on the distribution of the scores\n  load(str_c(\"../output/real-data/priors_\", name, \".rda\"))\n}\nThe results:\nfiles &lt;- str_c(\"../output/real-data/xgb_resul_\", datasets$name, \".rda\")\n# Loop through each file and load it with a new name\nwalk2(files, datasets$name, ~{\n  loaded_object_name &lt;- load(.x)\n  new_name &lt;- str_c(\"xgb_resul_\", .y)\n  assign(new_name, get(loaded_object_name), envir = .GlobalEnv)\n})",
    "crumbs": [
      "IV. Real Data",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Real-data Results</span>"
    ]
  },
  {
    "objectID": "book_real_results.html#distribution-of-scores",
    "href": "book_real_results.html#distribution-of-scores",
    "title": "10  Real-data Results",
    "section": "10.2 Distribution of Scores",
    "text": "10.2 Distribution of Scores\n\n\nGraph functions\nplot_bp_interest &lt;- function(metrics_interest,\n                             scores_hist_interest,\n                             label,\n                             recalib_method) {\n  subtitle &lt;- str_c(\n    \"Depth = \", metrics_interest$max_depth, \", \",\n    \"AUC = \", round(metrics_interest$AUC, 2), \", \\n\",\n    \"Brier = \", round(metrics_interest$brier, 2), \",\",\n    \"ICI = \", round(metrics_interest$ici, 2), \", \",\n    \"KL = \", round(metrics_interest$KL_20_true_probas, 2)\n  )\n  \n  if (recalib_method == \"none\") {\n    plot(\n      main = str_c(label, \" (iter = \", metrics_interest$nb_iter,\")\"),\n      scores_hist_interest$test,\n      xlab = latex2exp::TeX(\"$\\\\hat{s}(x)$\"),\n      ylab = \"\"\n    )\n  } else if (recalib_method == \"platt\") {\n    plot(\n      main = str_c(label, \" (iter = \", metrics_interest$nb_iter,\")\"),\n      scores_hist_interest$test_c_platt,\n      xlab = latex2exp::TeX(\"$\\\\hat{s}(x)$\"),\n      ylab = \"\",\n      col = colour_recalib[[\"Platt\"]]\n    )\n  } else if (recalib_method == \"iso\") {\n    plot(\n      main = str_c(label, \" (iter = \", metrics_interest$nb_iter,\")\"),\n      scores_hist_interest$test_c_iso,\n      xlab = latex2exp::TeX(\"$\\\\hat{s}(x)$\"),\n      ylab = \"\",\n      col = colour_recalib[[\"Isotonic\"]]\n    )\n  }\n  mtext(side = 3, line = -0.25, adj = .5, subtitle, cex = .5)\n}\n\nplot_bp_xgb &lt;- function(paper_version = FALSE) {\n  max_depth_val &lt;- map_dbl(scores_hist_all, ~.x[[1]]$max_depth)\n  true_prob &lt;- prior$scores_gamsel$scores_train\n  \n  for (recalib_method in c(\"none\", \"platt\", \"iso\")) {\n    \n    i_method &lt;- match(recalib_method, c(\"none\", \"platt\", \"iso\"))\n    recalib_method_lab &lt;- c(\"None\", \"Platt\", \"Isotonic\")[i_method]\n    \n    # The metrics for the corresponding results, on the validation set\n    metrics_xgb_current_valid &lt;-\n      metrics_xgb_all |&gt;\n      filter(\n        sample == \"Validation\",\n        recalib == \"None\"\n      )\n    # and on the test set\n    metrics_xgb_current_test &lt;-\n      metrics_xgb_all |&gt;\n      filter(\n        sample == \"Test\",\n        recalib == recalib_method_lab\n      )\n    \n    if (paper_version == FALSE) {\n      hist(\n        true_prob,\n        breaks = seq(0, 1, by = .05),\n        xlab = \"p\", ylab = \"\",\n        main = \"Prior Probabilities\",\n        xlim = c(0, 1)\n      )\n      mtext(\n        side = 2, recalib_method_lab, line = 2.5, cex = 1,\n        font.lab = 2\n      )\n      # Iterations of interest----\n      ## Start of iterations\n      scores_hist_start &lt;- scores_hist_all[[1]][[1]]\n      metrics_start &lt;- metrics_xgb_current_test |&gt;\n        filter(\n          nb_iter == scores_hist_start$nb_iter,\n          max_depth == scores_hist_start$max_depth\n        )\n      \n      plot_bp_interest(\n        metrics_interest = metrics_start,\n        scores_hist_interest = scores_hist_start,\n        label = \"Start\",\n        recalib_method = recalib_method\n      )\n      \n      ## End of iterations\n      scores_hist_end &lt;- scores_hist_all[[1]][[length(scores_hist_all[[1]])]]\n      metrics_end &lt;- metrics_xgb_current_test |&gt;\n        filter(\n          nb_iter == scores_hist_end$nb_iter,\n          max_depth == scores_hist_start$max_depth\n        )\n      plot_bp_interest(\n        metrics_interest = metrics_end,\n        scores_hist_interest = scores_hist_end,\n        label = \"End\",\n        recalib_method = recalib_method\n      )\n    }\n    ## Iteration with max AUC on validation set\n    metrics_valid_auc_star &lt;-\n      metrics_xgb_current_valid |&gt; arrange(desc(AUC)) |&gt; dplyr::slice(1)\n    nb_iter_auc &lt;- metrics_valid_auc_star$nb_iter\n    max_depth_auc_star &lt;- metrics_valid_auc_star$max_depth\n    i_max_depth_auc_star &lt;- which(max_depth_val == max_depth_auc_star)\n    \n    metrics_max_auc &lt;-\n      metrics_xgb_current_test |&gt;\n      filter(nb_iter == !!nb_iter_auc, max_depth == max_depth_auc_star)\n    # Note: indexing at 0 here...\n    ind_auc &lt;- which(map_dbl(scores_hist_all[[i_max_depth_auc_star]], \"nb_iter\") == nb_iter_auc)\n    scores_hist_max_auc &lt;- scores_hist_all[[i_max_depth_auc_star]][[ind_auc]]\n    plot_bp_interest(\n      metrics_interest = metrics_max_auc,\n      scores_hist_interest = scores_hist_max_auc,\n      label = \"AUC*\",\n      recalib_method = recalib_method\n    )\n    if (paper_version == TRUE) {\n      mtext(\n        side = 2, recalib_method_lab, line = 2.5, cex = 1,\n        font.lab = 2\n      )\n    }\n    \n    ## Min Brier on validation set\n    metrics_valid_brier_star &lt;-\n      metrics_xgb_current_valid |&gt; arrange(brier) |&gt; dplyr::slice(1)\n    nb_iter_brier &lt;- metrics_valid_brier_star$nb_iter\n    max_depth_brier_star &lt;- metrics_valid_brier_star$max_depth\n    i_max_depth_brier_star &lt;- which(max_depth_val == max_depth_brier_star)\n    \n    metrics_min_brier &lt;-\n      metrics_xgb_current_test |&gt;\n      filter(nb_iter == !!nb_iter_brier, max_depth == max_depth_brier_star)\n    ind_brier &lt;- which(map_dbl(scores_hist_all[[i_max_depth_brier_star]], \"nb_iter\") == nb_iter_brier)\n    scores_hist_min_brier &lt;- scores_hist_all[[i_max_depth_brier_star]][[ind_brier]]\n    plot_bp_interest(\n      metrics_interest = metrics_min_brier,\n      scores_hist_interest = scores_hist_min_brier,\n      label = \"Brier*\",\n      recalib_method = recalib_method\n    )\n    \n    ## Min ICI on validation set\n    metrics_valid_ici_star &lt;-\n      metrics_xgb_current_valid |&gt; arrange(ici) |&gt; dplyr::slice(1)\n    nb_iter_ici &lt;-   metrics_valid_ici_star$nb_iter\n    max_depth_ici_star &lt;- metrics_valid_ici_star$max_depth\n    i_max_depth_ici_star &lt;- which(max_depth_val == max_depth_ici_star)\n    \n    metrics_min_ici &lt;-\n      metrics_xgb_current_test |&gt;\n      filter(nb_iter == !!nb_iter_ici, max_depth == max_depth_ici_star)\n    ind_ici &lt;- which(map_dbl(scores_hist_all[[i_max_depth_ici_star]], \"nb_iter\") == nb_iter_ici)\n    scores_hist_min_ici &lt;- scores_hist_all[[i_max_depth_ici_star]][[ind_ici]]\n    plot_bp_interest(\n      metrics_interest = metrics_min_ici,\n      scores_hist_interest = scores_hist_min_ici,\n      label = \"ICI*\",\n      recalib_method = recalib_method\n    )\n    \n    ## Min KL on validation set\n    metrics_valid_kl_star &lt;-\n      metrics_xgb_current_valid |&gt; arrange(KL_20_true_probas) |&gt; dplyr::slice(1)\n    nb_iter_kl &lt;- metrics_valid_kl_star$nb_iter\n    max_depth_kl_star &lt;- metrics_valid_kl_star$max_depth\n    i_max_depth_kl_star &lt;- which(max_depth_val == max_depth_kl_star)\n    \n    metrics_min_kl &lt;-\n      metrics_xgb_current_test |&gt;\n      filter(nb_iter == !!nb_iter_kl, max_depth == max_depth_kl_star)\n    ind_kl &lt;- which(map_dbl(scores_hist_all[[i_max_depth_kl_star]], \"nb_iter\") == nb_iter_kl)\n    scores_hist_min_kl &lt;- scores_hist_all[[i_max_depth_kl_star]][[ind_kl]]\n    plot_bp_interest(\n      metrics_interest = metrics_min_kl,\n      scores_hist_interest = scores_hist_min_kl,\n      label = \"KL*\",\n      recalib_method = recalib_method\n    )\n    \n    ## Mediocre ICI on validation set\n    identified_mici &lt;-  mediocre_ici_xgb\n    \n    metrics_valid_mici_star &lt;- metrics_xgb_current_valid |&gt; \n      filter(ind == identified_mici$ind, nb_iter == identified_mici$nb_iter)\n    nb_iter_mici &lt;- metrics_valid_mici_star$nb_iter\n    max_depth_mici_star &lt;- metrics_valid_mici_star$max_depth\n    i_max_depth_mici_star &lt;- which(max_depth_val == max_depth_mici_star)\n    \n    metrics_mici &lt;-\n      metrics_xgb_current_test |&gt;\n      filter(nb_iter == !!nb_iter_mici, max_depth == max_depth_mici_star)\n    ind_mici &lt;- which(map_dbl(scores_hist_all[[i_max_depth_mici_star]], \"nb_iter\") == nb_iter_mici)\n    scores_hist_mici &lt;- scores_hist_all[[i_max_depth_mici_star]][[ind_mici]]\n    plot_bp_interest(\n      metrics_interest = metrics_mici,\n      scores_hist_interest = scores_hist_mici,\n      label = \"Med. ICI\",\n      recalib_method = recalib_method\n    )\n  }\n}\n\n\n\nplot_bp_xgb &lt;- function(scores_hist_all, \n                        prior, \n                        metrics_xgb_all,\n                        mediocre_ici_xgb,\n                        paper_version = FALSE) {\n  # # Focus on a value for max_depth\n  max_depth_val &lt;- map_dbl(scores_hist_all, ~.x[[1]]$max_depth)\n  \n  # True Probabilities\n  true_prob &lt;- prior$scores_gamsel$scores_train\n  \n  for (recalib_method in c(\"none\", \"platt\", \"iso\")) {\n    \n    i_method &lt;- match(recalib_method, c(\"none\", \"platt\", \"iso\"))\n    recalib_method_lab &lt;- c(\"None\", \"Platt\", \"Isotonic\")[i_method]\n    \n    # The metrics for the corresponding results, on the validation set\n    metrics_xgb_current_valid &lt;-\n      metrics_xgb_all |&gt;\n      filter(\n        sample == \"Validation\",\n        recalib == \"None\"\n      )\n    # and on the test set\n    metrics_xgb_current_test &lt;-\n      metrics_xgb_all |&gt;\n      filter(\n        sample == \"Test\",\n        recalib == recalib_method_lab\n      )\n    \n    if (paper_version == FALSE) {\n      hist(\n        true_prob,\n        breaks = seq(0, 1, by = .05),\n        xlab = \"p\", ylab = \"\",\n        main = \"Prior Probabilities\",\n        xlim = c(0, 1)\n      )\n      mtext(\n        side = 2, recalib_method_lab, line = 2.5, cex = 1,\n        font.lab = 2\n      )\n      # Iterations of interest----\n      ## Start of iterations\n      scores_hist_start &lt;- scores_hist_all[[1]][[1]]\n      metrics_start &lt;- metrics_xgb_current_test |&gt;\n        filter(\n          nb_iter == scores_hist_start$nb_iter,\n          max_depth == scores_hist_start$max_depth\n        )\n      \n      plot_bp_interest(\n        metrics_interest = metrics_start,\n        scores_hist_interest = scores_hist_start,\n        label = \"Start\",\n        recalib_method = recalib_method\n      )\n      \n      ## End of iterations\n      scores_hist_end &lt;- scores_hist_all[[1]][[length(scores_hist_all[[1]])]]\n      metrics_end &lt;- metrics_xgb_current_test |&gt;\n        filter(\n          nb_iter == scores_hist_end$nb_iter,\n          max_depth == scores_hist_start$max_depth\n        )\n      plot_bp_interest(\n        metrics_interest = metrics_end,\n        scores_hist_interest = scores_hist_end,\n        label = \"End\",\n        recalib_method = recalib_method\n      )\n    }\n    ## Iteration with max AUC on validation set\n    metrics_valid_auc_star &lt;-\n      metrics_xgb_current_valid |&gt; arrange(desc(AUC)) |&gt; dplyr::slice(1)\n    nb_iter_auc &lt;- metrics_valid_auc_star$nb_iter\n    max_depth_auc_star &lt;- metrics_valid_auc_star$max_depth\n    i_max_depth_auc_star &lt;- which(max_depth_val == max_depth_auc_star)\n    \n    metrics_max_auc &lt;-\n      metrics_xgb_current_test |&gt;\n      filter(nb_iter == !!nb_iter_auc, max_depth == max_depth_auc_star)\n    # Note: indexing at 0 here...\n    ind_auc &lt;- which(map_dbl(scores_hist_all[[i_max_depth_auc_star]], \"nb_iter\") == nb_iter_auc)\n    scores_hist_max_auc &lt;- scores_hist_all[[i_max_depth_auc_star]][[ind_auc]]\n    plot_bp_interest(\n      metrics_interest = metrics_max_auc,\n      scores_hist_interest = scores_hist_max_auc,\n      label = \"AUC*\",\n      recalib_method = recalib_method\n    )\n    if (paper_version == TRUE) {\n      mtext(\n        side = 2, recalib_method_lab, line = 2.5, cex = 1,\n        font.lab = 2\n      )\n    }\n    \n    ## Min Brier on validation set\n    metrics_valid_brier_star &lt;-\n      metrics_xgb_current_valid |&gt; arrange(brier) |&gt; dplyr::slice(1)\n    nb_iter_brier &lt;- metrics_valid_brier_star$nb_iter\n    max_depth_brier_star &lt;- metrics_valid_brier_star$max_depth\n    i_max_depth_brier_star &lt;- which(max_depth_val == max_depth_brier_star)\n    \n    metrics_min_brier &lt;-\n      metrics_xgb_current_test |&gt;\n      filter(nb_iter == !!nb_iter_brier, max_depth == max_depth_brier_star)\n    ind_brier &lt;- which(map_dbl(scores_hist_all[[i_max_depth_brier_star]], \"nb_iter\") == nb_iter_brier)\n    scores_hist_min_brier &lt;- scores_hist_all[[i_max_depth_brier_star]][[ind_brier]]\n    plot_bp_interest(\n      metrics_interest = metrics_min_brier,\n      scores_hist_interest = scores_hist_min_brier,\n      label = \"Brier*\",\n      recalib_method = recalib_method\n    )\n    \n    ## Min ICI on validation set\n    metrics_valid_ici_star &lt;-\n      metrics_xgb_current_valid |&gt; arrange(ici) |&gt; dplyr::slice(1)\n    nb_iter_ici &lt;-   metrics_valid_ici_star$nb_iter\n    max_depth_ici_star &lt;- metrics_valid_ici_star$max_depth\n    i_max_depth_ici_star &lt;- which(max_depth_val == max_depth_ici_star)\n    \n    metrics_min_ici &lt;-\n      metrics_xgb_current_test |&gt;\n      filter(nb_iter == !!nb_iter_ici, max_depth == max_depth_ici_star)\n    ind_ici &lt;- which(map_dbl(scores_hist_all[[i_max_depth_ici_star]], \"nb_iter\") == nb_iter_ici)\n    scores_hist_min_ici &lt;- scores_hist_all[[i_max_depth_ici_star]][[ind_ici]]\n    plot_bp_interest(\n      metrics_interest = metrics_min_ici,\n      scores_hist_interest = scores_hist_min_ici,\n      label = \"ICI*\",\n      recalib_method = recalib_method\n    )\n    \n    ## Min KL on validation set\n    metrics_valid_kl_star &lt;-\n      metrics_xgb_current_valid |&gt; arrange(KL_20_true_probas) |&gt; dplyr::slice(1)\n    nb_iter_kl &lt;- metrics_valid_kl_star$nb_iter\n    max_depth_kl_star &lt;- metrics_valid_kl_star$max_depth\n    i_max_depth_kl_star &lt;- which(max_depth_val == max_depth_kl_star)\n    \n    metrics_min_kl &lt;-\n      metrics_xgb_current_test |&gt;\n      filter(nb_iter == !!nb_iter_kl, max_depth == max_depth_kl_star)\n    ind_kl &lt;- which(map_dbl(scores_hist_all[[i_max_depth_kl_star]], \"nb_iter\") == nb_iter_kl)\n    scores_hist_min_kl &lt;- scores_hist_all[[i_max_depth_kl_star]][[ind_kl]]\n    plot_bp_interest(\n      metrics_interest = metrics_min_kl,\n      scores_hist_interest = scores_hist_min_kl,\n      label = \"KL*\",\n      recalib_method = recalib_method\n    )\n    \n    ## Mediocre ICI on validation set\n    identified_mici &lt;-  mediocre_ici_xgb\n    \n    metrics_valid_mici_star &lt;- metrics_xgb_current_valid |&gt; \n      filter(ind == identified_mici$ind, nb_iter == identified_mici$nb_iter)\n    nb_iter_mici &lt;- metrics_valid_mici_star$nb_iter\n    max_depth_mici_star &lt;- metrics_valid_mici_star$max_depth\n    i_max_depth_mici_star &lt;- which(max_depth_val == max_depth_mici_star)\n    \n    metrics_mici &lt;-\n      metrics_xgb_current_test |&gt;\n      filter(nb_iter == !!nb_iter_mici, max_depth == max_depth_mici_star)\n    ind_mici &lt;- which(map_dbl(scores_hist_all[[i_max_depth_mici_star]], \"nb_iter\") == nb_iter_mici)\n    scores_hist_mici &lt;- scores_hist_all[[i_max_depth_mici_star]][[ind_mici]]\n    plot_bp_interest(\n      metrics_interest = metrics_mici,\n      scores_hist_interest = scores_hist_mici,\n      label = \"Med. ICI\",\n      recalib_method = recalib_method\n    )\n  }\n}\n\n\nplot_bp_xgb_dataset &lt;- function(xgb_resul, prior) {\n  scores_hist_all &lt;- xgb_resul$scores_hist\n  \n  metrics_xgb_all &lt;- xgb_resul$metrics_simul |&gt;\n    mutate(\n      sample = factor(\n        sample,\n        levels = c(\"train\", \"valid\", \"calib\", \"test\"),\n        labels = c(\"Train\",\"Validation\", \"Calibration\" ,\"Test\")\n      ),\n      recalib = factor(\n        recalib,\n        levels = c(\"none\", \"platt\", \"isotonic\"),\n        labels = c(\"None\", \"Platt\", \"Isotonic\")\n      )\n    )\n  \n  mediocre_ici_xgb &lt;- \n    metrics_xgb_all |&gt;\n    filter(sample == \"Validation\", recalib == \"None\") |&gt;\n    arrange(desc(ici)) |&gt;\n    #slice_head(n = 1) |&gt;\n    filter(nb_iter == 10 & ind == 1) |&gt;\n    select(ind, nb_iter, recalib) |&gt;\n    mutate(result_type = \"mediocre_ici\")\n  \n  plot_bp_xgb(\n    scores_hist_all = scores_hist_all, \n    prior = prior, \n    metrics_xgb_all = metrics_xgb_all,\n    mediocre_ici_xgb = mediocre_ici_xgb)\n}\n\n\n\nAbaloneAdultBankDefaultDrybeanCouponMushroomOccupancyWinequalitySpambase\n\n\n\n\nCode\npar(mfrow = c(3,8), mar = c(4.1, 4, 3.5, 1.5))\nplot_bp_xgb_dataset(xgb_resul = xgb_resul_abalone, prior = priors_abalone)\n\n\n\n\n\nFigure 10.1: Distribution of scores on the test set (abalone)\n\n\n\n\n\n\n\n\n\n\n\n\nCode\npar(mfrow = c(3,8), mar = c(4.1, 4, 3.5, 1.5))\nplot_bp_xgb_dataset(xgb_resul = xgb_resul_adult, prior = priors_adult)\n\n\n\n\n\nFigure 10.2: Distribution of scores on the test set (adult)\n\n\n\n\n\n\n\n\n\n\n\n\nCode\npar(mfrow = c(3,8), mar = c(4.1, 4, 3.5, 1.5))\nplot_bp_xgb_dataset(xgb_resul = xgb_resul_bank, prior = priors_bank)\n\n\n\n\n\nFigure 10.3: Distribution of scores on the test set (bank)\n\n\n\n\n\n\n\n\n\n\n\n\nCode\npar(mfrow = c(3,8), mar = c(4.1, 4, 3.5, 1.5))\nplot_bp_xgb_dataset(xgb_resul = xgb_resul_default, prior = priors_default)\n\n\n\n\n\nFigure 10.4: Distribution of scores on the test set (default)\n\n\n\n\n\n\n\n\n\n\n\n\nCode\npar(mfrow = c(3,8), mar = c(4.1, 4, 3.5, 1.5))\nplot_bp_xgb_dataset(xgb_resul = xgb_resul_drybean, prior = priors_drybean)\n\n\n\n\n\nFigure 10.5: Distribution of scores on the test set (drybean)\n\n\n\n\n\n\n\n\n\n\n\n\nCode\npar(mfrow = c(3,8), mar = c(4.1, 4, 3.5, 1.5))\nplot_bp_xgb_dataset(xgb_resul = xgb_resul_coupon, prior = priors_coupon)\n\n\n\n\n\nFigure 10.6: Distribution of scores on the test set (coupon)\n\n\n\n\n\n\n\n\n\n\n\n\nCode\npar(mfrow = c(3,8), mar = c(4.1, 4, 3.5, 1.5))\nplot_bp_xgb_dataset(xgb_resul = xgb_resul_mushroom, prior = priors_mushroom)\n\n\n\n\n\nFigure 10.7: Distribution of scores on the test set (mushroom)\n\n\n\n\n\n\n\n\n\n\n\n\nCode\npar(mfrow = c(3,8), mar = c(4.1, 4, 3.5, 1.5))\nplot_bp_xgb_dataset(xgb_resul = xgb_resul_occupancy, prior = priors_occupancy)\n\n\n\n\n\nFigure 10.8: Distribution of scores on the test set (occupancy)\n\n\n\n\n\n\n\n\n\n\n\n\nCode\npar(mfrow = c(3,8), mar = c(4.1, 4, 3.5, 1.5))\nplot_bp_xgb_dataset(xgb_resul = xgb_resul_winequality, prior = priors_winequality)\n\n\n\n\n\nFigure 10.9: Distribution of scores on the test set (winequality)\n\n\n\n\n\n\n\n\n\n\n\n\nCode\npar(mfrow = c(3,8), mar = c(4.1, 4, 3.5, 1.5))\nplot_bp_xgb_dataset(xgb_resul = xgb_resul_spambase, prior = priors_spambase)\n\n\n\n\n\nFigure 10.10: Distribution of scores on the test set (spambase)",
    "crumbs": [
      "IV. Real Data",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Real-data Results</span>"
    ]
  },
  {
    "objectID": "book_real_results.html#before-vs.-after-recalibration",
    "href": "book_real_results.html#before-vs.-after-recalibration",
    "title": "10  Real-data Results",
    "section": "10.3 Before vs. After Recalibration",
    "text": "10.3 Before vs. After Recalibration\nLet us have a look at the evolution of ICI and KL divergence before and after calibration with both methods (Platt-scaling and Isotonic regression). We consider three models for each dataset: the one for which the hyperparameters were selected based on the maximization of AUC (AUC*), the one for which the hyperparameters were selected based on the minimization of the Kullback-Leibler divergence between the distribution of the estimated scores and that of the prior distribution, and one with a relatively high ICI, i.e., with poor calibration.\n\n\nFunctions to extract the metrics from the results\n#' Function to extract the results for models of interest\ngather_metrics &lt;- function(xgb_resul) {\n  \n  metrics_xgb_all &lt;- xgb_resul$metrics_simul |&gt;\n    mutate(\n      sample = factor(\n        sample,\n        levels = c(\"train\", \"valid\", \"calib\", \"test\"),\n        labels = c(\"Train\",\"Validation\", \"Calibration\" ,\"Test\")\n      ),\n      recalib = factor(\n        recalib,\n        levels = c(\"none\", \"platt\", \"isotonic\"),\n        labels = c(\"None\", \"Platt\", \"Isotonic\")\n      )\n    )\n  \n  # Identify the smallest tree on the validation set, when the scores are not\n  # recalibrated\n  smallest_xgb &lt;-\n    metrics_xgb_all |&gt;\n    filter(sample == \"Validation\", recalib == \"None\") |&gt;\n    arrange(nb_iter) |&gt;\n    slice_head(n = 1) |&gt;\n    select(ind, nb_iter, recalib) |&gt;\n    mutate(result_type = \"smallest\")\n  \n  # Identify the largest tree\n  largest_xgb &lt;-\n    metrics_xgb_all |&gt;\n    filter(sample == \"Validation\", recalib == \"None\") |&gt;\n    arrange(desc(nb_iter)) |&gt;\n    slice_head(n = 1) |&gt;\n    select(ind, nb_iter, recalib) |&gt;\n    mutate(result_type = \"largest\")\n  \n  # Identify tree with highest AUC on test set\n  highest_auc_xgb &lt;-\n    metrics_xgb_all |&gt;\n    filter(sample == \"Validation\", recalib == \"None\") |&gt;\n    arrange(desc(AUC)) |&gt;\n    slice_head(n = 1) |&gt;\n    select(ind, nb_iter, recalib) |&gt;\n    mutate(result_type = \"largest_auc\")\n  \n  # Identify tree with lowest brier\n  lowest_brier_xgb &lt;-\n    metrics_xgb_all |&gt;\n    filter(sample == \"Validation\", recalib == \"None\") |&gt;\n    arrange(brier) |&gt;\n    slice_head(n = 1) |&gt;\n    select(ind, nb_iter, recalib) |&gt;\n    mutate(result_type = \"lowest_brier\")\n  \n  # Identify tree with lowest ICI\n  lowest_ici_xgb &lt;-\n    metrics_xgb_all |&gt;\n    filter(sample == \"Validation\", recalib == \"None\") |&gt;\n    arrange(ici) |&gt;\n    slice_head(n = 1) |&gt;\n    select(ind, nb_iter, recalib) |&gt;\n    mutate(result_type = \"lowest_ici\")\n  \n  # Identify tree with lowest KL\n  lowest_kl_xgb &lt;-\n    metrics_xgb_all |&gt;\n    filter(sample == \"Validation\", recalib == \"None\") |&gt;\n    arrange(KL_20_true_probas) |&gt;\n    slice_head(n = 1) |&gt;\n    select(ind, nb_iter, recalib) |&gt;\n    mutate(result_type = \"lowest_kl\")\n  \n  mediocre_ici_xgb &lt;- \n    metrics_xgb_all |&gt;\n    filter(sample == \"Validation\", recalib == \"None\") |&gt;\n    arrange(desc(ici)) |&gt;\n    #slice_head(n = 1) |&gt;\n    filter(nb_iter == 10 & ind == 1) |&gt;\n    select(ind, nb_iter, recalib) |&gt;\n    mutate(result_type = \"mediocre_ici\")\n  \n  # Merge these\n  models_of_interest_xgb &lt;-\n    smallest_xgb |&gt;\n    bind_rows(largest_xgb) |&gt;\n    bind_rows(highest_auc_xgb) |&gt;\n    bind_rows(lowest_brier_xgb) |&gt;\n    bind_rows(lowest_ici_xgb) |&gt;\n    bind_rows(lowest_kl_xgb) |&gt; \n    bind_rows(mediocre_ici_xgb)\n  \n  models_of_interest_metrics &lt;- NULL\n  for (recalibration_method in c(\"None\", \"Platt\", \"Isotonic\")) {\n    # Add metrics now\n    models_of_interest_metrics &lt;-\n      models_of_interest_metrics |&gt;\n      bind_rows(\n        models_of_interest_xgb |&gt; select(-recalib) |&gt;\n          left_join(\n            metrics_xgb_all |&gt;\n              filter(\n                recalib == recalibration_method,\n                sample %in% c(\"Validation\", \"Test\")\n              ),\n            by = c(\"ind\", \"nb_iter\"),\n            relationship = \"many-to-many\" # (calib, test)\n          )\n      )\n  }\n  \n  models_of_interest_metrics &lt;-\n    models_of_interest_metrics |&gt;\n    mutate(\n      result_type = factor(\n        result_type,\n        levels = c(\n          \"smallest\", \"largest\", \"lowest_mse\", \"largest_auc\",\n          \"lowest_brier\", \"lowest_ici\", \"lowest_kl\", \"mediocre_ici\"),\n        labels = c(\n          \"Smallest\", \"Largest\", \"MSE*\", \"AUC*\",\n          \"Brier*\", \"ICI*\", \"KL*\", \"High ICI\"\n        )\n      )\n    )\n  models_of_interest_metrics\n}\n\n#' Function to format results for selected models for later use in ggplot2\nget_data_plot_before_after_recalib &lt;- function(xgb_resul, dataset_name) {\n  models_of_interest_metrics &lt;- gather_metrics(xgb_resul = xgb_resul)\n  \n  table_models_interest &lt;- \n    models_of_interest_metrics |&gt; \n    filter(sample == \"Test\") |&gt; \n    select(\n      recalib, sample, result_type, \n      AUC, brier, ici, kl = KL_20_true_probas\n    ) |&gt; \n    filter(\n      result_type %in% c(\"AUC*\", \"KL*\", \"High ICI\")\n    ) |&gt; \n    mutate(dataset = dataset_name)\n  \n  \n  initial_points &lt;- table_models_interest |&gt; \n    filter(recalib == \"None\") |&gt; \n    mutate(dataset = dataset_name)\n  \n  points_after_c &lt;- initial_points |&gt; \n    select(result_type, ici, kl) |&gt; \n    left_join(\n      table_models_interest |&gt; \n        filter(recalib %in% c(\"Platt\", \"Isotonic\")) |&gt; \n        select(recalib, result_type, ici, kl) |&gt; \n        rename(ici_end = ici, kl_end = kl),\n      by = \"result_type\",\n      relationship = \"many-to-many\" # (Platt and Isotonic)\n    ) |&gt; \n    mutate(dataset = dataset_name)\n  \n  list(\n    table_models_interest = table_models_interest,\n    initial_points = initial_points,\n    points_after_c = points_after_c\n  )\n}\n\n\n# The models of interest for all datasets\ndata_plot_before_after &lt;- map2(\n  .x = str_c(\"xgb_resul_\", datasets$name), \n  .y = datasets$name, \n  .f = ~get_data_plot_before_after_recalib(xgb_resul = get(.x), dataset_name = .y)\n)\n\n# The metrics for these models\ntable_models_interest &lt;- \n  map(data_plot_before_after, \"table_models_interest\") |&gt; \n  list_rbind()\n\n# The metrics for the three models of interest in the subsequent plot\n# before recalibration\ninitial_points &lt;- \n  map(data_plot_before_after, \"initial_points\") |&gt; \n  list_rbind()\n\n# and after recalibration\npoints_after_c &lt;- \n  map(data_plot_before_after, \"points_after_c\") |&gt; \n  list_rbind()\n\n\n\n10.3.1 Tables\n\n\nDisplay codes to create the Table\ndigits &lt;- 2\n\ntable_models_interest |&gt; \n  relocate(dataset, .before = recalib) |&gt; \n  select(-sample, -AUC) |&gt; \n  pivot_longer(cols = c(brier, ici, kl)) |&gt; \n  pivot_wider(names_from = c(recalib, name), values_from = value) |&gt; \n  knitr::kable(\n    align = str_c(\"l\", str_c(rep(\"c\", 3*3), collapse = \"\"), collapse = \"\"),\n    escape = FALSE, booktabs = T, digits = 3, format = \"markdown\",\n    col.names = c(\n      \"Dataset\", \"Optim.\",\n      rep(c(\"BS\", \"ICI\", \"KL\"), 3)\n    )\n  ) |&gt; \n  kableExtra::collapse_rows(columns = 1, valign = \"top\") |&gt; \n  kableExtra::add_header_above(\n    c(\" \" = 2,\n      \"None\" = 3,\n      \"Platt\" = 3,\n      \"Isotonic\" = 3\n    )\n  ) |&gt; \n  kableExtra::scroll_box(fixed_thead = TRUE, height = \"500px\")\n\n\n\n\nTable 10.1: Performance and calibration metrics (Brier Score, Integrated Calibration Index, Kullback-Leibler Divergence) computed on the test set, on scores returned by the model (column ‘None’), on scores recalibrated using Platt scaling (column ‘Platt’), or Isotonic regression (coliumn ‘Isotonic’)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNone\n\n\nPlatt\n\n\nIsotonic\n\n\n\nDataset\nOptim.\nBS\nICI\nKL\nBS\nICI\nKL\nBS\nICI\nKL\n\n\n\n\nabalone\nAUC*\n0.267\n0.206\n6.029\n0.209\n0.047\n0.702\n0.208\n0.039\n1.072\n\n\nKL*\n0.212\n0.093\n0.319\n0.205\n0.042\n0.163\n0.205\n0.036\n0.887\n\n\nHigh ICI\n0.208\n0.071\n0.883\n0.205\n0.048\n1.003\n0.203\n0.039\n1.292\n\n\nadult\nAUC*\n0.086\n0.016\n0.322\n0.088\n0.041\n0.442\n0.086\n0.016\n0.424\n\n\nKL*\n0.097\n0.045\n0.105\n0.097\n0.035\n0.333\n0.096\n0.022\n0.370\n\n\nHigh ICI\n0.110\n0.068\n0.351\n0.108\n0.040\n0.367\n0.104\n0.019\n0.640\n\n\nbank\nAUC*\n0.065\n0.017\n0.490\n0.069\n0.051\n0.675\n0.065\n0.009\n0.526\n\n\nKL*\n0.077\n0.036\n0.091\n0.079\n0.043\n0.446\n0.075\n0.008\n0.417\n\n\nHigh ICI\n0.080\n0.050\n0.981\n0.079\n0.036\n0.458\n0.076\n0.009\n0.420\n\n\ndefault\nAUC*\n0.133\n0.028\n0.308\n0.133\n0.033\n0.756\n0.132\n0.011\n0.518\n\n\nKL*\n0.133\n0.024\n0.283\n0.133\n0.026\n0.695\n0.132\n0.008\n0.491\n\n\nHigh ICI\n0.137\n0.051\n0.736\n0.137\n0.040\n1.390\n0.134\n0.011\n0.847\n\n\ndrybean\nAUC*\n0.033\n0.010\n0.662\n0.036\n0.027\n0.835\n0.033\n0.010\n0.678\n\n\nKL*\n0.037\n0.028\n0.392\n0.040\n0.032\n0.826\n0.036\n0.009\n0.692\n\n\nHigh ICI\n0.040\n0.040\n0.593\n0.041\n0.031\n0.858\n0.037\n0.007\n0.808\n\n\ncoupon\nAUC*\n0.201\n0.144\n3.834\n0.180\n0.023\n1.312\n0.179\n0.028\n1.021\n\n\nKL*\n0.197\n0.022\n0.052\n0.196\n0.014\n0.191\n0.197\n0.007\n0.478\n\n\nHigh ICI\n0.208\n0.061\n0.356\n0.203\n0.020\n0.205\n0.202\n0.003\n0.615\n\n\nmushroom\nAUC*\n0.000\n0.003\n1.384\n0.000\n0.002\n1.403\n0.000\n0.002\n1.403\n\n\nKL*\n0.013\n0.049\n0.620\n0.011\n0.021\n1.222\n0.005\n0.006\n1.337\n\n\nHigh ICI\n0.014\n0.059\n0.667\n0.011\n0.023\n1.272\n0.005\n0.004\n1.337\n\n\noccupancy\nAUC*\n0.009\n0.008\n1.148\n0.009\n0.006\n1.191\n0.008\n0.006\n1.088\n\n\nKL*\n0.011\n0.041\n0.862\n0.009\n0.006\n1.128\n0.009\n0.006\n1.032\n\n\nHigh ICI\n0.011\n0.031\n0.923\n0.010\n0.006\n1.217\n0.011\n0.010\n1.026\n\n\nwinequality\nAUC*\n0.178\n0.129\n4.184\n0.162\n0.031\n1.566\n0.157\n0.022\n1.093\n\n\nKL*\n0.173\n0.029\n0.146\n0.174\n0.043\n0.439\n0.171\n0.028\n1.109\n\n\nHigh ICI\n0.181\n0.042\n0.214\n0.181\n0.043\n0.411\n0.179\n0.025\n0.761\n\n\nspambase\nAUC*\n0.044\n0.010\n0.708\n0.046\n0.019\n1.112\n0.045\n0.016\n0.957\n\n\nKL*\n0.059\n0.045\n0.260\n0.057\n0.012\n0.647\n0.056\n0.009\n0.607\n\n\nHigh ICI\n0.077\n0.090\n0.951\n0.067\n0.015\n0.513\n0.067\n0.020\n0.934\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n10.3.2 Figures\n\n\nCodes to crate the figure\nplot_before_after &lt;- ggplot() +\n  geom_point(\n    data = initial_points,\n    mapping = aes(x = ici, y = kl, shape = result_type),\n    size = 2\n  ) +\n  geom_segment(\n    data = points_after_c,\n    mapping = aes(\n      x = ici, y = kl, xend = ici_end, yend = kl_end,\n      colour = recalib, linetype = recalib\n    ),\n    arrow = arrow(length=unit(.2, 'cm'))\n  ) +\n  scale_shape_manual(\n    NULL,\n    values = c(\n      \"AUC*\" = 19,\n      \"KL*\" = 15,\n      \"High ICI\" = 17\n    )\n  ) +\n  ggh4x::facet_wrap2(~dataset, scales = \"free_y\", ncol = 5, axes = \"all\") +\n  labs(x = \"ICI\", y = \"KL Divergence\") +\n  scale_colour_manual(\"Recalibration\", values = colour_recalib) +\n  scale_linetype_discrete(\"Recalibration\") +\n  theme_paper()\n\nggsave(\n  plot_before_after, file = \"../figs/real-dataset-before-after-calib.pdf\",\n  width = 12, height = 5\n)\n\nplot_before_after\n\n\n\n\n\nFigure 10.11: Average KL divergence and ICI before and after recalibration of the estimated scores.",
    "crumbs": [
      "IV. Real Data",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Real-data Results</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Austin, Peter C., and Ewout W. Steyerberg. 2019. “The Integrated\nCalibration Index (ICI) and Related Metrics for Quantifying the\nCalibration of Logistic Regression Models.” Statistics in\nMedicine 38 (21): 4051–65. https://doi.org/10.1002/sim.8281.\n\n\nBecker, Barry, and Ronny Kohavi. 1996.\n“Adult.” UCI Machine Learning Repository.\n\n\nBrier, Glenn W. 1950. “Verification of Forecasts Expressed in\nTerms of Probability.” Monthly Weather Review 78 (1):\n1–3.\n\n\nCaffo, Brian S, James G Booth, and AC Davison. 2002. “Empirical\nSupremum Rejection Sampling.” Biometrika 89 (4): 745–54.\n\n\nCandanedo, Luis. 2016. “Occupancy Detection .”\nUCI Machine Learning Repository.\n\n\nChen, Song Xi. 1999. “Beta Kernel Estimators for Density\nFunctions.” Computational Statistics & Data Analysis\n31 (2): 131–45.\n\n\nCortez, Paulo, A. Cerdeira, F. Almeida, T. Matos, and J. Reis. 2009.\n“Wine Quality.” UCI Machine Learning\nRepository.\n\n\n“Dry Bean.” 2020. UCI Machine Learning\nRepository.\n\n\nHopkins, Mark, Erik Reeber, George Forman, and Jaap Suermondt. 1999.\n“Spambase.” UCI Machine Learning Repository.\n\n\n“In-Vehicle Coupon Recommendation.” 2020. UCI\nMachine Learning Repository.\n\n\nKullback, S., and R. A. Leibler. 1951. “On Information and\nSufficiency.” The Annals of Mathematical Statistics 22\n(1): 79–86. https://doi.org/10.1214/aoms/1177729694.\n\n\nMoro, S., P. Rita, and P. Cortez. 2012. “Bank\nMarketing.” UCI Machine Learning Repository.\n\n\n“Mushroom.” 1987. UCI Machine Learning\nRepository.\n\n\nNash, Warwick, Tracy Sellers, Simon Talbot, Andrew Cawthorn, and Wes\nFord. 1995. “Abalone.” UCI Machine Learning\nRepository.\n\n\nOjeda, Francisco M., Max L. Jansen, Alexandre Thiéry, Stefan\nBlankenberg, Christian Weimar, Matthias Schmid, and Andreas Ziegler.\n2023. “Calibrating Machine Learning Approaches for Probability\nEstimation: A Comprehensive Comparison.” Statistics in\nMedicine 42 (29): 5451–78. https://doi.org/10.1002/sim.9921.\n\n\nRumbell, Timothy, Jaimit Parikh, James Kozloski, and Viatcheslav Gurev.\n2023. “Novel and Flexible Parameter Estimation Methods for\nData-Consistent Inversion in Mechanistic Modelling.” Royal\nSociety Open Science 10 (11): 230668.\n\n\nYeh, I-Cheng. 2016. “Default of Credit Card\nClients.” UCI Machine Learning Repository.",
    "crumbs": [
      "References"
    ]
  }
]