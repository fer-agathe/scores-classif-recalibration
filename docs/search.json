[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Recalibration",
    "section": "",
    "text": "1 Introduction\nThis notebook is the online appendix of the article titled xxx.” It provides supplementary materials to the main part of the paper.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#abstract-of-the-paper",
    "href": "index.html#abstract-of-the-paper",
    "title": "Recalibration",
    "section": "1.1 Abstract of the Paper",
    "text": "1.1 Abstract of the Paper",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#outline",
    "href": "index.html#outline",
    "title": "Recalibration",
    "section": "1.2 Outline",
    "text": "1.2 Outline",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#replication-codes",
    "href": "index.html#replication-codes",
    "title": "Recalibration",
    "section": "1.3 Replication Codes",
    "text": "1.3 Replication Codes\nThe codes to replicate the results displayed in the paper are presented in this ebook. We also provide the codes in an archive file with the following structure:\nSupplementary-materials\n├ ── ebook\n├ ── scripts\n│   └── functions\n|   |   └── data-ojeda.R\n|   |   └── data-setup-dgp-scenarios.R\n|   |   └── metrics.R\n|   |   └── real-data.R\n|   |   └── subsample_target_distribution.R\n|   |   └── utils.R\n│   └── 01_data_targeted_distrib.R\n│   └── 02_data-simulated.R\n│   └── 03_simul-trees.R\n│   └── 04_simul-random-forests.R\n│   └── 05_simul-random-forests-ntrees.R\n│   └── 06_simul-xgb.R\n│   └── 07_simul-glm.R\n│   └── 08_simul-gam.R\n│   └── 09_simul-gamsel.R\n│   └── 10_simul-comparison.R\n│   └── 11_real-priors-illustration.R\n│   └── 12_real-datasets-priors.R\n│   └── 13_real-estimations.R\n│   └── 14_real_results.R\n│   └── proj.Rproj\n Download the Replication Codes (Zip archive)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "target-distribution.html",
    "href": "target-distribution.html",
    "title": "2  Targeted Distribution",
    "section": "",
    "text": "2.1 Algorithm\nIn our generated sample, \\(\\mathcal{D}=\\{(\\boldsymbol{x}_i,y_i,{s}_i),i\\in\\{1,\\cdots,n\\}\\}\\), let \\(\\widehat{f}\\) denote the (empirical) density of scores. For the various scenarios, suppose that we want a specific distribution for the scores, denoted \\(g\\) (uniform, Beta, etc.). A classical idea is to use ‘’rejection sampling’’ techniques to create a subsample of the dataset. Set \\[\nc = \\sup_{s\\in(0,1)} \\frac{\\widehat{f}(s)}{g(s)} \\leq \\infty.\n\\] If \\(c\\) is finite, and not too large, we can use the standard rejection technique, described in Algorithm 2.1. In a nutshell, point \\(i\\) is kept with probability \\((cg(s_i))^{-1}\\widehat{f}(s_i)\\).\nNote: the reference mentioned: Chen (1999)\nIf \\(c\\) is too large, we use an iterative algorithm, described in Algorithm 2.2, inspired by Rumbell et al. (2023) (alternative options could be the ‘’Empirical Supremum Rejection Sampling’’ introduced in Caffo, Booth, and Davison (2002), for instance)\nTo implement this, we define the subset_target() function.\n#' @param data dataset\n#' @param score_name name of the column in data that contains the scores\n#' @param target_fun target distribution function.\n#' @param iter number of iterations.\n#' @param draw if TRUE (default) the distribution of scores (gray bars) and the\n#'  target distribution (in red) are plotted at each iteration.\n#' @seed if not `NULL`, seed to use\n#' @param data dataset\n#' @param probs_name name of the column in data that contains the observed\n#'  probabilities\n#' @param target_fun target distribution function.\n#' @param iter number of iterations.\n#' @param draw if TRUE (default) the distribution of scores (gray bars) and the\n#'  target distribution (in red) are plotted at each iteration.\n#' @seed if not `NULL`, seed to use\n#' @param verbose if `FALSE`, size of subsamplings at each iteration and KS test\n#'  results are hiddent\nsubset_target &lt;- function(data,\n                          probs_name,\n                          target_fun = function(x) dbeta(x,2,2),\n                          iter = 1,\n                          draw = TRUE,\n                          seed = NULL,\n                          verbose = TRUE){\n  select &lt;- rep(nrow(data),iter + 1)\n  if (!is.null(seed)) set.seed(seed)\n\n  # Get the scores from the dataset\n  probs_01 &lt;- data |&gt; pull(!!probs_name)\n  if (verbose == TRUE) cat(\"1) Size ...... \", nrow(data), \"\\n\", sep = \"\")\n\n  # Kolmogorov-Smirnov Test\n  fun &lt;- Vectorize(function(x) integrate(target_fun, 0, x)$value)\n  K &lt;- ks.test(probs_01, fun)\n\n  if (verbose) {\n    cat(\"1)  ks ............ \", K$statistic, \"\\n\", sep = \"\")\n    cat(\"1)  (pvalue) ...... \", K$p.value, \"\\n\", sep = \"\")\n  }\n\n  if (draw) {\n    # Histogram of scores (gray) and target distribution (red)\n    hist(probs_01,probability = TRUE, xlab = \"\", ylab = \"\", main = \"Initial\")\n    val_x &lt;- seq(0,1,length = 601)\n    lines(val_x,target_fun(val_x), col = \"red\")\n  }\n\n  data_subset &lt;- data\n\n  for (k in 1:iter) {\n    n &lt;- nrow(data_subset)\n    initial_density &lt;- kde(x = probs_01, eval.points = probs_01)\n    # Probability to include each observation in the current subset\n    prob_acceptation &lt;- target_fun(probs_01) / initial_density$estimate\n    prob_acceptation &lt;- pmin(prob_acceptation / max(prob_acceptation), 1)\n    # For each scores from the current data subset, decide whether or not to\n    # include it based on a random draw from a Ber(prob_acceptation)\n    index_acceptation &lt;- rbinom(n, size = 1, prob = prob_acceptation)\n    # Use this index to keep only the selected data\n    data_subset &lt;- data_subset[which(index_acceptation ==1 ), ]\n    select[k + 1] &lt;- nrow(data_subset)\n    probs_01 &lt;- data_subset |&gt; pull(!!probs_name)\n    if (verbose == TRUE)\n      cat(k + 1, \") Size ...... \", nrow(data_subset), \"\\n\", sep = \"\")\n    # Kolmogorov-Smirnov Test\n    K &lt;- ks.test(probs_01, fun)\n    if (verbose) {\n      cat(k + 1, \")   ks ............ \", K$statistic, \"\\n\", sep = \"\")\n      cat(k + 1, \")   (pvalue) ...... \", K$p.value, \"\\n\", sep = \"\")\n    }\n    if (draw) {\n      hist(\n        probs_01, probability = TRUE, xlab = \"\", ylab = \"\",\n        main = paste(\"Iteration \", k)\n      )\n      val_x &lt;- seq(0, 1, length = 601)\n      lines(val_x, target_fun(val_x), col = \"red\")\n    }\n  }\n  data_subset\n}",
    "crumbs": [
      "I. Subsampling",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Targeted Distribution</span>"
    ]
  },
  {
    "objectID": "target-distribution.html#algorithm",
    "href": "target-distribution.html#algorithm",
    "title": "2  Targeted Distribution",
    "section": "",
    "text": "\\begin{algorithm} \\caption{Subsample a dataset so that the distribution of scores has density $g$ (Rejection, $c$ small)} \\begin{algorithmic} \\Require $\\mathcal{D}=\\{(\\boldsymbol{x}_i,y_i,{s}_i),i\\in\\{1,\\cdots,n\\}\\}$ and $g$ (target density) \\State $\\mathcal{I} \\gets ,i\\in\\{1,\\cdots,n\\}$ \\State $\\widehat{f} \\gets$ density of $\\{({s}_i),i\\in\\mathcal{I}\\}$, using Chen (1999) \\State $c = \\displaystyle\\sup_{s\\in(0,1)} \\frac{\\widehat{f}(s)}{g(s)} \\gets \\max_{i=1,\\cdots,n}\\displaystyle\\frac{\\widehat{f}(s_i)}{g(s_i)} $ \\For{$i\\in\\{1,\\cdots,n\\}$} \\State $U \\gets \\mathcal{U}([0,1])$ \\If{$\\displaystyle U &gt; \\frac{\\widehat{f}(s_i)}{c\\,g(s_i)}$} \\State $\\mathcal{I} \\gets \\mathcal{I}\\backslash\\{i\\}$ , i.e. ``reject\" \\EndIf \\EndFor \\State $s\\mathcal{D}=\\{(\\boldsymbol{x}_i,y_i,{s}_i),i\\in\\mathcal{I}\\}$ \\end{algorithmic} \\end{algorithm}\n\n\n\n\n\n\n\\begin{algorithm} \\caption{Subsample a dataset so that the distribution of scores has density $g$ (Iterative Rejection, $c$ large)} \\begin{algorithmic} \\Require $\\mathcal{D}=\\{(\\boldsymbol{x}_i,y_i,{s}_i),i\\in\\{1,\\cdots,n\\}\\}$, $\\epsilon&gt;0$ and $g$ (target density) \\State $\\mathcal{I} \\gets \\{1,\\cdots,n\\}$ \\State $\\widehat{f} \\gets$ density of $\\{({s}_i),i\\in\\mathcal{I}\\}$, using Chen (1999) \\State $d \\gets \\|\\widehat{F}-G\\|_{\\infty}$ (Kolmogorov-Smirnov distance) \\While{$d&gt;\\epsilon$} \\State $\\mathcal{J} \\gets \\mathcal{I}$ \\For{$i\\in\\mathcal{I}$} \\State $U \\gets \\mathcal{U}([0,1])$ \\If{$\\displaystyle U&gt;\\frac{\\widehat{f}(s_i)}{g(s_i)}$} \\State $\\mathcal{J} \\gets \\mathcal{J}\\backslash\\{i\\}$ , i.e. 'reject' observation $i$ \\EndIf \\EndFor \\State $\\mathcal{I} \\gets \\mathcal{J}$ \\State $\\widehat{f} \\gets$ density of $\\{({s}_i),i\\in\\mathcal{I}\\}$ \\State $d \\gets \\|\\widehat{F}-G\\|_{\\infty}$ \\EndWhile \\State $s\\mathcal{D}=\\{(\\boldsymbol{x}_i,y_i,{s}_i),i\\in\\mathcal{I}\\}$ \\end{algorithmic} \\end{algorithm}",
    "crumbs": [
      "I. Subsampling",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Targeted Distribution</span>"
    ]
  },
  {
    "objectID": "target-distribution.html#first-example",
    "href": "target-distribution.html#first-example",
    "title": "2  Targeted Distribution",
    "section": "2.2 First Example",
    "text": "2.2 First Example\nLet us begin with generating some binary data, using a linear predictor for the true probability.\n\nn &lt;- 1e5 # Number of obs.\n# Covariates\nx1 &lt;- rnorm(n)\nx2 &lt;- rnorm(n)\n# True probabilities\np &lt;- function(x1, x2) .4 * x1 - .2*x2\n# Observed event\ny &lt;- rnorm(n,p(x1, x2), .4)\ntb &lt;- tibble(y = y, x1 = x1, x2 = x2)\n\nLet us consider a linear model to predict the observed event:\n\nreg &lt;- lm(y ~ x1 + x2, data = tb)\nscores &lt;- predict(reg)\ntb$scores &lt;- scores\n\nKeeping only scores between 0 and 1 (would not need to do so for glm)\n\ntb_01 &lt;- tb[(scores &gt; 0) & (scores &lt; 1), ]\ndata &lt;- tb_01\n\n\nB &lt;- subset_target(data = data, probs_name = \"scores\", iter = 4)\n\n1) Size ...... 48486\n1)  ks ............ 0.2997993\n1)  (pvalue) ...... 0\n\n\n\n\n\n\n\n\n\n2) Size ...... 17127\n2)   ks ............ 0.004201355\n2)   (pvalue) ...... 0.9229854\n\n\n\n\n\n\n\n\n\n3) Size ...... 16802\n3)   ks ............ 0.00263152\n3)   (pvalue) ...... 0.9998175\n\n\n\n\n\n\n\n\n\n4) Size ...... 16252\n4)   ks ............ 0.002203808\n4)   (pvalue) ...... 0.9999985\n\n\n\n\n\n\n\n\n\n5) Size ...... 15982\n5)   ks ............ 0.002392562\n5)   (pvalue) ...... 0.9999885\n\n\n\n\n\n\n\n\n\nLet us consider another example.\n\nlibrary(splines)",
    "crumbs": [
      "I. Subsampling",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Targeted Distribution</span>"
    ]
  },
  {
    "objectID": "target-distribution.html#second-examplle",
    "href": "target-distribution.html#second-examplle",
    "title": "2  Targeted Distribution",
    "section": "2.3 Second Examplle",
    "text": "2.3 Second Examplle\nWe generate another dataset.\n\nn &lt;- 1e6\nx &lt;- rbeta(n, 1, 2)\ny &lt;- rbinom(n, size = 1, prob = x)\nbase &lt;- tibble(\n  x = x,\n  y = y,\n  id = 1:n\n)\n\nLet us assume that the scores are estimated using a logistic model.\n\nreg &lt;- glm(y ~ bs(x), data = base, family = binomial)\nbase$scores &lt;- predict(reg, type = \"response\")\n\nLet us further assume that we want the scores to be distributed according to a Beta(2,1).\n\nB &lt;- subset_target(\n  data = base, \n  probs_name = \"scores\", \n  iter = 1, \n  target_fun = function(x) dbeta(x,2,1)\n)\n\n1) Size ...... 1000000\n\n\nWarning in ks.test.default(probs_01, fun): ties should not be present for the\none-sample Kolmogorov-Smirnov test\n\n\n1)  ks ............ 0.502429\n1)  (pvalue) ...... 0\n\n\n\n\n\n\n\n\n\n2) Size ...... 7433\n2)   ks ............ 0.05460928\n2)   (pvalue) ...... 1.115475e-19\n\n\n\n\n\n\n\n\n\nWe check the new observations:\n\nreg2 &lt;- glm(y ~ bs(x), data = B, family = binomial)\n\n\nval_x &lt;- seq(0, 1, length = 601)\nplot(\n  val_x,\n  predict(reg, type = \"response\", newdata = data.frame(x = val_x)),\n  type = \"l\", lwd = 2\n)\n\nWarning in bs(x, degree = 3L, knots = numeric(0), Boundary.knots =\nc(1.13039405272881e-06, : some 'x' values beyond boundary knots may cause\nill-conditioned bases\n\nlines(\n  val_x,\n  predict(reg2, type = \"response\", newdata = data.frame(x = val_x)),\n  type = \"l\", lwd = 2, col = \"red\"\n)\n\nWarning in bs(x, degree = 3L, knots = numeric(0), Boundary.knots =\nc(0.00521055325908034, : some 'x' values beyond boundary knots may cause\nill-conditioned bases\n\n\n\n\n\n\n\n\n\n\n\n\n\nCaffo, Brian S, James G Booth, and AC Davison. 2002. “Empirical Supremum Rejection Sampling.” Biometrika 89 (4): 745–54.\n\n\nChen, Song Xi. 1999. “Beta Kernel Estimators for Density Functions.” Computational Statistics & Data Analysis 31 (2): 131–45.\n\n\nRumbell, Timothy, Jaimit Parikh, James Kozloski, and Viatcheslav Gurev. 2023. “Novel and Flexible Parameter Estimation Methods for Data-Consistent Inversion in Mechanistic Modelling.” Royal Society Open Science 10 (11): 230668.",
    "crumbs": [
      "I. Subsampling",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Targeted Distribution</span>"
    ]
  },
  {
    "objectID": "metrics.html",
    "href": "metrics.html",
    "title": "3  Metrics",
    "section": "",
    "text": "3.1 Performance and Calibration Metrics\nTo measure performance, we chose to compute:\nTo measure calibration, we compute two metrics:\nAll these metrics are computed in a function we name compute_metrics() which takes three arguments:\n#' Brier Score\n#'\n#' The Brier Score \\citep{brier_1950}, is expressed as: \\deqn{\\text{BS} =\n#' \\frac{1}{n}\\sum_{i=1}^{n} \\big(\\hat{s}(\\mathbf{x}_i) - d_i\\big)^{2}} where\n#' \\eqn{d_i \\in \\{0,1\\}} is the observed event for observation \\eqn{i}.\n#'\n#' @param scores vector of scores\n#' @param obs vector of observed binary events\n#'\n#' @references Brier, G. W. (1950). Verification of forecasts expressed in terms\n#' of probability. Monthly Weather Review 78: 1–3.\n#'\n#' @export\nbrier_score &lt;- function(obs, scores) mean((scores - obs)^2)\n#' Computes the calibration metrics for a set of observed and predicted\n#' probabilities\n#'\n#' @returns\n#' \\itemize{\n#'   \\item \\code{mse}: True Mean Squared Error based on true probability.\n#'   \\item \\code{acc}: accuracy with a .5 probability threshold.\n#'   \\item \\code{AUC}: Area Under the ROC Curve.\n#'   \\item \\code{brier}: Brier score.\n#'   \\item \\code{ici}: Integrated Calibration Index.\n#' }\n#'\n#' @param obs observed events\n#' @param scores predicted scores\n#' @param true_probas true probabilities from the PGD (to compute MSE)\n#'\n#' @importFrom purrr map\n#' @importFrom tibble tibble\n#' @importFrom dplyr bind_rows\n#'\n#' @export\ncompute_metrics &lt;- function(obs,\n                            scores,\n                            true_probas = NULL) {\n\n  # True MSE\n  if (!is.null(true_probas)) {\n    mse &lt;- mean((true_probas - scores)^2)\n  } else {\n    mse &lt;- NA\n  }\n\n  # True MAE\n  if (!is.null(true_probas)) {\n    mae &lt;- mean(abs(true_probas - scores))\n  } else {\n    mae &lt;- NA\n  }\n\n  # AUC\n  AUC &lt;- pROC::auc(obs, scores, levels = c(\"0\", \"1\"), quiet = TRUE) |&gt;\n    as.numeric()\n\n  # Brier Score\n  brier &lt;- brier_score(obs = as.numeric(as.character(obs)), scores = scores)\n  # gmish::brier(pred = scores, obs = obs) #same results\n\n  # ICI\n  ici_quiet &lt;- purrr::quietly(gmish::ici)\n  ici &lt;- ici_quiet(pred = scores, obs = as.numeric(as.character(obs)))\n  ici &lt;- ici$result\n\n  # Accuracy\n  pred_class &lt;- ifelse(scores &gt; .5, yes = 1, no = 0)\n  acc &lt;- sum(diag(table(obs = obs, pred = pred_class))) / length(scores)\n\n  tibble(\n    mse = mse,\n    mae = mae,\n    acc = acc,\n    AUC = AUC,\n    brier = brier,\n    ici = ici\n  )\n}",
    "crumbs": [
      "II. Metrics",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Metrics</span>"
    ]
  },
  {
    "objectID": "metrics.html#performance-and-calibration-metrics",
    "href": "metrics.html#performance-and-calibration-metrics",
    "title": "3  Metrics",
    "section": "",
    "text": "the true Mean Squared Error (MSE): the average of the quadratic difference between predicted scores and true probabilities (only if the true probabilities are available thanks to the knowledge of the PGD)\nthe accuracy, which gives the proportion of correctly predicted instances; we use a probability threshold of 0.5)\nthe AUC.\n\n\n\nthe Brier score (Brier (1950))\nthe Integrated Calibration Index (Austin and Steyerberg (2019)).\n\n\n\n\n\n\n\nBrier Score\n\n\n\nGiven a sample size \\(n\\), the Brier Score Brier (1950), is expressed as: \\[\n\\begin{equation}\n\\text{BS} = \\frac{1}{n}\\sum_{i=1}^{n} \\big(\\hat{s}(\\mathbf{x}_i) - d_i\\big)^{2}\\enspace ,\n\\end{equation}\n\\tag{3.1}\\]\nwhere \\(\\hat{s}(\\mathbf{x}_i)\\) and \\(d_i \\in \\{0,1\\}\\) are the predicted score and observed event, respectively, for observation \\(i\\).\n\n\n\n\n\n\n\n\nIntegrated Calibration Index\n\n\n\nInstead of defining bins, the Integrated Calibration Index or ICI (Austin and Steyerberg (2019)) measures calibration using a local estimation (loess if the number of observation is lower than 1000 ; using a GAM otherwise).\nThe occurrence of the binary event is regressed on the predicted scores, employing either locally estimated scatterplot smoothing (LOESS) when the number of observations is small (\\(n &lt; 1000\\)) or cubic regression splines for larger datasets. The ICI is defined as \\[\n\\begin{equation}\n    \\text{ICI} = \\int_{0}^{1} f(p)  \\phi(p)\\, dp\n\\end{equation}\n\\tag{3.2}\\] where \\(f(p) = | p - \\g(p) |\\) is the absolute difference between the calibration curve and the bisector where \\(p\\) denotes a predicted score (i.e., \\(p=\\hat{s}(\\mathbf{x})\\)) and \\(\\g(p)\\) is the value of the calibration curve at this predicted score. The density function of the distribution of predicted scores is denoted \\(\\phi(p)\\).\n\n\n\n\nobs: a vector of observed binary events\nscores: a vector of predicted scores\ntrue_probas: if available, a vector of true probabilities from the PGD (to compute MSE).",
    "crumbs": [
      "II. Metrics",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Metrics</span>"
    ]
  },
  {
    "objectID": "metrics.html#dispersion-metrics",
    "href": "metrics.html#dispersion-metrics",
    "title": "3  Metrics",
    "section": "3.2 Dispersion Metrics",
    "text": "3.2 Dispersion Metrics\nWe compute the Kullback-Leibler divergence Kullback and Leibler (1951) between the distribution of the estimated scores and the distribution of the true probabilities. Denoting (Q) the distribution of the scores and (P) the distribution of the true probabilities, the Kullback Leibler divergence of \\(Q\\) with respect to \\(P\\) is :% \\[\\begin{equation}\nD_{KL}(Q || P) = \\sum_{i} Q(i) \\log \\frac{Q(i)}{P(i)}.\n\\end{equation}\\]\nThe distributions both need to be discretized. We divide the segment ([0,1]) into (m) bins.\nIn the dispersion_metrics() that we define to that end, we consider \\(m=20\\) bins. We also consider switching the reference distribution (where \\(Q\\) denotes the distribution of the true probabilities and \\(P\\) denotes the distribution of scores).\n\n#' Computes the dispersion metrics for a set of observed and predicted\n#' probabilities\n#'\n#' @returns\n#' \\itemize{\n#'   \\item \\code{inter_quantile_25_75}: Difference of inter-quantile between 25% and 75%\n#'   \\item \\code{inter_quantile_10_90}: Difference of inter-quantile between 10% and 90%\n#'   \\item \\code{KL_20_true_probas}: KL of of predicted probabilities w.r. to true probabilities with 20 bins\n#'   \\item \\code{KL_20_scores}: KL of of true probabilities w.r. to predicted probabilities with 20 bins\n#'   \\item \\code{ind_cov}: Difference between the variance of true probabilities and the covariance between true probabilities and predicted scores\n#' }\n#'\n#' @param true_probas true probabilities from simulations\n#' @param scores predicted scores\n#'\ndispersion_metrics &lt;- function(true_probas, scores){\n\n  # Inter-quantiles\n  inter_q_80 &lt;- diff(quantile(scores, c(.9, .1))) /\n    diff(quantile(true_probas, c(.9, .1)))\n  inter_q_50 &lt;- diff(quantile(scores, c(.75,.25))) /\n    diff(quantile(true_probas, c(.75, .25)))\n\n  # KL divergences\n  m &lt;- 20 # Number of bins\n  h_p &lt;- hist(true_probas,breaks = seq(0, 1, length = m + 1), plot = FALSE)\n  h_phat &lt;- hist(scores, breaks = seq(0, 1, length = m + 1), plot = FALSE)\n  # Densities\n  h1 &lt;- rbind(h_phat$density / m,h_p$density / m) # Reference : true probabilities\n  h2 &lt;- rbind(h_p$density / m, h_phat$density / m) # Reference : predicted scores\n  KL_20_true_probas &lt;- distance(\n    h1, method = \"kullback-leibler\", unit = \"log2\", mute.message = TRUE)\n  KL_20_scores &lt;- distance(\n    h2, method = \"kullback-leibler\", unit = \"log2\", mute.message = TRUE)\n\n  # Indicator of the difference between variance and covariance\n  var_p &lt;- var(true_probas)\n  cov_p_phat &lt;- cov(true_probas, scores)\n  ind_cov &lt;- abs(cov_p_phat - var_p)\n\n  # Collection\n  dispersion_metrics &lt;- tibble(\n    \"inter_quantile_25_75\" = as.numeric(inter_q_50),\n    \"inter_quantile_10_90\" = as.numeric(inter_q_80),\n    \"KL_20_true_probas\" = as.numeric(KL_20_true_probas),\n    \"KL_20_scores\" = as.numeric(KL_20_scores),\n    \"ind_cov\" = ind_cov\n    )\n\n  dispersion_metrics\n}\n\nLastly, we estimate \\(\\mathbb{P}(q_1 &lt; \\hat{s}(\\mathbf{x}) &lt; q_2)\\), with \\(q_2 = 1-q_1\\), for different values of \\(q_1\\) and \\(q_2\\). To do so, we simply calculate the sample proportion of scores between \\(q_1\\) and \\(q_2\\). The prop_btw_quantiles() does it.\n\n#' Computes \\hat{P}(q_1 &lt; s &lt; q_2)\n#'\n#' @param s scores\n#' @param q1 lower quantile\n#' @param q2 upper quantile (default to 1-q2)\nprop_btw_quantiles &lt;- function(s, q1, q2 = 1 - q1) {\n  tibble(q1 = q1, q2 = q2, freq = mean(s &lt; q2 & s &gt; q1))\n}\n\n\n\n\n\nAustin, Peter C., and Ewout W. Steyerberg. 2019. “The Integrated Calibration Index (ICI) and Related Metrics for Quantifying the Calibration of Logistic Regression Models.” Statistics in Medicine 38 (21): 4051–65. https://doi.org/10.1002/sim.8281.\n\n\nBrier, Glenn W. 1950. “Verification of Forecasts Expressed in Terms of Probability.” Monthly Weather Review 78 (1): 1–3.\n\n\nKullback, S., and R. A. Leibler. 1951. “On Information and Sufficiency.” The Annals of Mathematical Statistics 22 (1): 79–86. https://doi.org/10.1214/aoms/1177729694.",
    "crumbs": [
      "II. Metrics",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Metrics</span>"
    ]
  },
  {
    "objectID": "simul-data.html",
    "href": "simul-data.html",
    "title": "4  Simulated Data",
    "section": "",
    "text": "4.1 Functions\nWe load the functions from Chapter 2 to subsample from a dataset so that the true probability follows a beta distribution.\nlibrary(ks)\nsource(\"../scripts/functions/subsample_target_distribution.R\")\nThe simulate_data() function generates data for one of the 12 first scenarios described in the article or one of our additional 4 scenarios. This is a helper function that is called in the second one, simulate_data_wrapper() which generates datasets.\nCode\n#' Simulates train/validation/calibration/test\n#'\n#' @details\n#' This function is a modified version of the function 'simulateData' in the\n#' R script 'functions-for-calibrating-random-forests.R' provided in the\n#' supplementary material of  Dankowski, T., & Ziegler, A. (2016). Calibrating\n#' random forests for probability estimation. Statistics in medicine, 35(22),\n#' 3949-3960.\n#'\n#' @param n_num number of numerical covariates\n#' @param add_categ if `TRUE`, add 5 categorical variables\n#' @param coeff vector of coefficients (of length n_num + 5)\n#' @param n_noise number of noise variables (drawn from N(0,1))\n#' @param mean_num vector of mean for the numerical variables\n#' @param sd_num vector of standard deviations for the numerical variables\n#' @param size_train size for the train set\n#' @param size_valid size for the validation set\n#' @param size_calib size for the calibration set\n#' @param size_test size for the test set\n#' @param transform_probs if `TRUE`, the true probability is taken to the power of 3\n#' @param linear_predictor if `TRUE`, the predictor of the true probability is a\n#'  linear combination of the covariates. Otherwise, the squared term for x1 is\n#'  added, as well as an interaction term between x2 and x3 (`n_num` thus need\n#'  to be at least 3).\n#' @param seed desired seed (default to `NULL`)\n#' @param linear_predictor_factor if `transform_probs = TRUE`, scalar used to\n#'  draw more observation before subsampling. Default to 3 (a sample 3 times\n#'  larger than `the size of the samples will first be generated before\n#'  subsampling so that the true probability follows a Beta(2,2).\n#'\n#' @returns A list with the following components:\n#'  - train: train set\n#'  - valid: validation set\n#'  - calib: calibration set\n#'  - test: test set\n#'  - probs_train: true probabilities for binary event in train set\n#'  - probs_valid: true probabilities for binary event in validation set\n#'  - probs_calib: true probabilities for binary event in calibration set \n#'  - probs_test: true probabilities for binary event in test set\nsimulate_data &lt;- function(n_num = 2,\n                          add_categ = FALSE,\n                          coeff,\n                          n_noise = 0,\n                          mean_num,\n                          sd_num,\n                          size_train,\n                          size_valid,\n                          size_calib,\n                          size_test,\n                          transform_probs = FALSE,\n                          linear_predictor = TRUE,\n                          linear_predictor_factor = 3,\n                          seed = NULL) {\n\n  n_obs &lt;- size_train + size_valid + size_calib + size_test\n  if (linear_predictor == FALSE) {\n    n_obs &lt;- n_obs * linear_predictor_factor\n  }\n\n  if (!is.null(seed)) {\n    set.seed(seed)\n  }\n\n  # Numerical covariates\n  covariates &lt;- map2(\n    .x = mean_num,\n    .y = sd_num,\n    .f = ~rnorm(n = n_obs, mean = .x, sd = .y)\n  )\n  names(covariates) &lt;- str_c(\"x\", 1:n_num)\n  covariates &lt;- as_tibble(covariates)\n\n  # Categorical covariates\n  if (add_categ == TRUE) {\n    x_c1 &lt;- base::sample(c(0, 1), n_obs, replace = TRUE)\n    x_c2 &lt;- base::sample(c(0, 1), n_obs, replace = TRUE)\n    x_c3 &lt;- base::sample(c(1, 2, 3), n_obs, replace = TRUE)\n    x_c4 &lt;- base::sample(c(1, 2, 3, 4), n_obs, replace = TRUE)\n    x_c5 &lt;- base::sample(c(1, 2, 3, 4, 5), n_obs, replace = TRUE)\n\n    categ_covariates &lt;- tibble(x_c1, x_c2, x_c3, x_c4, x_c5)\n    colnames(categ_covariates) &lt;- str_c(\"x\", (n_num + 1):(n_num + 5))\n    covariates &lt;- bind_cols(covariates, categ_covariates)\n  }\n\n  if (linear_predictor == TRUE) {\n    # Linear predictor\n    eta &lt;- as.matrix(covariates) %*% coeff\n  } else {\n    if (n_num &lt; 3) stop(\"If linear_predictor=TRUE, n_num must be greater than 2\")\n    eta &lt;- as.matrix(covariates) %*% coeff +\n      covariates$x1^2 + covariates$x2^2 * covariates$x3\n  }\n\n  # True probability\n  true_prob &lt;- as.numeric(1 / (1 + exp(-eta)))\n  if (transform_probs) true_prob &lt;- true_prob^3\n\n  # Observed event\n  y &lt;- rbinom(n_obs, size = 1, prob = true_prob)\n\n  # Create dataset with observed event and covariates\n  tb &lt;- tibble(y, covariates)\n\n  if (linear_predictor == FALSE) {\n    # We would like the probabilities to be distributed as a Beta(2,2)\n    tb &lt;- tb |&gt; mutate(p = true_prob)\n    tb &lt;- subset_target(\n      data = tb,\n      probs_name = \"p\",\n      target_fun = function(x) dbeta(x,2,2),\n      iter = 1, draw = FALSE,\n      seed = seed,\n      verbose = FALSE\n    )\n    n_obs &lt;- size_train + size_calib + size_valid + size_test\n    if (nrow(tb) &lt; n_obs) {\n      stop(\n        str_c(\"The number of observation generated is lower than the \",\n              \"desired number. Increase `linear_predictor_factor`.\")\n      )\n    }\n    true_prob &lt;- tb$p[1:n_obs]\n    tb &lt;- tb |&gt; select(-p) |&gt; dplyr::slice_head(n = n_obs)\n  }\n\n\n  # Noise variables\n  if (n_noise &gt; 0) {\n    noise &lt;- matrix(\n      rnorm(n_noise * n_obs, mean = 0, sd = 1),\n      ncol = n_noise,\n      nrow = n_obs,\n      byrow = FALSE\n    ) |&gt;\n      as_tibble()\n    colnames(noise) &lt;- str_c(\"noise_\", 1:n_noise)\n    tb &lt;- bind_cols(tb, noise)\n  }\n\n  # Split data into train/calib/valid/test\n  tb_train &lt;- tb |&gt; dplyr::slice(1:size_train)\n  true_prob_train &lt;- true_prob[1:size_train]\n  \n  # Validation\n  ind_valid &lt;- (size_train + 1):(size_train + size_valid)\n  tb_valid &lt;- tb |&gt; dplyr::slice(ind_valid)\n  true_prob_valid &lt;- true_prob[ind_valid]\n  \n  # Calibration\n  ind_calib &lt;- (size_train + size_valid + 1):(size_train + size_valid + size_calib)\n  tb_calib &lt;- tb |&gt; dplyr::slice(ind_calib)\n  true_prob_calib &lt;- true_prob[ind_calib]\n  \n  # Test\n  ind_test &lt;- (size_train + size_valid + size_calib + 1):n_obs\n  tb_test &lt;- tb |&gt; dplyr::slice(ind_test)\n  true_prob_test &lt;- true_prob[ind_test]\n  \n  list(\n    train = tb_train,\n    valid = tb_valid,\n    calib = tb_calib,\n    test = tb_test,\n    probs_train = true_prob_train,\n    probs_valid = true_prob_valid,\n    probs_calib = true_prob_calib,\n    probs_test = true_prob_test\n  )\n}\nThe simulate_data_wrapper() is the one we call to generate a dataset, given a scenario and a seed.\nCode\n#' Generates data for a given simulation scenario.\n#'\n#' @details\n#' Wrapper of 'simulate_data' function that generates the data for a given\n#' simulation scenario.\n#'\n#' @param scenario simulation scenario number.\n#' @param params_df data frame containing the parameters to be passed to the\n#'  `simulate_data` for each simulation scenario.\n#' @param repn Number of current replication to be generated for the given\n#'  simulation scenario.\n#'\n#' @returns A list with the following components:\n#'  - scenario: the scenario ID\n#'  - params_df: the parameters used for the data generation for the given\n#'               scenario.\n#'  - repn: Number of current replication that was generated for the given\n#'          simulation scenario.\n#'  - data: list with the simulated data (train, valid, test, probs_train,\n#'          probs_valid and probs_test)\n#'          see result of `simulate_data()`.\nsimulate_data_wrapper &lt;- function(scenario, params_df, repn) {\n  params &lt;- params_df[params_df[[\"scenario\"]] == scenario, ]\n  if(nrow(params) != 1) stop(\"More than one row from params_df chosen\")\n\n  seed_for_repn &lt;- pull(params, \"seed\") + repn\n\n  args &lt;- list(\n    coeff = params |&gt; pull(\"coefficients\") |&gt; pluck(1),\n    n_num = params |&gt; pull(\"n_num\"),\n    add_categ = params |&gt; pull(\"add_categ\"),\n    n_noise = params |&gt; pull(\"n_noise\"),\n    mean_num = params |&gt; pull(\"mean_num\") |&gt; pluck(1),\n    sd_num = params |&gt; pull(\"sd_num\") |&gt; pluck(1),\n    size_train = params |&gt; pull(\"size_train\"),\n    size_valid = params |&gt; pull(\"size_valid\"),\n    size_calib = params |&gt; pull(\"size_calib\"),\n    size_test = params |&gt; pull(\"size_test\"),\n    transform_probs = params |&gt; pull(\"transform_probs\"),\n    linear_predictor = params |&gt; pull(\"linear_predictor\"),\n    seed = seed_for_repn\n  )\n  sim_data &lt;- do.call(\"simulate_data\", args)\n\n  list(\n    scenario = scenario,\n    params_df = params,\n    repn = repn,\n    data = sim_data\n  )\n\n}",
    "crumbs": [
      "III. Simulated Data",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Simulated Data</span>"
    ]
  },
  {
    "objectID": "simul-data.html#scenarios",
    "href": "simul-data.html#scenarios",
    "title": "4  Simulated Data",
    "section": "4.2 Scenarios",
    "text": "4.2 Scenarios\nLet us define the 12 first scenarios, using the code provided in Ojeda et al. (2023).\n\nDGP 1:\n\nScenario 1: basic scenario with two continuous predictors, without noise variable\nScenarios 2, 3, 4: same as 1 but with noise variables (10, 50, 100)\n\nDGP 2:\n\nScenarios 5 to 8: similar to 1 to 4 but with right-skewed true probability distribution (true probability taken to the power of 3)\n\nDGP 3:\n\nScenarios 9 to 12: similar to 1 to 4 but with ten predictors instead of two (5 numerical and 5 categorical)\n\n\nWe add four other scenarios, in which the predictor is nonlinear:\n\nDGP 4:\n\nScenarios 13 to 16: similar to 1 to 4 but with 3 covariates instead of 2 and with a nonlinear predictor which also contains an interaction term (\\(\\eta = \\alpha _1x_1 + \\alpha_2 x_2 + \\alpha_3 x_3 + \\beta x_1^2 + \\gamma x_2 \\times x_3\\)). In addition, the distribution of the true probabilities of the observed data follows a Beta(2,2) distribution.\n\n\nThe desired number of replications for each scenario needs to be set:\n\nrepns_vector &lt;- 1:100\n\nWe set the different parameters for each scenario.\n\n# Coefficients beta\ncoefficients &lt;- list(\n  # First category (baseline, 2 covariates)\n  c(0.5, 1),  # scenario 1, 0 noise variable\n  c(0.5, 1),  # scenario 2, 10 noise variables\n  c(0.5, 1),  # scenario 3, 50 noise variables\n  c(0.5, 1),  # scenario 4, 100 noise variables\n  # Second category (same as baseline, with lower number of 1s)\n  c(0.5, 1),  # scenario 5, 0 noise variable\n  c(0.5, 1),  # scenario 6, 10 noise variables\n  c(0.5, 1),  # scenario 7, 50 noise variables\n  c(0.5, 1),  # scenario 8, 100 noise variables\n  # Third category (same as baseline but with 5 num. and 5 categ. covariates)\n  c(0.1, 0.2, 0.3, 0.4, 0.5, 0.01, 0.02, 0.03, 0.04, 0.05),\n  c(0.1, 0.2, 0.3, 0.4, 0.5, 0.01, 0.02, 0.03, 0.04, 0.05),\n  c(0.1, 0.2, 0.3, 0.4, 0.5, 0.01, 0.02, 0.03, 0.04, 0.05),\n  c(0.1, 0.2, 0.3, 0.4, 0.5, 0.01, 0.02, 0.03, 0.04, 0.05),\n  # Fourth category (nonlinear predictor, 3 covariates)\n  c(0.5, 1, .3),  # scenario 5, 0 noise variable\n  c(0.5, 1, .3),  # scenario 6, 10 noise variables\n  c(0.5, 1, .3),  # scenario 7, 50 noise variables\n  c(0.5, 1, .3)  # scenario 8, 100 noise variables\n)\n\n# Mean parameter for the normal distribution to draw from to draw num covariates\nmean_num &lt;- list(\n  # First category (baseline, 2 covariates)\n  rep(0, 2),  # scenario 1, 0 noise variable\n  rep(0, 2),  # scenario 2, 10 noise variables\n  rep(0, 2),  # scenario 3, 50 noise variables\n  rep(0, 2),  # scenario 4, 100 noise variables\n  # Second category (same as baseline, with lower number of 1s)\n  rep(0, 2),  # scenario 5, 0 noise variable\n  rep(0, 2),  # scenario 6, 10 noise variables\n  rep(0, 2),  # scenario 7, 50 noise variables\n  rep(0, 2),  # scenario 8, 100 noise variables\n  # Third category (same as baseline but with 5 num. and 5 categ. covariates)\n  rep(0, 5),\n  rep(0, 5),\n  rep(0, 5),\n  rep(0, 5),\n  # Fourth category (nonlinear predictor, 3 covariates)\n  rep(0, 3),\n  rep(0, 3),\n  rep(0, 3),\n  rep(0, 3)\n)\n# Sd parameter for the normal distribution to draw from to draw num covariates\nsd_num &lt;- list(\n  # First category (baseline, 2 covariates)\n  rep(1, 2),  # scenario 1, 0 noise variable\n  rep(1, 2),  # scenario 2, 10 noise variables\n  rep(1, 2),  # scenario 3, 50 noise variables\n  rep(1, 2),  # scenario 4, 100 noise variables\n  # Second category (same as baseline, with lower number of 1s)\n  rep(1, 2),  # scenario 5, 0 noise variable\n  rep(1, 2),  # scenario 6, 10 noise variables\n  rep(1, 2),  # scenario 7, 50 noise variables\n  rep(1, 2),  # scenario 8, 100 noise variables\n  # Third category (same as baseline but with 5 num. and 5 categ. covariates)\n  rep(1, 5),\n  rep(1, 5),\n  rep(1, 5),\n  rep(1, 5),\n  # Fourth category (nonlinear predictor, 3 covariates)\n  rep(1, 3),\n  rep(1, 3),\n  rep(1, 3),\n  rep(1, 3)\n)\n\nparams_df &lt;- tibble(\n  scenario = 1:16,\n  coefficients = coefficients,\n  n_num = c(rep(2, 8), rep(5, 4), rep(3, 4)),\n  add_categ = c(rep(FALSE, 8), rep(TRUE, 4), rep(FALSE, 4)),\n  n_noise = rep(c(0, 10, 50, 100), 4),\n  mean_num = mean_num,\n  sd_num = sd_num,\n  size_train = rep(10000, 16),\n  size_valid = rep(10000, 16),\n  size_calib = rep(10000, 16),\n  size_test = rep(10000, 16),\n  transform_probs = c(rep(FALSE, 4), rep(TRUE, 4), rep(FALSE, 4), rep(FALSE, 4)),\n  linear_predictor = c(rep(TRUE, 12), rep(FALSE, 4)),\n  seed = 202105\n)\nrm(coefficients, mean_num, sd_num)",
    "crumbs": [
      "III. Simulated Data",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Simulated Data</span>"
    ]
  },
  {
    "objectID": "simul-data.html#example",
    "href": "simul-data.html#example",
    "title": "4  Simulated Data",
    "section": "4.3 Example",
    "text": "4.3 Example\nLet us draw a sample of 10,000 observations in each set (train, calibration, validation, test), for each scenario. We can then plot the histogram of the true probabilities in each sample (Figure 4.1).\n\n\nCode\npar(mfrow = c(16, 4), mar = c(2, 2, 2, 2))\nfor (scenario in 1:16) {\n  simu_data &lt;- simulate_data_wrapper(\n    scenario = scenario,\n    params_df = params_df,\n    repn = repns_vector[1] # only one replication here\n  )\n  colour_samples &lt;- c(\n    \"Train\" = \"#0072B2\",\n    \"Calib\" = \"#CC79A7\",\n    \"Valid\" = \"#009E73\",\n    \"Test\" = \"#D55E00\"\n  )\n  for (sample in c(\"train\", \"valid\", \"calib\", \"test\")) {\n    if (sample == \"train\") {\n      true_prob &lt;- simu_data$data$probs_train\n      i_scores &lt;- 1\n    } else if (sample == \"valid\") {\n      true_prob &lt;- simu_data$data$probs_valid\n      i_scores &lt;- 2\n    } else if (sample == \"calib\") {\n      true_prob &lt;- simu_data$data$probs_calib\n      i_scores &lt;- 3\n    } else {\n      true_prob &lt;- simu_data$data$probs_test\n      i_scores &lt;- 4\n    }\n    hist(\n      true_prob,\n      breaks = seq(0, 1, by = .05),\n      col = colour_samples[i_scores],\n      border = \"white\",\n      xlab = \"p\", ylab = \"\",\n      main = str_c(\n        \"Scen. \", scenario, \", \", c(\"Train\", \"Valid\", \"Calib\", \"Test\")[i_scores]\n      ),\n      xlim = c(0, 1)\n    )\n  }\n}\n\n\n\n\n\nFigure 4.1: Histogram of true probabilities for each scenario\n\n\n\n\n\n\n\n\nFor each group of scenarios, the only thing that varies is the number of noise variables. This has no impact on the distribution of the true probability. Hence, we can create a simple figure with the distribution of the true probability for each group of scenario.\n\n\nOnly on Train test, for each category of scenarios.\nsave_graph &lt;- FALSE\n\nif (save_graph) {\n  cairo_pdf(\n    \"../figs/sim-data-hist-categories.pdf\", \n    width = 8, height = 2\n  )\n}\n\npar(mfrow = c(1, 4), mar = c(4.1, 3.1, 2.1, 1.1))\nfor (i_dgp in 1:4) {\n  scenario &lt;- c(1, 5, 9, 13)[i_dgp]\n  simu_data &lt;- simulate_data_wrapper(\n    scenario = scenario,\n    params_df = params_df,\n    repn = repns_vector[1] # only one replication here\n  )\n  \n  true_prob &lt;- simu_data$data$probs_test\n  title &lt;- str_c(\"DGP \", i_dgp)\n  \n  hist(\n    true_prob,\n    breaks = seq(0, 1, by = .05),\n    # col = ,\n    # border = \"white\",\n    xlab = \"p\", ylab = \"\",\n    main = title,\n    xlim = c(0, 1)\n  )\n}\n\n\nWarning in ks.test.default(probs_01, fun): ties should not be present for the\none-sample Kolmogorov-Smirnov test\n\n\nOnly on Train test, for each category of scenarios.\nif (save_graph) dev.off()\n\n\n\n\n\nFigure 4.2: Distribution of the underlying probabilities in the different categories of scenarios.\n\n\n\n\n\n\n\n\n\n\n\n\nOjeda, Francisco M., Max L. Jansen, Alexandre Thiéry, Stefan Blankenberg, Christian Weimar, Matthias Schmid, and Andreas Ziegler. 2023. “Calibrating Machine Learning Approaches for Probability Estimation: A Comprehensive Comparison.” Statistics in Medicine 42 (29): 5451–78. https://doi.org/10.1002/sim.9921.",
    "crumbs": [
      "III. Simulated Data",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Simulated Data</span>"
    ]
  },
  {
    "objectID": "simul-xgb.html",
    "href": "simul-xgb.html",
    "title": "5  Extreme Gradient Boosting",
    "section": "",
    "text": "5.1 Data\nWe generate data using the first 12 scenarios from Ojeda et al. (2023) and an additional set of 4 scenarios in which the true probability does not depend on the predictors in a linear way (see Chapter 4).\nsource(\"../scripts/functions/simul-data.R\")\nlibrary(ks)\nsource(\"../scripts/functions/subsample_target_distribution.R\")\nWhen we simulate a dataset, we draw the following number of observations:\nnb_obs &lt;- 10000\nDefinition of the 16 scenarios\n# Coefficients beta\ncoefficients &lt;- list(\n  # First category (baseline, 2 covariates)\n  c(0.5, 1),  # scenario 1, 0 noise variable\n  c(0.5, 1),  # scenario 2, 10 noise variables\n  c(0.5, 1),  # scenario 3, 50 noise variables\n  c(0.5, 1),  # scenario 4, 100 noise variables\n  # Second category (same as baseline, with lower number of 1s)\n  c(0.5, 1),  # scenario 5, 0 noise variable\n  c(0.5, 1),  # scenario 6, 10 noise variables\n  c(0.5, 1),  # scenario 7, 50 noise variables\n  c(0.5, 1),  # scenario 8, 100 noise variables\n  # Third category (same as baseline but with 5 num. and 5 categ. covariates)\n  c(0.1, 0.2, 0.3, 0.4, 0.5, 0.01, 0.02, 0.03, 0.04, 0.05),\n  c(0.1, 0.2, 0.3, 0.4, 0.5, 0.01, 0.02, 0.03, 0.04, 0.05),\n  c(0.1, 0.2, 0.3, 0.4, 0.5, 0.01, 0.02, 0.03, 0.04, 0.05),\n  c(0.1, 0.2, 0.3, 0.4, 0.5, 0.01, 0.02, 0.03, 0.04, 0.05),\n  # Fourth category (nonlinear predictor, 3 covariates)\n  c(0.5, 1, .3),  # scenario 5, 0 noise variable\n  c(0.5, 1, .3),  # scenario 6, 10 noise variables\n  c(0.5, 1, .3),  # scenario 7, 50 noise variables\n  c(0.5, 1, .3)  # scenario 8, 100 noise variables\n)\n\n# Mean parameter for the normal distribution to draw from to draw num covariates\nmean_num &lt;- list(\n  # First category (baseline, 2 covariates)\n  rep(0, 2),  # scenario 1, 0 noise variable\n  rep(0, 2),  # scenario 2, 10 noise variables\n  rep(0, 2),  # scenario 3, 50 noise variables\n  rep(0, 2),  # scenario 4, 100 noise variables\n  # Second category (same as baseline, with lower number of 1s)\n  rep(0, 2),  # scenario 5, 0 noise variable\n  rep(0, 2),  # scenario 6, 10 noise variables\n  rep(0, 2),  # scenario 7, 50 noise variables\n  rep(0, 2),  # scenario 8, 100 noise variables\n  # Third category (same as baseline but with 5 num. and 5 categ. covariates)\n  rep(0, 5),\n  rep(0, 5),\n  rep(0, 5),\n  rep(0, 5),\n  # Fourth category (nonlinear predictor, 3 covariates)\n  rep(0, 3),\n  rep(0, 3),\n  rep(0, 3),\n  rep(0, 3)\n)\n# Sd parameter for the normal distribution to draw from to draw num covariates\nsd_num &lt;- list(\n  # First category (baseline, 2 covariates)\n  rep(1, 2),  # scenario 1, 0 noise variable\n  rep(1, 2),  # scenario 2, 10 noise variables\n  rep(1, 2),  # scenario 3, 50 noise variables\n  rep(1, 2),  # scenario 4, 100 noise variables\n  # Second category (same as baseline, with lower number of 1s)\n  rep(1, 2),  # scenario 5, 0 noise variable\n  rep(1, 2),  # scenario 6, 10 noise variables\n  rep(1, 2),  # scenario 7, 50 noise variables\n  rep(1, 2),  # scenario 8, 100 noise variables\n  # Third category (same as baseline but with 5 num. and 5 categ. covariates)\n  rep(1, 5),\n  rep(1, 5),\n  rep(1, 5),\n  rep(1, 5),\n  # Fourth category (nonlinear predictor, 3 covariates)\n  rep(1, 3),\n  rep(1, 3),\n  rep(1, 3),\n  rep(1, 3)\n)\n\nparams_df &lt;- tibble(\n  scenario = 1:16,\n  coefficients = coefficients,\n  n_num = c(rep(2, 8), rep(5, 4), rep(3, 4)),\n  add_categ = c(rep(FALSE, 8), rep(TRUE, 4), rep(FALSE, 4)),\n  n_noise = rep(c(0, 10, 50, 100), 4),\n  mean_num = mean_num,\n  sd_num = sd_num,\n  size_train = rep(nb_obs, 16),\n  size_valid = rep(nb_obs, 16),\n  size_calib = rep(nb_obs, 16),\n  size_test = rep(nb_obs, 16),\n  transform_probs = c(rep(FALSE, 4), rep(TRUE, 4), rep(FALSE, 4), rep(FALSE, 4)),\n  linear_predictor = c(rep(TRUE, 12), rep(FALSE, 4)),\n  seed = 202105\n)\nrm(coefficients, mean_num, sd_num)",
    "crumbs": [
      "III. Simulated Data",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Extreme Gradient Boosting</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Austin, Peter C., and Ewout W. Steyerberg. 2019. “The Integrated\nCalibration Index (ICI) and Related Metrics for Quantifying the\nCalibration of Logistic Regression Models.” Statistics in\nMedicine 38 (21): 4051–65. https://doi.org/10.1002/sim.8281.\n\n\nBrier, Glenn W. 1950. “Verification of Forecasts Expressed in\nTerms of Probability.” Monthly Weather Review 78 (1):\n1–3.\n\n\nCaffo, Brian S, James G Booth, and AC Davison. 2002. “Empirical\nSupremum Rejection Sampling.” Biometrika 89 (4): 745–54.\n\n\nChen, Song Xi. 1999. “Beta Kernel Estimators for Density\nFunctions.” Computational Statistics & Data Analysis\n31 (2): 131–45.\n\n\nKullback, S., and R. A. Leibler. 1951. “On Information and\nSufficiency.” The Annals of Mathematical Statistics 22\n(1): 79–86. https://doi.org/10.1214/aoms/1177729694.\n\n\nOjeda, Francisco M., Max L. Jansen, Alexandre Thiéry, Stefan\nBlankenberg, Christian Weimar, Matthias Schmid, and Andreas Ziegler.\n2023. “Calibrating Machine Learning Approaches for Probability\nEstimation: A Comprehensive Comparison.” Statistics in\nMedicine 42 (29): 5451–78. https://doi.org/10.1002/sim.9921.\n\n\nRumbell, Timothy, Jaimit Parikh, James Kozloski, and Viatcheslav Gurev.\n2023. “Novel and Flexible Parameter Estimation Methods for\nData-Consistent Inversion in Mechanistic Modelling.” Royal\nSociety Open Science 10 (11): 230668.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "simul-xgb.html#metrics",
    "href": "simul-xgb.html#metrics",
    "title": "5  Extreme Gradient Boosting",
    "section": "5.2 Metrics",
    "text": "5.2 Metrics\nWe load the functions from Chapter 3 to compute performance, calibration and divergence metrics.\n\nsource(\"../scripts/functions/metrics.R\")",
    "crumbs": [
      "III. Simulated Data",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Extreme Gradient Boosting</span>"
    ]
  },
  {
    "objectID": "simul-xgb.html#simulations-setup",
    "href": "simul-xgb.html#simulations-setup",
    "title": "5  Extreme Gradient Boosting",
    "section": "5.3 Simulations Setup",
    "text": "5.3 Simulations Setup\nTo train the models, we rely on the {xgboost} R package.\n\nlibrary(xgboost)\n\n\nAttaching package: 'xgboost'\n\n\nThe following object is masked from 'package:dplyr':\n\n    slice\n\n\nHere, we define a function to recalibrate predicted scores using either Platt scaling or isotonic regression. The recalibration algorithm is first trained on the calibration set and then applied to both the calibration and test sets.\n\n#' Recalibrates scores using a calibration\n#' \n#' @param obs_calib vector of observed events in the calibration set\n#' @param scores_calib vector of predicted probabilities in the calibration set\n#' @param obs_test vector of observed events in the test set\n#' @param scores_test vector of predicted probabilities in the test set\n#' @param method recalibration method (`\"platt\"` for Platt scaling, \n#'   `\"isotonic\"` for isotonic regression)\n#' @returns list of two elements: recalibrated scores on the calibration set,\n#'   recalibrated scores on the test set\nrecalibrate &lt;- function(obs_calib,\n                        obs_test,\n                        pred_calib,\n                        pred_test,\n                        method = c(\"platt\", \"isotonic\")) {\n  data_calib &lt;- tibble(d = obs_calib, scores = pred_calib)\n  data_test &lt;- tibble(d = obs_test, scores = pred_test)\n  \n  if (method == \"platt\") {\n    lr &lt;- glm(d ~ scores, family = binomial(link = 'logit'), data = data_calib)\n    score_c_calib &lt;- predict(lr, newdata = data_calib, type = \"response\")\n    score_c_test &lt;- predict(lr, newdata = data_test, type = \"response\")\n  } else if (method == \"isotonic\") {\n    iso &lt;- isoreg(x = data_calib$scores, y = data_calib$d)\n    fit_iso &lt;- as.stepfun(iso)\n    score_c_calib &lt;- fit_iso(data_calib$scores)\n    score_c_test &lt;- fit_iso(data_test$scores)\n    \n  } else {\n    stop(\"Unrecognized method: platt or isotonic only\")\n  }\n  # Format results in tibbles:\n  # For calibration set\n  tb_score_c_calib &lt;- tibble(\n    d = obs_calib,\n    p_u = pred_calib,\n    p_c = score_c_calib\n  )\n  # For test set\n  tb_score_c_test &lt;- tibble(\n    d = obs_test,\n    p_u = pred_test,\n    p_c = score_c_test\n  )\n  \n  list(\n    tb_score_c_calib = tb_score_c_calib,\n    tb_score_c_test = tb_score_c_test\n  )\n  \n}\n\nAs explained in the foreword of this page, we compute metrics based on scores obtained at various boosting iterations. To do so, we define a function, get_metrics_nb_iter(), that will be applied to a fitted model. This function will be called for all the boosting iterations (controlled by the nb_iter argument). The function returns a list with the following elements:\n\nscenario: the ID of the scenario\nind: the index of the grid search (so that we can join with the hyperparameters values, if needed)\nrepn: the ID of the replication\nnb_iter: the boosting iteration at which the metrics are computed\ntb_metrics: the tibble with the performance, calibration, and divergence metrics (one row for the train sample, one row for the calibration sample, one row for the validation sample, and one row for the test sample)\ntb_prop_scores: additional metrics (\\(\\mathbb{P}(q_1 &lt; \\hat{s}(\\mathbf{x}) &lt; q_2)\\) for multiple values for \\(q_1\\) and \\(q_2 = 1-q_1\\))\nscores_hist: elements to be able to plot an histogram of the scores on both the train set and the test set (using 20 equally-sized bins over \\([0,1]\\)).\n\n\n\nFunction get_metrics_nb_iter()\n#' Computes the performance and calibration metrics for an xgb model,\n#' depending on the number of iterations kept.\n#'\n#' @param nb_iter number of boosting iterations to keep\n#' @param params hyperparameters of the current model\n#' @param fitted_xgb xgb estimated model\n#' @param tb_train_xgb train data (in xgb.DMatrix format)\n#' @param tb_valid_xgb validation data (in xgb.DMatrix format)\n#' @param tb_calib_xgb calibration data (in xgb.DMatrix format)\n#' @param tb_test_xgb test data (in xgb.DMatrix format)\n#' @param simu_data simulated dataset\n#' @param true_prob list with true probabilities on train, calibration,\n#'  validation and test sets\nget_metrics_nb_iter &lt;- function(nb_iter,\n                                params,\n                                fitted_xgb,\n                                tb_train_xgb,\n                                tb_valid_xgb,\n                                tb_calib_xgb,\n                                tb_test_xgb,\n                                simu_data,\n                                true_prob) {\n\n  ind &lt;- params$ind\n  max_depth &lt;- params$max_depth\n  tb_train &lt;- simu_data$data$train |&gt; rename(d = y)\n  tb_valid &lt;- simu_data$data$valid |&gt; rename(d = y)\n  tb_calib &lt;- simu_data$data$calib |&gt; rename(d = y)\n  tb_test &lt;- simu_data$data$test |&gt; rename(d = y)\n\n  # Predicted scores\n  scores_train &lt;- predict(fitted_xgb, tb_train_xgb, iterationrange = c(1, nb_iter))\n  scores_valid &lt;- predict(fitted_xgb, tb_valid_xgb, iterationrange = c(1, nb_iter))\n  scores_calib &lt;- predict(fitted_xgb, tb_calib_xgb, iterationrange = c(1, nb_iter))\n  scores_test &lt;- predict(fitted_xgb, tb_test_xgb, iterationrange = c(1, nb_iter))\n  \n  # Recalibration\n  # Platt scaling\n  res_recalibration_platt &lt;- recalibrate(\n    obs_calib = tb_calib$d, \n    obs_test = tb_test$d, \n    pred_calib = scores_calib, \n    pred_test = scores_test, \n    method = \"platt\"\n  )\n  scores_c_platt_calib &lt;- res_recalibration_platt$tb_score_c_calib$p_c\n  scores_c_platt_test &lt;- res_recalibration_platt$tb_score_c_test$p_c\n  \n  # Isotonic regression\n  res_recalibration_iso &lt;- recalibrate(\n    obs_calib = tb_calib$d, \n    obs_test = tb_test$d, \n    pred_calib = scores_calib, \n    pred_test = scores_test, \n    method = \"isotonic\"\n  )\n  scores_c_iso_calib &lt;- res_recalibration_iso$tb_score_c_calib$p_c\n  scores_c_iso_test &lt;- res_recalibration_iso$tb_score_c_test$p_c\n\n  ## Histogram of scores----\n  breaks &lt;- seq(0, 1, by = .05)\n  scores_train_hist &lt;- hist(scores_train, breaks = breaks, plot = FALSE)\n  scores_calib_hist &lt;- hist(scores_calib, breaks = breaks, plot = FALSE)\n  scores_valid_hist &lt;- hist(scores_valid, breaks = breaks, plot = FALSE)\n  scores_test_hist &lt;- hist(scores_test, breaks = breaks, plot = FALSE)\n  scores_c_platt_calib_hist &lt;- hist(scores_c_platt_calib, breaks = breaks, plot = FALSE)\n  scores_c_platt_test_hist &lt;- hist(scores_c_platt_test, breaks = breaks, plot = FALSE)\n  scores_c_iso_calib_hist &lt;- hist(scores_c_iso_calib, breaks = breaks, plot = FALSE)\n  scores_c_iso_test_hist &lt;- hist(scores_c_iso_test, breaks = breaks, plot = FALSE)\n  \n  scores_hist &lt;- list(\n    train = scores_train_hist,\n    valid = scores_valid_hist,\n    calib = scores_calib_hist,\n    test = scores_test_hist,\n    calib_c_platt = scores_c_platt_calib_hist,\n    test_c_platt = scores_c_platt_test_hist,\n    calib_c_iso = scores_c_iso_calib_hist,\n    test_c_iso = scores_c_iso_test_hist,\n    scenario = simu_data$scenario,\n    ind = ind,\n    repn = simu_data$repn,\n    max_depth = params$max_depth,\n    nb_iter = nb_iter\n  )\n\n  ## Estimation of P(q1 &lt; score &lt; q2)----\n  prop_btw_q_h &lt;- function(s, sample_name, recalib_name) {\n    map(\n      c(.1, .2, .3, .4),\n      ~prop_btw_quantiles(s = s, q1 = .x)\n    ) |&gt;\n      list_rbind() |&gt;\n      mutate(sample = sample_name, recalib = recalib_name)\n  }\n  \n  proq_scores_train &lt;- prop_btw_q_h(\n    scores_train, sample_name = \"train\", recalib_name = \"none\"\n  )\n  proq_scores_valid &lt;- prop_btw_q_h(\n    scores_valid, sample_name = \"valid\", recalib_name = \"none\"\n  )\n  proq_scores_calib &lt;- prop_btw_q_h(\n    scores_calib, sample_name = \"calib\", recalib_name = \"none\"\n  )\n  proq_scores_test &lt;- prop_btw_q_h(\n    scores_test, sample_name = \"test\", recalib_name = \"none\"\n  )\n  proq_scores_c_platt_calib &lt;- prop_btw_q_h(\n    scores_c_platt_calib, sample_name = \"calib\", recalib_name = \"platt\"\n  )\n  proq_scores_c_platt_test &lt;- prop_btw_q_h(\n    scores_c_platt_test, sample_name = \"test\", recalib_name = \"platt\"\n  )\n  proq_scores_c_iso_calib &lt;- prop_btw_q_h(\n    scores_c_iso_calib, sample_name = \"calib\", recalib_name = \"isotonic\"\n  )\n  proq_scores_c_iso_test &lt;- prop_btw_q_h(\n    scores_c_iso_test, sample_name = \"test\", recalib_name = \"isotonic\"\n  )\n  \n\n  ## Dispersion Metrics----\n  disp_train &lt;- dispersion_metrics(\n    true_probas = true_prob$train, scores = scores_train\n  ) |&gt; \n    mutate(sample = \"train\", recalib = \"none\")\n  disp_valid &lt;- dispersion_metrics(\n    true_probas = true_prob$valid, scores = scores_valid\n  ) |&gt;\n    mutate(sample = \"valid\", recalib = \"none\")\n  \n  disp_calib &lt;- dispersion_metrics(\n    true_probas = true_prob$calib, scores = scores_calib\n  ) |&gt;\n    mutate(sample = \"calib\", recalib = \"none\")\n  \n  disp_test &lt;- dispersion_metrics(\n    true_probas = true_prob$test, scores = scores_test\n  ) |&gt; \n    mutate(sample = \"test\", recalib = \"none\")\n  \n  \n  disp_c_platt_calib &lt;- dispersion_metrics(\n    true_probas = true_prob$calib, scores = scores_c_platt_calib\n  ) |&gt;\n    mutate(sample = \"calib\", recalib = \"platt\")\n  \n  disp_c_platt_test &lt;- dispersion_metrics(\n    true_probas = true_prob$test, scores = scores_c_platt_test\n  ) |&gt; \n    mutate(sample = \"test\", recalib = \"platt\")\n  \n  disp_c_iso_calib &lt;- dispersion_metrics(\n    true_probas = true_prob$calib, scores = scores_c_iso_calib\n  ) |&gt;\n    mutate(sample = \"calib\", recalib = \"isotonic\")\n  \n  disp_c_iso_test &lt;- dispersion_metrics(\n    true_probas = true_prob$test, scores = scores_c_iso_test\n  ) |&gt; \n    mutate(sample = \"test\", recalib = \"isotonic\")\n  \n  # Performance and Calibration Metrics\n  # We add very small noise to predicted scores\n  # otherwise the local regression may crash\n  scores_train_noise &lt;- scores_train +\n    runif(n = length(scores_train), min = 0, max = 0.01)\n  scores_train_noise[scores_train_noise &gt; 1] &lt;- 1\n  metrics_train &lt;- compute_metrics(\n    obs = tb_train$d, scores = scores_train_noise, true_probas = true_prob$train\n  ) |&gt; mutate(sample = \"train\", recalib = \"none\")\n  \n  scores_valid_noise &lt;- scores_valid +\n    runif(n = length(scores_valid), min = 0, max = 0.01)\n  scores_valid_noise[scores_valid_noise &gt; 1] &lt;- 1\n  metrics_valid &lt;- compute_metrics(\n    obs = tb_valid$d, scores = scores_valid_noise, true_probas = true_prob$valid\n  ) |&gt; mutate(sample = \"valid\", recalib = \"none\")\n  \n  scores_calib_noise &lt;- scores_calib +\n    runif(n = length(scores_calib), min = 0, max = 0.01)\n  scores_calib_noise[scores_calib_noise &gt; 1] &lt;- 1\n  metrics_calib &lt;- compute_metrics(\n    obs = tb_calib$d, scores = scores_calib_noise, true_probas = true_prob$calib\n  ) |&gt; mutate(sample = \"calib\", recalib = \"none\")\n  \n  scores_test_noise &lt;- scores_test +\n    runif(n = length(scores_test), min = 0, max = 0.01)\n  scores_test_noise[scores_test_noise &gt; 1] &lt;- 1\n  metrics_test &lt;- compute_metrics(\n    obs = tb_test$d, scores = scores_test_noise, true_probas = true_prob$test\n  ) |&gt; mutate(sample = \"test\", recalib = \"none\")\n  \n  # With recalibrated scores (platt)\n  scores_c_platt_calib_noise &lt;- scores_c_platt_calib +\n    runif(n = length(scores_c_platt_calib), min = 0, max = 0.01)\n  scores_c_platt_calib_noise[scores_c_platt_calib_noise &gt; 1] &lt;- 1\n  metrics_c_platt_calib &lt;- compute_metrics(\n    obs = tb_calib$d, scores = scores_c_platt_calib_noise, \n    true_probas = true_prob$calib\n  ) |&gt; mutate(sample = \"calib\", recalib = \"platt\")\n  \n  scores_c_platt_test_noise &lt;- scores_c_platt_test +\n    runif(n = length(scores_c_platt_test), min = 0, max = 0.01)\n  scores_c_platt_test_noise[scores_c_platt_test_noise &gt; 1] &lt;- 1\n  metrics_c_platt_test &lt;- compute_metrics(\n    obs = tb_test$d, scores = scores_c_platt_test_noise, \n    true_probas = true_prob$test\n  ) |&gt; mutate(sample = \"test\", recalib = \"platt\")\n  \n  # With recalibrated scores (isotonic)\n  scores_c_iso_calib_noise &lt;- scores_c_iso_calib +\n    runif(n = length(scores_c_iso_calib), min = 0, max = 0.01)\n  scores_c_iso_calib_noise[scores_c_iso_calib_noise &gt; 1] &lt;- 1\n  metrics_c_iso_calib &lt;- compute_metrics(\n    obs = tb_calib$d, scores = scores_c_iso_calib_noise, \n    true_probas = true_prob$calib\n  ) |&gt; mutate(sample = \"calib\", recalib = \"isotonic\")\n  \n  scores_c_iso_test_noise &lt;- scores_c_iso_test +\n    runif(n = length(scores_c_iso_test), min = 0, max = 0.01)\n  scores_c_iso_test_noise[scores_c_iso_test_noise &gt; 1] &lt;- 1\n  metrics_c_iso_test &lt;- compute_metrics(\n    obs = tb_test$d, scores = scores_c_iso_test_noise, \n    true_probas = true_prob$test\n  ) |&gt; mutate(sample = \"test\", recalib = \"isotonic\")\n  \n  tb_metrics &lt;- metrics_train |&gt;\n    bind_rows(metrics_valid) |&gt;\n    bind_rows(metrics_calib) |&gt;\n    bind_rows(metrics_test) |&gt;\n    bind_rows(metrics_c_platt_calib) |&gt;\n    bind_rows(metrics_c_platt_test) |&gt;\n    bind_rows(metrics_c_iso_calib) |&gt;\n    bind_rows(metrics_c_iso_test) |&gt;\n    left_join(\n      disp_train |&gt;\n        bind_rows(disp_valid) |&gt; \n        bind_rows(disp_calib) |&gt; \n        bind_rows(disp_test) |&gt; \n        bind_rows(disp_c_platt_calib) |&gt; \n        bind_rows(disp_c_platt_test) |&gt; \n        bind_rows(disp_c_iso_calib) |&gt; \n        bind_rows(disp_c_iso_test),\n      by = c(\"sample\", \"recalib\")\n    ) |&gt;\n    mutate(\n      scenario = simu_data$scenario,\n      ind = ind,\n      repn = simu_data$repn,\n      max_depth = params$max_depth,\n      nb_iter = nb_iter\n    )\n  \n  tb_prop_scores &lt;- proq_scores_train |&gt;\n    bind_rows(proq_scores_valid) |&gt;\n    bind_rows(proq_scores_calib) |&gt;\n    bind_rows(proq_scores_test) |&gt;\n    bind_rows(proq_scores_c_platt_calib) |&gt;\n    bind_rows(proq_scores_c_platt_test) |&gt;\n    bind_rows(proq_scores_c_iso_calib) |&gt;\n    bind_rows(proq_scores_c_iso_test) |&gt;\n    mutate(\n      scenario = simu_data$scenario,\n      ind = ind,\n      repn = simu_data$repn,\n      max_depth = params$max_depth,\n      nb_iter = nb_iter\n    )\n\n  list(\n    scenario = simu_data$scenario,     # data scenario\n    ind = ind,                         # index for grid\n    repn = simu_data$repn,             # data replication ID\n    nb_iter = nb_iter,                 # number of boosting iterations\n    tb_metrics = tb_metrics,           # table with performance/calib/divergence\n                                       #  metrics\n    tb_prop_scores = tb_prop_scores,   # table with P(q1 &lt; score &lt; q2)\n    scores_hist = scores_hist          # histogram of scores\n  )\n}\n\n\nWe define another function, simul_xgb() which trains an extreme gradient boosting model for a single replication. It calls the get_metrics_nb_iter() on each of the boosting iterations of the model from the second to the last (400th), and returns a list of length 400-1 where each element is a list returned by the get_metrics_nb_iter().\n\n\nFunction simul_xgb()\n#' Train an xgboost model and compute performance, calibration, and dispersion\n#' metrics\n#'\n#' @param params tibble with hyperparameters for the simulation\n#' @param ind index of the grid (numerical ID)\n#' @param simu_data simulated data obtained with `simulate_data_wrapper()`\nsimul_xgb &lt;- function(params,\n                      ind,\n                      simu_data) {\n  tb_train &lt;- simu_data$data$train |&gt; rename(d = y)\n  tb_valid &lt;- simu_data$data$valid |&gt; rename(d = y)\n  tb_calib &lt;- simu_data$data$calib |&gt; rename(d = y)\n  tb_test &lt;- simu_data$data$test |&gt; rename(d = y)\n  true_prob &lt;-\n    list(\n      train = simu_data$data$probs_train,\n      valid = simu_data$data$probs_valid,\n      calib = simu_data$data$probs_calib,\n      test = simu_data$data$probs_test\n    )\n\n  ## Format data for xgboost----\n  tb_train_xgb &lt;- xgb.DMatrix(\n    data = model.matrix(d ~ -1 + ., tb_train), label = tb_train$d\n  )\n  tb_valid_xgb &lt;- xgb.DMatrix(\n    data = model.matrix(d ~ -1 + ., tb_valid), label = tb_valid$d\n  )\n  tb_calib_xgb &lt;- xgb.DMatrix(\n    data = model.matrix(d ~ -1 + ., tb_calib), label = tb_calib$d\n  )\n  tb_test_xgb &lt;- xgb.DMatrix(\n    data = model.matrix(d ~ -1 + ., tb_test), label = tb_test$d\n  )\n  # Parameters for the algorithm\n  param &lt;- list(\n    max_depth = params$max_depth, #Note: root node is indexed 0\n    eta = params$eta,\n    nthread = 1,\n    objective = \"binary:logistic\",\n    eval_metric = \"auc\"\n  )\n  watchlist &lt;- list(train = tb_train_xgb, eval = tb_valid_xgb)\n\n  ## Estimation----\n  xgb_fit &lt;- xgb.train(\n    param, tb_train_xgb,\n    nrounds = params$nb_iter_total,\n    watchlist,\n    verbose = 0\n  )\n\n  # Then, for each boosting iteration number up to params$nb_iter_total\n  # compute the predicted scores and evaluate the metrics\n  resul &lt;- map(\n    seq(2, params$nb_iter_total),\n    ~get_metrics_nb_iter(\n      nb_iter = .x,\n      params = params,\n      fitted_xgb = xgb_fit,\n      tb_train_xgb = tb_train_xgb,\n      tb_valid_xgb = tb_valid_xgb,\n      tb_calib_xgb = tb_calib_xgb,\n      tb_test_xgb = tb_test_xgb,\n      simu_data = simu_data,\n      true_prob = true_prob\n    ),\n  )\n  resul\n}\n\nsimulate_xgb_scenario &lt;- function(scenario, params_df, repn) {\n  # Generate Data\n  simu_data &lt;- simulate_data_wrapper(\n    scenario = scenario,\n    params_df = params_df,\n    repn = repn\n  )\n\n  # Looping over the grid hyperparameters for the scenario\n  res_simul &lt;- vector(mode = \"list\", length = nrow(grid))\n  cli::cli_progress_bar(\"Iteration grid\", total = nrow(grid), type = \"tasks\")\n  for (j in 1:nrow(grid)) {\n    curent_params &lt;- grid |&gt; dplyr::slice(!!j)\n    res_simul[[j]] &lt;- simul_xgb(\n      params = curent_params,\n      ind = curent_params$ind,\n      simu_data = simu_data\n    )\n    cli::cli_progress_update()\n  }\n\n\n  # The metrics computed for all set of hyperparameters (identified with `ind`)\n  # and for each number of boosting iterations (`nb_iter`), for the current\n  # scenario (`scenario`) and current replication number (`repn`)\n  metrics_simul &lt;- map(\n    res_simul,\n    function(simul_grid_j) map(simul_grid_j, \"tb_metrics\") |&gt; list_rbind()\n  ) |&gt;\n    list_rbind()\n\n  # P(q_1&lt;s(x)&lt;q_2)\n  prop_scores_simul &lt;- map(\n    res_simul,\n    function(simul_grid_j) map(simul_grid_j, \"tb_prop_scores\") |&gt; list_rbind()\n  ) |&gt;\n    list_rbind()\n\n  # Histogram of estimated scores\n  scores_hist &lt;- map(\n    res_simul,\n    function(simul_grid_j) map(simul_grid_j, \"scores_hist\")\n  )\n\n  list(\n    metrics_simul = metrics_simul,\n    scores_hist = scores_hist,\n    prop_scores_simul = prop_scores_simul\n  )\n}\n\n\n\n5.3.1 Grid\nWe consider the following grid:\n\ngrid &lt;- expand_grid(\n  # max_depth = c(2, 4, 6),\n  max_depth = c(2,4,6),\n  nb_iter_total = 400,\n  eta = 0.3\n) |&gt;\n  mutate(ind = row_number())\n\nThe desired number of replications for each scenario needs to be set:\n\nrepns_vector &lt;- 1:100\n\nThe different configurations are reported in Table 5.1.\n\nDT::datatable(grid)\n\n\n\nTable 5.1: Grid Search Values\n\n\n\n\n\n\n\n\n\n\nWe define a function, simulate_xgb_scenario() to train the model on a dataset for all different values of the hyperparameters of the grid. This function performs a single replication of the simulations for a single scenario.\n\n\nFunction simulate_xgb_scenario()\nsimulate_xgb_scenario &lt;- function(scenario, params_df, repn) {\n  # Generate Data\n  simu_data &lt;- simulate_data_wrapper(\n    scenario = scenario,\n    params_df = params_df,\n    repn = repn\n  )\n\n  # Looping over the grid hyperparameters for the scenario\n  res_simul &lt;- vector(mode = \"list\", length = nrow(grid))\n  cli::cli_progress_bar(\"Iteration grid\", total = nrow(grid), type = \"tasks\")\n  for (j in 1:nrow(grid)) {\n    curent_params &lt;- grid |&gt; dplyr::slice(!!j)\n    res_simul[[j]] &lt;- simul_xgb(\n      params = curent_params,\n      ind = curent_params$ind,\n      simu_data = simu_data\n    )\n    cli::cli_progress_update()\n  }\n\n\n  # The metrics computed for all set of hyperparameters (identified with `ind`)\n  # and for each number of boosting iterations (`nb_iter`), for the current\n  # scenario (`scenario`) and current replication number (`repn`)\n  metrics_simul &lt;- map(\n    res_simul,\n    function(simul_grid_j) map(simul_grid_j, \"tb_metrics\") |&gt; list_rbind()\n  ) |&gt;\n    list_rbind()\n\n  # Sanity check\n  # metrics_simul |&gt; count(scenario, repn, ind, sample, nb_iter) |&gt;\n  #   filter(n &gt; 1)\n\n  # P(q_1&lt;s(x)&lt;q_2)\n  prop_scores_simul &lt;- map(\n    res_simul,\n    function(simul_grid_j) map(simul_grid_j, \"tb_prop_scores\") |&gt; list_rbind()\n  ) |&gt;\n    list_rbind()\n\n  # Sanity check\n  # prop_scores_simul |&gt; count(scenario, repn, ind, sample, nb_iter)\n\n  # Histogram of estimated scores\n  scores_hist &lt;- map(\n    res_simul,\n    function(simul_grid_j) map(simul_grid_j, \"scores_hist\")\n  )\n\n  list(\n    metrics_simul = metrics_simul,\n    scores_hist = scores_hist,\n    prop_scores_simul = prop_scores_simul\n  )\n}",
    "crumbs": [
      "III. Simulated Data",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Extreme Gradient Boosting</span>"
    ]
  },
  {
    "objectID": "simul-xgb.html#estimations",
    "href": "simul-xgb.html#estimations",
    "title": "5  Extreme Gradient Boosting",
    "section": "5.4 Estimations",
    "text": "5.4 Estimations\nWe loop over the 16 scenarios and run the 100 replications in parallel.\n\n\nEstimation codes\nlibrary(pbapply)\nlibrary(parallel)\nncl &lt;- detectCores()-1\n(cl &lt;- makeCluster(ncl))\n\nclusterEvalQ(cl, {\n  library(tidyverse)\n  library(locfit)\n  library(philentropy)\n  library(xgboost)\n  library(ks)\n}) |&gt;\n  invisible()\n\nclusterExport(\n  cl, c(\n    # Functions\n    \"brier_score\",\n    \"compute_metrics\",\n    \"dispersion_metrics\",\n    \"prop_btw_quantiles\",\n    \"subset_target\",\n    \"simulate_data\",\n    \"simulate_data_wrapper\",\n    \"simul_xgb\",\n    \"simulate_xgb_scenario\",\n    \"get_metrics_nb_iter\",\n    \"recalibrate\",\n    # Objects\n    \"grid\",\n    \"params_df\",\n    \"repns_vector\"\n  )\n)\n\n# make directory if not existing\nif (!dir.exists(\"../output/simul/\")) {\n  dir.create(\"../output/simul/\", recursive = TRUE)\n}\n\nfor (i_scenario in 1:16) {\n  scenario &lt;- i_scenario\n  print(str_c(\"Scenario \", scenario, \"/\", nrow(params_df)))\n  clusterExport(cl, c(\"scenario\"))\n  resul_xgb_scenario &lt;-\n    pblapply(\n      1:length(repns_vector), function(i) simulate_xgb_scenario(\n        scenario = scenario, params_df = params_df, repn = repns_vector[i]\n      ),\n      cl = cl\n    )\n  save(\n    resul_xgb_scenario,\n    file = str_c(\"../output/simul/resul_xgb_scenario_\", scenario, \".rda\")\n  )\n}\nstopCluster(cl)\n\n\nThe results can be loaded as follows:\n\nscenarios &lt;- 1:16 \nfiles &lt;- str_c(\n  \"../output/simul/resul_xgb_scenario_\", scenarios, \".rda\"\n)\nresul_xgb &lt;- map(files[file.exists(files)], ~{load(.x) ; resul_xgb_scenario})\n\nThe resul_rf object is of length 16: each element contains the simulations for a scenario. For each scenario, the elements are a list of length max(repns_vector), i.e., the number of replications. Each replication gives, in a list, the following elements:\n\nmetrics_simul: the metrics (AUC, Calibration, KL Divergence, etc.) for each model from the grid search, for all boosting iterations\nscores_hist: the counts on bins defined on estimated scores (on train, validation, calibration, and test sets ; for calibration and test sets, the counts are given with or without recalibration)\nprop_scores_simul: the estimations of \\(\\mathbb{P}(q_1 &lt; \\hat{\\mathbf{x}}&lt; q_2)\\) for various values of q_1 and q_2.",
    "crumbs": [
      "III. Simulated Data",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Extreme Gradient Boosting</span>"
    ]
  },
  {
    "objectID": "simul-xgb.html#results",
    "href": "simul-xgb.html#results",
    "title": "5  Extreme Gradient Boosting",
    "section": "5.5 Results",
    "text": "5.5 Results\nWe can now extract some information from the results.\nWe first aggregate all the computed metrics performance/calibration/divergence in a single tibble, metrics_xgb_all.\n\n\nCodes to create the metrics table\nmetrics_xgb_all &lt;- map(\n  resul_xgb,\n  function(resul_xgb_sc) map(resul_xgb_sc, \"metrics_simul\") |&gt; list_rbind()\n) |&gt;\n  list_rbind() |&gt;\n  mutate(\n    sample = factor(\n      sample,\n      levels = c(\"train\", \"valid\", \"calib\", \"test\"),\n      labels = c(\"Train\",\"Validation\", \"Calibration\" ,\"Test\")\n    ),\n    recalib = factor(\n      recalib,\n      levels = c(\"none\", \"platt\", \"isotonic\"),\n      labels = c(\"None\", \"Platt\", \"Isotonic\")\n    )\n  )\n\n# Sanity check\n# metrics_xgb_all |&gt; count(scenario, ind, sample, nb_iter) |&gt;\n#   filter(n != max(repns_vector))\n\n\nFor each replication, we made some hyperparameters vary. Let us identify some models of interest:\n\nsmallest: model with the lowest number of boosting iteration\nlargest: model with the highest number of boosting iteration\nlargest_auc: model with the highest AUC on validation set\nlowest_mse: model with the lowest MSE on validation set\nlowest_ici: model with the lowest ICI on validation set\nlowest_kl: model with the lowest KL Divergence on validation set\n\n\n\nCode\n# Identify the smallest tree on the validation set, when the scores are not\n# recalibrated\nsmallest_xgb &lt;-\n  metrics_xgb_all |&gt;\n  filter(sample == \"Validation\", recalib == \"None\") |&gt;\n  group_by(scenario, repn) |&gt;\n  arrange(nb_iter) |&gt;\n  slice_head(n = 1) |&gt;\n  select(scenario, repn, ind, nb_iter, recalib) |&gt;\n  mutate(result_type = \"smallest\") |&gt;\n  ungroup()\n\n# Identify the largest tree\nlargest_xgb &lt;-\n  metrics_xgb_all |&gt;\n  filter(sample == \"Validation\", recalib == \"None\") |&gt;\n  group_by(scenario, repn) |&gt;\n  arrange(desc(nb_iter)) |&gt;\n  slice_head(n = 1) |&gt;\n  select(scenario, repn, ind, nb_iter, recalib) |&gt;\n  mutate(result_type = \"largest\") |&gt;\n  ungroup()\n\n# Identify tree with highest AUC on test set\nhighest_auc_xgb &lt;-\n  metrics_xgb_all |&gt;\n  filter(sample == \"Validation\", recalib == \"None\") |&gt;\n  group_by(scenario, repn) |&gt;\n  arrange(desc(AUC)) |&gt;\n  slice_head(n = 1) |&gt;\n  select(scenario, repn, ind, nb_iter, recalib) |&gt;\n  mutate(result_type = \"largest_auc\") |&gt;\n  ungroup()\n\n# Identify tree with lowest MSE\nlowest_mse_xgb &lt;-\n  metrics_xgb_all |&gt;\n  filter(sample == \"Validation\", recalib == \"None\") |&gt;\n  group_by(scenario, repn) |&gt;\n  arrange(mse) |&gt;\n  slice_head(n = 1) |&gt;\n  select(scenario, repn, ind, nb_iter, recalib) |&gt;\n  mutate(result_type = \"lowest_mse\") |&gt;\n  ungroup()\n\n# Identify tree with lowest brier\nlowest_brier_xgb &lt;-\n  metrics_xgb_all |&gt;\n  filter(sample == \"Validation\", recalib == \"None\") |&gt;\n  group_by(scenario, repn) |&gt;\n  arrange(brier) |&gt;\n  slice_head(n = 1) |&gt;\n  select(scenario, repn, ind, nb_iter, recalib) |&gt;\n  mutate(result_type = \"lowest_brier\") |&gt;\n  ungroup()\n\n# Identify tree with lowest ICI\nlowest_ici_xgb &lt;-\n  metrics_xgb_all |&gt;\n  filter(sample == \"Validation\", recalib == \"None\") |&gt;\n  group_by(scenario, repn) |&gt;\n  arrange(ici) |&gt;\n  slice_head(n = 1) |&gt;\n  select(scenario, repn, ind, nb_iter, recalib) |&gt;\n  mutate(result_type = \"lowest_ici\") |&gt;\n  ungroup()\n\n# Identify tree with lowest KL\nlowest_kl_xgb &lt;-\n  metrics_xgb_all |&gt;\n  filter(sample == \"Validation\", recalib == \"None\") |&gt;\n  group_by(scenario, repn) |&gt;\n  arrange(KL_20_true_probas) |&gt;\n  slice_head(n = 1) |&gt;\n  select(scenario, repn, ind, nb_iter, recalib) |&gt;\n  mutate(result_type = \"lowest_kl\") |&gt;\n  ungroup()\n\n# Merge these\nmodels_of_interest_xgb &lt;-\n  smallest_xgb |&gt;\n  bind_rows(largest_xgb) |&gt;\n  bind_rows(highest_auc_xgb) |&gt;\n  bind_rows(lowest_mse_xgb) |&gt;\n  bind_rows(lowest_brier_xgb) |&gt;\n  bind_rows(lowest_ici_xgb) |&gt;\n  bind_rows(lowest_kl_xgb)\n\nmodels_of_interest_metrics &lt;- NULL\nfor (recalibration_method in c(\"None\", \"Platt\", \"Isotonic\")) {\n  # Add metrics now\n  models_of_interest_metrics &lt;-\n    models_of_interest_metrics |&gt;\n    bind_rows(\n      models_of_interest_xgb |&gt; select(-recalib) |&gt;\n        left_join(\n          metrics_xgb_all |&gt;\n            filter(\n              recalib == recalibration_method,\n              sample %in% c(\"Validation\", \"Test\")\n            ),\n          by = c(\"scenario\", \"repn\", \"ind\", \"nb_iter\"),\n          relationship = \"many-to-many\" # (calib, test)\n        )\n    )\n}\n\n\nmodels_of_interest_metrics &lt;-\n  models_of_interest_metrics |&gt;\n  mutate(\n    result_type = factor(\n      result_type,\n      levels = c(\n        \"smallest\", \"largest\", \"lowest_mse\", \"largest_auc\",\n        \"lowest_brier\", \"lowest_ici\", \"lowest_kl\"),\n      labels = c(\n        \"Smallest\", \"Largest\", \"MSE*\", \"AUC*\",\n        \"Brier*\", \"ICI*\", \"KL*\"\n      )\n    )\n  )\n\n# Sanity check\n# models_of_interest_metrics |&gt; count(scenario, sample, result_type)\n\n\n\n5.5.1 Metrics vs Number of Iterations\nWe define a function, plot_metrics() to plot selected metrics (AUC, ICI, and KL Divergence) as a function of the number of boosting iterations, for a given value for the hyperparameter max_depth. Each curve corresponds to a value of the maximal depth hyperparameter.\nTBD\n\n\n5.5.2 Distribution of Scores\nLet us extract all the histogram information computed over the simulations and put that in a single object, scores_hist_all.\n\nscores_hist_all &lt;-\n  map(\n    resul_xgb,\n    function(resul_xgb_sc) map(resul_xgb_sc, \"scores_hist\")\n  )\n\nWe then define a function, plot_bp_xgb() which plots the distribution of scores on the test set for a single replication (repn), for a scenario, (scenario). We also define a helper function, plot_bp_interest(), which plots the histogram of the scores at a specific iteration number. We will then be able to plot the distributions at the beginning of the boosting iterations, at the end, at a point where the AUC was the highest on the validation set, and at a point where the KL divergence between the distribution of scores on the validation set and the distribution of the true probabilities was the lowest. We will plot the distributions of the scores returned by the classifier, as well as those obtained with the reclibrators.\n\n\nFunction plot_metrics()\nplot_bp_interest &lt;- function(metrics_interest,\n                             scores_hist_interest,\n                             label,\n                             recalib_method) {\n  subtitle &lt;- str_c(\n    \"Depth = \", metrics_interest$max_depth, \", \",\n    \"MSE = \", round(metrics_interest$mse, 2), \", \",\n    \"AUC = \", round(metrics_interest$AUC, 2), \", \\n\",\n    \"Brier = \", round(metrics_interest$brier, 2), \",\",\n    \"ICI = \", round(metrics_interest$ici, 2), \", \",\n    \"KL = \", round(metrics_interest$KL_20_true_probas, 2)\n  )\n\n  if (recalib_method == \"none\") {\n    plot(\n      main = str_c(label, \" (iter = \", metrics_interest$nb_iter,\")\"),\n      scores_hist_interest$test,\n      xlab = latex2exp::TeX(\"$\\\\hat{s}(x)$\"),\n      ylab = \"\"\n    )\n  } else if (recalib_method == \"platt\") {\n    plot(\n      main = str_c(label, \" (iter = \", metrics_interest$nb_iter,\")\"),\n      scores_hist_interest$test_c_platt,\n      xlab = latex2exp::TeX(\"$\\\\hat{s}(x)$\"),\n      ylab = \"\",\n      col = colour_recalib[[\"Platt\"]]\n    )\n  } else if (recalib_method == \"iso\") {\n    plot(\n      main = str_c(label, \" (iter = \", metrics_interest$nb_iter,\")\"),\n      scores_hist_interest$test_c_iso,\n      xlab = latex2exp::TeX(\"$\\\\hat{s}(x)$\"),\n      ylab = \"\",\n      col = colour_recalib[[\"Isotonic\"]]\n    )\n  }\n  mtext(side = 3, line = -0.25, adj = .5, subtitle, cex = .5)\n}\n\nplot_bp_xgb &lt;- function(scenario,\n                        repn,\n                        paper_version = FALSE) {\n  # Focus on current scenario\n  scores_hist_scenario &lt;- scores_hist_all[[scenario]]\n  # Focus on a particular replication\n  scores_hist_repn &lt;- scores_hist_scenario[[repn]]\n  # # Focus on a value for max_depth\n  max_depth_val &lt;- map_dbl(scores_hist_repn, ~.x[[1]]$max_depth)\n  # i_max_depth &lt;- which(max_depth_val == max_depth)\n  # scores_hist &lt;- scores_hist_repn[[i_max_depth]]\n\n  # True Probabilities\n  simu_data &lt;- simulate_data_wrapper(\n    scenario = scenario,\n    params_df = params_df,\n    repn = repn # only one replication here\n  )\n  true_prob &lt;- simu_data$data$probs_train\n\n  for (recalib_method in c(\"none\", \"platt\", \"iso\")) {\n\n    i_method &lt;- match(recalib_method, c(\"none\", \"platt\", \"iso\"))\n    recalib_method_lab &lt;- c(\"None\", \"Platt\", \"Isotonic\")[i_method]\n\n    # The metrics for the corresponding simulations, on the validation set\n    metrics_xgb_current_valid &lt;-\n      metrics_xgb_all |&gt;\n      filter(\n        scenario == !!scenario,\n        repn == !!repn,\n        sample == \"Validation\",\n        recalib == \"None\"\n      )\n    # and on the test set\n    metrics_xgb_current_test &lt;-\n      metrics_xgb_all |&gt;\n      filter(\n        scenario == !!scenario,\n        repn == !!repn,\n        sample == \"Test\",\n        recalib == recalib_method_lab\n      )\n\n    if (paper_version == FALSE) {\n      hist(\n        true_prob,\n        breaks = seq(0, 1, by = .05),\n        xlab = \"p\", ylab = \"\",\n        main = \"True Probabilities\",\n        xlim = c(0, 1)\n      )\n      mtext(\n        side = 2, recalib_method_lab, line = 2.5, cex = 1,\n        font.lab = 2\n      )\n      # Iterations of interest----\n      ## Start of iterations\n      scores_hist_start &lt;- scores_hist_repn[[1]][[1]]\n      metrics_start &lt;- metrics_xgb_current_test |&gt;\n        filter(\n          nb_iter == scores_hist_start$nb_iter,\n          max_depth == scores_hist_start$max_depth\n        )\n\n      plot_bp_interest(\n        metrics_interest = metrics_start,\n        scores_hist_interest = scores_hist_start,\n        label = \"Start\",\n        recalib_method = recalib_method\n      )\n\n      ## End of iterations\n      scores_hist_end &lt;- scores_hist_repn[[1]][[length(scores_hist_repn[[1]])]]\n      metrics_end &lt;- metrics_xgb_current_test |&gt;\n        filter(\n          nb_iter == scores_hist_end$nb_iter,\n          max_depth == scores_hist_start$max_depth\n        )\n      plot_bp_interest(\n        metrics_interest = metrics_end,\n        scores_hist_interest = scores_hist_end,\n        label = \"End\",\n        recalib_method = recalib_method\n      )\n\n      ## Iteration with min MSE on validation set\n      metrics_valid_mse_star &lt;- metrics_xgb_current_valid |&gt; arrange(mse) |&gt;\n        dplyr::slice(1)\n      nb_iter_mse &lt;- metrics_valid_mse_star$nb_iter\n      max_depth_mse_star &lt;- metrics_valid_mse_star$max_depth\n      i_max_depth_mse_star &lt;- which(max_depth_val == max_depth_mse_star)\n      # Metrics at the same iteration on the test set\n      metrics_min_mse &lt;-\n        metrics_xgb_current_test |&gt;\n        filter(\n          nb_iter == !!nb_iter_mse,\n          max_depth == max_depth_mse_star\n        )\n      # Note: indexing at 0 here...\n      ind_mse &lt;- which(map_dbl(scores_hist_repn[[i_max_depth_mse_star]], \"nb_iter\") == nb_iter_mse)\n      scores_hist_min_mse &lt;- scores_hist_repn[[i_max_depth_mse_star]][[ind_mse]]\n      plot_bp_interest(\n        metrics_interest = metrics_min_mse,\n        scores_hist_interest = scores_hist_min_mse,\n        label = \"MSE*\",\n        recalib_method = recalib_method\n      )\n    }\n    ## Iteration with max AUC on validation set\n    metrics_valid_auc_star &lt;-\n      metrics_xgb_current_valid |&gt; arrange(desc(AUC)) |&gt; dplyr::slice(1)\n    nb_iter_auc &lt;- metrics_valid_auc_star$nb_iter\n    max_depth_auc_star &lt;- metrics_valid_auc_star$max_depth\n    i_max_depth_auc_star &lt;- which(max_depth_val == max_depth_auc_star)\n\n    metrics_max_auc &lt;-\n      metrics_xgb_current_test |&gt;\n      filter(nb_iter == !!nb_iter_auc, max_depth == max_depth_auc_star)\n    # Note: indexing at 0 here...\n    ind_auc &lt;- which(map_dbl(scores_hist_repn[[i_max_depth_auc_star]], \"nb_iter\") == nb_iter_auc)\n    scores_hist_max_auc &lt;- scores_hist_repn[[i_max_depth_auc_star]][[ind_auc]]\n    plot_bp_interest(\n      metrics_interest = metrics_max_auc,\n      scores_hist_interest = scores_hist_max_auc,\n      label = \"AUC*\",\n      recalib_method = recalib_method\n    )\n    if (paper_version == TRUE) {\n      mtext(\n        side = 2, recalib_method_lab, line = 2.5, cex = 1,\n        font.lab = 2\n      )\n    }\n\n    ## Min Brier on validation set\n    metrics_valid_brier_star &lt;-\n      metrics_xgb_current_valid |&gt; arrange(brier) |&gt; dplyr::slice(1)\n    nb_iter_brier &lt;- metrics_valid_brier_star$nb_iter\n    max_depth_brier_star &lt;- metrics_valid_brier_star$max_depth\n    i_max_depth_brier_star &lt;- which(max_depth_val == max_depth_brier_star)\n\n    metrics_min_brier &lt;-\n      metrics_xgb_current_test |&gt;\n      filter(nb_iter == !!nb_iter_brier, max_depth == max_depth_brier_star)\n    ind_brier &lt;- which(map_dbl(scores_hist_repn[[i_max_depth_brier_star]], \"nb_iter\") == nb_iter_brier)\n    scores_hist_min_brier &lt;- scores_hist_repn[[i_max_depth_brier_star]][[ind_brier]]\n    plot_bp_interest(\n      metrics_interest = metrics_min_brier,\n      scores_hist_interest = scores_hist_min_brier,\n      label = \"Brier*\",\n      recalib_method = recalib_method\n    )\n\n    ## Min ICI on validation set\n    metrics_valid_ici_star &lt;-\n      metrics_xgb_current_valid |&gt; arrange(ici) |&gt; dplyr::slice(1)\n    nb_iter_ici &lt;-   metrics_valid_ici_star$nb_iter\n    max_depth_ici_star &lt;- metrics_valid_ici_star$max_depth\n    i_max_depth_ici_star &lt;- which(max_depth_val == max_depth_ici_star)\n\n    metrics_min_ici &lt;-\n      metrics_xgb_current_test |&gt;\n      filter(nb_iter == !!nb_iter_ici, max_depth == max_depth_ici_star)\n    ind_ici &lt;- which(map_dbl(scores_hist_repn[[i_max_depth_ici_star]], \"nb_iter\") == nb_iter_ici)\n    scores_hist_min_ici &lt;- scores_hist_repn[[i_max_depth_ici_star]][[ind_ici]]\n    plot_bp_interest(\n      metrics_interest = metrics_min_ici,\n      scores_hist_interest = scores_hist_min_ici,\n      label = \"ICI*\",\n      recalib_method = recalib_method\n    )\n\n    ## Min KL on validation set\n    metrics_valid_kl_star &lt;-\n      metrics_xgb_current_valid |&gt; arrange(KL_20_true_probas) |&gt; dplyr::slice(1)\n    nb_iter_kl &lt;-   metrics_valid_kl_star$nb_iter\n    max_depth_kl_star &lt;- metrics_valid_kl_star$max_depth\n    i_max_depth_kl_star &lt;- which(max_depth_val == max_depth_kl_star)\n\n    metrics_min_kl &lt;-\n      metrics_xgb_current_test |&gt;\n      filter(nb_iter == !!nb_iter_kl, max_depth == max_depth_kl_star)\n    ind_kl &lt;- which(map_dbl(scores_hist_repn[[i_max_depth_kl_star]], \"nb_iter\") == nb_iter_kl)\n    scores_hist_min_kl &lt;- scores_hist_repn[[i_max_depth_kl_star]][[ind_kl]]\n    plot_bp_interest(\n      metrics_interest = metrics_min_kl,\n      scores_hist_interest = scores_hist_min_kl,\n      label = \"KL*\",\n      recalib_method = recalib_method\n    )\n  }\n}\n\n\n\nDGP 1DGP 2DGP 3DGP 4\n\n\n\n0 noise variable10 noise variables50 noise variables100 noise variables\n\n\n\n\nCode\npar(mfrow = c(3,8), mar = c(4.1, 4, 3.5, 1.5))\nplot_bp_xgb(scenario = 1, repn = 1)\n\n\n\n\n\nFigure 5.1: Distribution of scores on the test set (DGP 1, 0 noise variable)\n\n\n\n\n\n\n\n\n\n\n\n\nCode\npar(mfrow = c(3,8), mar = c(4.1, 4, 3.5, 1.5))\nplot_bp_xgb(scenario = 2, repn = 1)\n\n\n\n\n\nFigure 5.2: Distribution of scores on the test set (DGP 1, 10 noise variables)\n\n\n\n\n\n\n\n\n\n\n\n\nCode\npar(mfrow = c(3,8), mar = c(4.1, 4, 3.5, 1.5))\nplot_bp_xgb(scenario = 3, repn = 1)\n\n\n\n\n\nFigure 5.3: Distribution of scores on the test set (DGP 1, 50 noise variables)\n\n\n\n\n\n\n\n\n\n\n\n\nCode\npar(mfrow = c(3,8), mar = c(4.1, 4, 3.5, 1.5))\nplot_bp_xgb(scenario = 4, repn = 1)\n\n\n\n\n\nFigure 5.4: Distribution of scores on the test set (DGP 1, 100 noise variables)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0 noise variable10 noise variables50 noise variables100 noise variables\n\n\n\n\nCode\npar(mfrow = c(3,8), mar = c(4.1, 4, 3.5, 1.5))\nplot_bp_xgb(scenario = 5, repn = 1)\n\n\n\n\n\nFigure 5.5: Distribution of scores on the test set (DGP 2, 0 noise variable)\n\n\n\n\n\n\n\n\n\n\n\n\nCode\npar(mfrow = c(3,8), mar = c(4.1, 4, 3.5, 1.5))\nplot_bp_xgb(scenario = 6, repn = 1)\n\n\n\n\n\nFigure 5.6: Distribution of scores on the test set (DGP 2, 10 noise variables)\n\n\n\n\n\n\n\n\n\n\n\n\nCode\npar(mfrow = c(3,8), mar = c(4.1, 4, 3.5, 1.5))\nplot_bp_xgb(scenario = 7, repn = 1)\n\n\n\n\n\nFigure 5.7: Distribution of scores on the test set (DGP 2, 50 noise variables)\n\n\n\n\n\n\n\n\n\n\n\n\nCode\npar(mfrow = c(3,8), mar = c(4.1, 4, 3.5, 1.5))\nplot_bp_xgb(scenario = 8, repn = 1)\n\n\n\n\n\nFigure 5.8: Distribution of scores on the test set (DGP 2, 100 noise variables)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0 noise variable10 noise variables50 noise variables100 noise variables\n\n\n\n\nCode\npar(mfrow = c(3,8), mar = c(4.1, 4, 3.5, 1.5))\nplot_bp_xgb(scenario = 9, repn = 1)\n\n\n\n\n\nFigure 5.9: Distribution of scores on the test set (DGP 3, 0 noise variable)\n\n\n\n\n\n\n\n\n\n\n\n\nCode\npar(mfrow = c(3,8), mar = c(4.1, 4, 3.5, 1.5))\nplot_bp_xgb(scenario = 10, repn = 1)\n\n\n\n\n\nFigure 5.10: Distribution of scores on the test set (DGP 3, 10 noise variables)\n\n\n\n\n\n\n\n\n\n\n\n\nCode\npar(mfrow = c(3,8), mar = c(4.1, 4, 3.5, 1.5))\nplot_bp_xgb(scenario = 11, repn = 1)\n\n\n\n\n\nFigure 5.11: Distribution of scores on the test set (DGP 3, 50 noise variables)\n\n\n\n\n\n\n\n\n\n\n\n\nCode\npar(mfrow = c(3,8), mar = c(4.1, 4, 3.5, 1.5))\nplot_bp_xgb(scenario = 12, repn = 1)\n\n\n\n\n\nFigure 5.12: Distribution of scores on the test set (DGP 3, 100 noise variables)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0 noise variable10 noise variables50 noise variables100 noise variables\n\n\n\n\nCode\npar(mfrow = c(3,8), mar = c(4.1, 4, 3.5, 1.5))\nplot_bp_xgb(scenario = 13, repn = 1)\n\n\n\n\n\nFigure 5.13: Distribution of scores on the test set (DGP 4, 0 noise variable)\n\n\n\n\n\n\n\n\n\n\n\n\nCode\npar(mfrow = c(3,8), mar = c(4.1, 4, 3.5, 1.5))\nplot_bp_xgb(scenario = 14, repn = 1)\n\n\n\n\n\nFigure 5.14: Distribution of scores on the test set (DGP 4, 10 noise variables)\n\n\n\n\n\n\n\n\n\n\n\n\nCode\npar(mfrow = c(3,8), mar = c(4.1, 4, 3.5, 1.5))\nplot_bp_xgb(scenario = 15, repn = 1)\n\n\n\n\n\nFigure 5.15: Distribution of scores on the test set (DGP 4, 50 noise variables)\n\n\n\n\n\n\n\n\n\n\n\n\nCode\npar(mfrow = c(3,8), mar = c(4.1, 4, 3.5, 1.5))\nplot_bp_xgb(scenario = 16, repn = 1)\n\n\n\n\n\nFigure 5.16: Distribution of scores on the test set (DGP 4, 100 noise variables)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode to create PDF figures\nfor (scenario in 1:16) {\n  pdf(\n    file = str_c(\"../figs/bp_synthetic_xbg_\", scenario, \".pdf\"),\n    height = 4.5, width = 10\n  )\n  par(mfrow = c(3,4), mar = c(4.1, 4, 3.5, 1.5))\n  plot_bp_xgb(scenario = scenario, repn = 1, paper_version = TRUE)\n  dev.off()\n}\n\n\n\n\n5.5.3 KL Divergence and Calibration along Boosting Iterations\nWe can examine the evolution of the relationship between the divergence of score distributions from true probabilities and model calibration across increasing boosting iterations.\n\n\nCodes to create the figure\ndf_plot &lt;-\n  metrics_xgb_all |&gt;\n  mutate(\n    dgp = case_when(\n      scenario %in% 1:4 ~ 1,\n      scenario %in% 5:8 ~ 2,\n      scenario %in% 9:12 ~ 3,\n      scenario %in% 13:16 ~ 4\n    ),\n    dgp = factor(dgp, levels = 1:4, labels = str_c(\"DGP \", 1:4)),\n    no_noise = c(0, 10, 50, 100)[(scenario-1)%%4 + 1],\n    no_noise = factor(\n      no_noise, levels = c(no_noise),\n      labels = str_c(no_noise, \" noise variables\")\n    )\n  ) |&gt;\n  select(\n    dgp, no_noise, scenario, recalib, ind, sample, nb_iter, max_depth,\n    brier, ici, KL_20_true_probas\n  ) |&gt;\n  group_by(dgp, no_noise, scenario, recalib, ind, sample, nb_iter, max_depth) |&gt;\n  summarise(\n    brier = mean(brier),\n    ici = mean(ici),\n    KL_20_true_probas = mean(KL_20_true_probas),\n    .groups = \"drop\"\n  ) |&gt;\n  mutate(\n    max_depth = factor(\n      max_depth,\n      levels = c(2, 4, 6)\n    )\n  )\n\nformatter1000 &lt;- function(x) x*1000\n\n\n\nBrierICI\n\n\n\n\nCodes to create the figure\np_brier &lt;- ggplot(\n  data = df_plot |&gt; arrange(nb_iter) |&gt; filter(max_depth == 2),\n  mapping = aes(x = brier, y = KL_20_true_probas)\n) +\n  geom_path(\n    mapping = aes(colour = sample, linetype = recalib),\n    arrow = arrow(type = \"closed\", ends = \"last\",\n                  length = unit(0.08, \"inches\"))\n  ) +\n  # facet_wrap(~scenario) +\n  ggh4x::facet_grid2(dgp~no_noise, scales = \"free_y\", independent = \"y\") +\n  labs(\n    x = latex2exp::TeX(\"Calibration (Brier), $\\\\times 10^{3}$, log scale\"),\n    y = \"KL Divergence\"\n  ) +\n  scale_x_log10(labels = formatter1000) + scale_y_log10() +\n  scale_colour_manual(\"Sample\", values = colour_samples) +\n  scale_linetype_discrete(\"Recalibration\") +\n  theme_paper() +\n  theme(legend.key.width = unit(1.5, \"cm\"))\n\n\nggsave(\n  p_brier, file = \"../figs/xgb-kl-calib-brier-leaves-all.pdf\",\n       width = 10, height = 8\n)\n\np_brier\n\n\n\n\n\nFigure 5.17: KL Divergence and Calibration (Brier) across increasing boosting iterations (log scales)\n\n\n\n\n\n\n\n\n\n\n\n\nCodes to create the figure\np_ici &lt;- ggplot(\n  data = df_plot |&gt; arrange(nb_iter) |&gt; filter(max_depth == 2),\n  mapping = aes(x = ici, y = KL_20_true_probas)\n) +\n  geom_path(\n    mapping = aes(colour = sample, linetype = recalib),\n    arrow = arrow(type = \"closed\", ends = \"last\",\n                  length = unit(0.08, \"inches\"))\n  ) +\n  # facet_wrap(~scenario) +\n  ggh4x::facet_grid2(dgp~no_noise, scales = \"free_y\", independent = \"y\") +\n  labs(\n    x = latex2exp::TeX(\"Calibration (ICI), $\\\\times 10^{3}$, log scale\"),\n    y = \"KL Divergence\"\n  ) +\n  scale_x_log10(labels = formatter1000) + scale_y_log10() +\n  scale_colour_manual(\"Sample\", values = colour_samples) +\n  scale_linetype_discrete(\"Recalibration\") +\n  theme_paper() +\n  theme(legend.key.width = unit(1.5, \"cm\"))\n\n\nggsave(\n  p_ici, file = \"../figs/xgb-kl-calib-ici-leaves-all.pdf\",\n  width = 10, height = 8\n)\np_ici\n\n\n\n\n\nFigure 5.18: KL Divergence and Calibration (ICI) across increasing boosting iterations (log scales)\n\n\n\n\n\n\n\n\n\n\n\n\n\n5.5.4 Tables\n\nmodels_interest_xgb &lt;- models_of_interest_metrics |&gt; \n  group_by(scenario, recalib, sample, result_type) |&gt; \n  summarise(\n    AUC_lower = quantile(AUC, probs = 2.5/100),\n    AUC_upper = quantile(AUC, probs = 97.5/100),\n    AUC_sd = sd(AUC),\n    AUC = mean(AUC),\n    brier_lower = quantile(brier, probs = 2.5/100),\n    brier_upper = quantile(brier, probs = 97.5/100),\n    brier_sd = sd(brier),\n    brier = mean(brier),\n    ici_lower = quantile(ici, probs = 2.5/100),\n    ici_upper = quantile(ici, probs = 97.5/100),\n    ici_sd = sd(ici),\n    ici = mean(ici),\n    KL_20_true_probas_lower = quantile(KL_20_true_probas, probs = 2.5/100),\n    KL_20_true_probas_upper = quantile(KL_20_true_probas, probs = 97.5/100),\n    KL_20_true_probas_sd = sd(KL_20_true_probas),\n    KL_20_true_probas = mean(KL_20_true_probas),\n    quant_ratio_sd = sd(inter_quantile_10_90),\n    quant_ratio = mean(inter_quantile_10_90),\n    .groups = \"drop\"\n  ) |&gt; \n  mutate(\n    model = \"xgb\",\n    sample = str_to_lower(as.character(sample))\n  )\n\n# Sanity check\n# metrics_xgb_all |&gt; count(scenario, recalib, ind, sample, nb_iter) |&gt;\n#   filter(n != max(repns_vector))\n\ntable_models_interest_mean &lt;- \n  models_interest_xgb |&gt; \n  filter(sample == \"test\") |&gt; \n  select(\n    scenario, recalib, sample, result_type, \n    AUC, brier, ici, kl = KL_20_true_probas, quant_ratio\n  ) |&gt; \n  filter(\n    result_type %in% c(\"AUC*\", \"Brier*\", \"ICI*\", \"KL*\")\n  ) |&gt; \n  mutate(value_type = \"mean\")\n\n\ntable_models_interest_sd &lt;- \n  models_interest_xgb |&gt; \n  filter(sample == \"test\") |&gt; \n  select(\n    scenario, sample, model, result_type, \n    AUC = AUC_sd, brier = brier_sd, ici = ici_sd, \n    kl = KL_20_true_probas_sd, quant_ratio = quant_ratio_sd\n  ) |&gt; \n  filter(\n    result_type %in% c(\"AUC*\", \"Brier*\", \"ICI*\", \"KL*\")\n  ) |&gt; \n  mutate(value_type = \"sd\")\n\n\ntable_models_interest_mean |&gt; \n  # bind_rows(table_models_interest_sd) |&gt; \n  filter(scenario==5) |&gt; \n  select(-sample, -brier, -quant_ratio) |&gt; \n  pivot_wider(names_from = recalib, values_from = c(AUC, ici, kl)) |&gt; \n  select(-value_type) |&gt; \n  filter(result_type != \"Brier*\") |&gt; \n  mutate(\n    delta_platt = kl_None - kl_Platt,\n    delta_iso = kl_None - kl_Isotonic,\n    )\n\n# A tibble: 3 × 13\n  scenario result_type AUC_None AUC_Platt AUC_Isotonic ici_None ici_Platt\n     &lt;dbl&gt; &lt;fct&gt;          &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1        5 AUC*           0.840     0.840        0.840  0.00949    0.0383\n2        5 ICI*           0.840     0.839        0.839  0.00874    0.0381\n3        5 KL*            0.840     0.839        0.839  0.00893    0.0382\n# ℹ 6 more variables: ici_Isotonic &lt;dbl&gt;, kl_None &lt;dbl&gt;, kl_Platt &lt;dbl&gt;,\n#   kl_Isotonic &lt;dbl&gt;, delta_platt &lt;dbl&gt;, delta_iso &lt;dbl&gt;\n\n\n\n\n\n\nOjeda, Francisco M., Max L. Jansen, Alexandre Thiéry, Stefan Blankenberg, Christian Weimar, Matthias Schmid, and Andreas Ziegler. 2023. “Calibrating Machine Learning Approaches for Probability Estimation: A Comprehensive Comparison.” Statistics in Medicine 42 (29): 5451–78. https://doi.org/10.1002/sim.9921.",
    "crumbs": [
      "III. Simulated Data",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Extreme Gradient Boosting</span>"
    ]
  }
]