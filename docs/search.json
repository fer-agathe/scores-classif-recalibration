[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Recalibration",
    "section": "",
    "text": "1 Introduction\nThis notebook is the online appendix of the article titled xxx.” It provides supplementary materials to the main part of the paper.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#abstract-of-the-paper",
    "href": "index.html#abstract-of-the-paper",
    "title": "Recalibration",
    "section": "1.1 Abstract of the Paper",
    "text": "1.1 Abstract of the Paper",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#outline",
    "href": "index.html#outline",
    "title": "Recalibration",
    "section": "1.2 Outline",
    "text": "1.2 Outline",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#replication-codes",
    "href": "index.html#replication-codes",
    "title": "Recalibration",
    "section": "1.3 Replication Codes",
    "text": "1.3 Replication Codes\nThe codes to replicate the results displayed in the paper are presented in this ebook. We also provide the codes in an archive file with the following structure:\nSupplementary-materials\n├ ── ebook\n├ ── scripts\n│   └── functions\n|   |   └── data-ojeda.R\n|   |   └── data-setup-dgp-scenarios.R\n|   |   └── metrics.R\n|   |   └── real-data.R\n|   |   └── subsample_target_distribution.R\n|   |   └── utils.R\n│   └── 01_data_targeted_distrib.R\n│   └── 02_data-simulated.R\n│   └── 03_simul-trees.R\n│   └── 04_simul-random-forests.R\n│   └── 05_simul-random-forests-ntrees.R\n│   └── 06_simul-xgb.R\n│   └── 07_simul-glm.R\n│   └── 08_simul-gam.R\n│   └── 09_simul-gamsel.R\n│   └── 10_simul-comparison.R\n│   └── 11_real-priors-illustration.R\n│   └── 12_real-datasets-priors.R\n│   └── 13_real-estimations.R\n│   └── 14_real_results.R\n│   └── proj.Rproj\n Download the Replication Codes (Zip archive)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "target-distribution.html",
    "href": "target-distribution.html",
    "title": "2  Targeted Distribution",
    "section": "",
    "text": "2.1 Algorithm\nIn our generated sample, \\(\\mathcal{D}=\\{(\\boldsymbol{x}_i,y_i,{s}_i),i\\in\\{1,\\cdots,n\\}\\}\\), let \\(\\widehat{f}\\) denote the (empirical) density of scores. For the various scenarios, suppose that we want a specific distribution for the scores, denoted \\(g\\) (uniform, Beta, etc.). A classical idea is to use ‘’rejection sampling’’ techniques to create a subsample of the dataset. Set \\[\nc = \\sup_{s\\in(0,1)} \\frac{\\widehat{f}(s)}{g(s)} \\leq \\infty.\n\\] If \\(c\\) is finite, and not too large, we can use the standard rejection technique, described in Algorithm 2.1. In a nutshell, point \\(i\\) is kept with probability \\((cg(s_i))^{-1}\\widehat{f}(s_i)\\).\nNote: the reference mentioned: Chen (1999)\nIf \\(c\\) is too large, we use an iterative algorithm, described in Algorithm 2.2, inspired by Rumbell et al. (2023) (alternative options could be the ‘’Empirical Supremum Rejection Sampling’’ introduced in Caffo, Booth, and Davison (2002), for instance)\nTo implement this, we define the subset_target() function.\n#' @param data dataset\n#' @param score_name name of the column in data that contains the scores\n#' @param target_fun target distribution function.\n#' @param iter number of iterations.\n#' @param draw if TRUE (default) the distribution of scores (gray bars) and the\n#'  target distribution (in red) are plotted at each iteration.\n#' @seed if not `NULL`, seed to use\n#' @param data dataset\n#' @param probs_name name of the column in data that contains the observed\n#'  probabilities\n#' @param target_fun target distribution function.\n#' @param iter number of iterations.\n#' @param draw if TRUE (default) the distribution of scores (gray bars) and the\n#'  target distribution (in red) are plotted at each iteration.\n#' @seed if not `NULL`, seed to use\n#' @param verbose if `FALSE`, size of subsamplings at each iteration and KS test\n#'  results are hiddent\nsubset_target &lt;- function(data,\n                          probs_name,\n                          target_fun = function(x) dbeta(x,2,2),\n                          iter = 1,\n                          draw = TRUE,\n                          seed = NULL,\n                          verbose = TRUE){\n  select &lt;- rep(nrow(data),iter + 1)\n  if (!is.null(seed)) set.seed(seed)\n\n  # Get the scores from the dataset\n  probs_01 &lt;- data |&gt; pull(!!probs_name)\n  if (verbose == TRUE) cat(\"1) Size ...... \", nrow(data), \"\\n\", sep = \"\")\n\n  # Kolmogorov-Smirnov Test\n  fun &lt;- Vectorize(function(x) integrate(target_fun, 0, x)$value)\n  K &lt;- ks.test(probs_01, fun)\n\n  if (verbose) {\n    cat(\"1)  ks ............ \", K$statistic, \"\\n\", sep = \"\")\n    cat(\"1)  (pvalue) ...... \", K$p.value, \"\\n\", sep = \"\")\n  }\n\n  if (draw) {\n    # Histogram of scores (gray) and target distribution (red)\n    hist(probs_01,probability = TRUE, xlab = \"\", ylab = \"\", main = \"Initial\")\n    val_x &lt;- seq(0,1,length = 601)\n    lines(val_x,target_fun(val_x), col = \"red\")\n  }\n\n  data_subset &lt;- data\n\n  for (k in 1:iter) {\n    n &lt;- nrow(data_subset)\n    initial_density &lt;- kde(x = probs_01, eval.points = probs_01)\n    # Probability to include each observation in the current subset\n    prob_acceptation &lt;- target_fun(probs_01) / initial_density$estimate\n    prob_acceptation &lt;- pmin(prob_acceptation / max(prob_acceptation), 1)\n    # For each scores from the current data subset, decide whether or not to\n    # include it based on a random draw from a Ber(prob_acceptation)\n    index_acceptation &lt;- rbinom(n, size = 1, prob = prob_acceptation)\n    # Use this index to keep only the selected data\n    data_subset &lt;- data_subset[which(index_acceptation ==1 ), ]\n    select[k + 1] &lt;- nrow(data_subset)\n    probs_01 &lt;- data_subset |&gt; pull(!!probs_name)\n    if (verbose == TRUE)\n      cat(k + 1, \") Size ...... \", nrow(data_subset), \"\\n\", sep = \"\")\n    # Kolmogorov-Smirnov Test\n    K &lt;- ks.test(probs_01, fun)\n    if (verbose) {\n      cat(k + 1, \")   ks ............ \", K$statistic, \"\\n\", sep = \"\")\n      cat(k + 1, \")   (pvalue) ...... \", K$p.value, \"\\n\", sep = \"\")\n    }\n    if (draw) {\n      hist(\n        probs_01, probability = TRUE, xlab = \"\", ylab = \"\",\n        main = paste(\"Iteration \", k)\n      )\n      val_x &lt;- seq(0, 1, length = 601)\n      lines(val_x, target_fun(val_x), col = \"red\")\n    }\n  }\n  data_subset\n}",
    "crumbs": [
      "I. Subsampling",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Targeted Distribution</span>"
    ]
  },
  {
    "objectID": "target-distribution.html#algorithm",
    "href": "target-distribution.html#algorithm",
    "title": "2  Targeted Distribution",
    "section": "",
    "text": "\\begin{algorithm} \\caption{Subsample a dataset so that the distribution of scores has density $g$ (Rejection, $c$ small)} \\begin{algorithmic} \\Require $\\mathcal{D}=\\{(\\boldsymbol{x}_i,y_i,{s}_i),i\\in\\{1,\\cdots,n\\}\\}$ and $g$ (target density) \\State $\\mathcal{I} \\gets ,i\\in\\{1,\\cdots,n\\}$ \\State $\\widehat{f} \\gets$ density of $\\{({s}_i),i\\in\\mathcal{I}\\}$, using Chen (1999) \\State $c = \\displaystyle\\sup_{s\\in(0,1)} \\frac{\\widehat{f}(s)}{g(s)} \\gets \\max_{i=1,\\cdots,n}\\displaystyle\\frac{\\widehat{f}(s_i)}{g(s_i)} $ \\For{$i\\in\\{1,\\cdots,n\\}$} \\State $U \\gets \\mathcal{U}([0,1])$ \\If{$\\displaystyle U &gt; \\frac{\\widehat{f}(s_i)}{c\\,g(s_i)}$} \\State $\\mathcal{I} \\gets \\mathcal{I}\\backslash\\{i\\}$ , i.e. ``reject\" \\EndIf \\EndFor \\State $s\\mathcal{D}=\\{(\\boldsymbol{x}_i,y_i,{s}_i),i\\in\\mathcal{I}\\}$ \\end{algorithmic} \\end{algorithm}\n\n\n\n\n\n\n\\begin{algorithm} \\caption{Subsample a dataset so that the distribution of scores has density $g$ (Iterative Rejection, $c$ large)} \\begin{algorithmic} \\Require $\\mathcal{D}=\\{(\\boldsymbol{x}_i,y_i,{s}_i),i\\in\\{1,\\cdots,n\\}\\}$, $\\epsilon&gt;0$ and $g$ (target density) \\State $\\mathcal{I} \\gets \\{1,\\cdots,n\\}$ \\State $\\widehat{f} \\gets$ density of $\\{({s}_i),i\\in\\mathcal{I}\\}$, using Chen (1999) \\State $d \\gets \\|\\widehat{F}-G\\|_{\\infty}$ (Kolmogorov-Smirnov distance) \\While{$d&gt;\\epsilon$} \\State $\\mathcal{J} \\gets \\mathcal{I}$ \\For{$i\\in\\mathcal{I}$} \\State $U \\gets \\mathcal{U}([0,1])$ \\If{$\\displaystyle U&gt;\\frac{\\widehat{f}(s_i)}{g(s_i)}$} \\State $\\mathcal{J} \\gets \\mathcal{J}\\backslash\\{i\\}$ , i.e. 'reject' observation $i$ \\EndIf \\EndFor \\State $\\mathcal{I} \\gets \\mathcal{J}$ \\State $\\widehat{f} \\gets$ density of $\\{({s}_i),i\\in\\mathcal{I}\\}$ \\State $d \\gets \\|\\widehat{F}-G\\|_{\\infty}$ \\EndWhile \\State $s\\mathcal{D}=\\{(\\boldsymbol{x}_i,y_i,{s}_i),i\\in\\mathcal{I}\\}$ \\end{algorithmic} \\end{algorithm}",
    "crumbs": [
      "I. Subsampling",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Targeted Distribution</span>"
    ]
  },
  {
    "objectID": "target-distribution.html#first-example",
    "href": "target-distribution.html#first-example",
    "title": "2  Targeted Distribution",
    "section": "2.2 First Example",
    "text": "2.2 First Example\nLet us begin with generating some binary data, using a linear predictor for the true probability.\n\nn &lt;- 1e5 # Number of obs.\n# Covariates\nx1 &lt;- rnorm(n)\nx2 &lt;- rnorm(n)\n# True probabilities\np &lt;- function(x1, x2) .4 * x1 - .2*x2\n# Observed event\ny &lt;- rnorm(n,p(x1, x2), .4)\ntb &lt;- tibble(y = y, x1 = x1, x2 = x2)\n\nLet us consider a linear model to predict the observed event:\n\nreg &lt;- lm(y ~ x1 + x2, data = tb)\nscores &lt;- predict(reg)\ntb$scores &lt;- scores\n\nKeeping only scores between 0 and 1 (would not need to do so for glm)\n\ntb_01 &lt;- tb[(scores &gt; 0) & (scores &lt; 1), ]\ndata &lt;- tb_01\n\n\nB &lt;- subset_target(data = data, probs_name = \"scores\", iter = 4)\n\n1) Size ...... 48486\n1)  ks ............ 0.2997993\n1)  (pvalue) ...... 0\n\n\n\n\n\n\n\n\n\n2) Size ...... 17127\n2)   ks ............ 0.004201355\n2)   (pvalue) ...... 0.9229854\n\n\n\n\n\n\n\n\n\n3) Size ...... 16802\n3)   ks ............ 0.00263152\n3)   (pvalue) ...... 0.9998175\n\n\n\n\n\n\n\n\n\n4) Size ...... 16252\n4)   ks ............ 0.002203808\n4)   (pvalue) ...... 0.9999985\n\n\n\n\n\n\n\n\n\n5) Size ...... 15982\n5)   ks ............ 0.002392562\n5)   (pvalue) ...... 0.9999885\n\n\n\n\n\n\n\n\n\nLet us consider another example.\n\nlibrary(splines)",
    "crumbs": [
      "I. Subsampling",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Targeted Distribution</span>"
    ]
  },
  {
    "objectID": "target-distribution.html#second-examplle",
    "href": "target-distribution.html#second-examplle",
    "title": "2  Targeted Distribution",
    "section": "2.3 Second Examplle",
    "text": "2.3 Second Examplle\nWe generate another dataset.\n\nn &lt;- 1e6\nx &lt;- rbeta(n, 1, 2)\ny &lt;- rbinom(n, size = 1, prob = x)\nbase &lt;- tibble(\n  x = x,\n  y = y,\n  id = 1:n\n)\n\nLet us assume that the scores are estimated using a logistic model.\n\nreg &lt;- glm(y ~ bs(x), data = base, family = binomial)\nbase$scores &lt;- predict(reg, type = \"response\")\n\nLet us further assume that we want the scores to be distributed according to a Beta(2,1).\n\nB &lt;- subset_target(\n  data = base, \n  probs_name = \"scores\", \n  iter = 1, \n  target_fun = function(x) dbeta(x,2,1)\n)\n\n1) Size ...... 1000000\n\n\nWarning in ks.test.default(probs_01, fun): ties should not be present for the\none-sample Kolmogorov-Smirnov test\n\n\n1)  ks ............ 0.502429\n1)  (pvalue) ...... 0\n\n\n\n\n\n\n\n\n\n2) Size ...... 7433\n2)   ks ............ 0.05460928\n2)   (pvalue) ...... 1.115475e-19\n\n\n\n\n\n\n\n\n\nWe check the new observations:\n\nreg2 &lt;- glm(y ~ bs(x), data = B, family = binomial)\n\n\nval_x &lt;- seq(0, 1, length = 601)\nplot(\n  val_x,\n  predict(reg, type = \"response\", newdata = data.frame(x = val_x)),\n  type = \"l\", lwd = 2\n)\n\nWarning in bs(x, degree = 3L, knots = numeric(0), Boundary.knots =\nc(1.13039405272881e-06, : some 'x' values beyond boundary knots may cause\nill-conditioned bases\n\nlines(\n  val_x,\n  predict(reg2, type = \"response\", newdata = data.frame(x = val_x)),\n  type = \"l\", lwd = 2, col = \"red\"\n)\n\nWarning in bs(x, degree = 3L, knots = numeric(0), Boundary.knots =\nc(0.00521055325908034, : some 'x' values beyond boundary knots may cause\nill-conditioned bases\n\n\n\n\n\n\n\n\n\n\n\n\n\nCaffo, Brian S, James G Booth, and AC Davison. 2002. “Empirical Supremum Rejection Sampling.” Biometrika 89 (4): 745–54.\n\n\nChen, Song Xi. 1999. “Beta Kernel Estimators for Density Functions.” Computational Statistics & Data Analysis 31 (2): 131–45.\n\n\nRumbell, Timothy, Jaimit Parikh, James Kozloski, and Viatcheslav Gurev. 2023. “Novel and Flexible Parameter Estimation Methods for Data-Consistent Inversion in Mechanistic Modelling.” Royal Society Open Science 10 (11): 230668.",
    "crumbs": [
      "I. Subsampling",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Targeted Distribution</span>"
    ]
  },
  {
    "objectID": "metrics.html",
    "href": "metrics.html",
    "title": "3  Metrics",
    "section": "",
    "text": "3.1 Performance and Calibration Metrics\nTo measure performance, we chose to compute:\nTo measure calibration, we compute two metrics:\nAll these metrics are computed in a function we name compute_metrics() which takes three arguments:\n#' Brier Score\n#'\n#' The Brier Score \\citep{brier_1950}, is expressed as: \\deqn{\\text{BS} =\n#' \\frac{1}{n}\\sum_{i=1}^{n} \\big(\\hat{s}(\\mathbf{x}_i) - d_i\\big)^{2}} where\n#' \\eqn{d_i \\in \\{0,1\\}} is the observed event for observation \\eqn{i}.\n#'\n#' @param scores vector of scores\n#' @param obs vector of observed binary events\n#'\n#' @references Brier, G. W. (1950). Verification of forecasts expressed in terms\n#' of probability. Monthly Weather Review 78: 1–3.\n#'\n#' @export\nbrier_score &lt;- function(obs, scores) mean((scores - obs)^2)\n#' Computes the calibration metrics for a set of observed and predicted\n#' probabilities\n#'\n#' @returns\n#' \\itemize{\n#'   \\item \\code{mse}: True Mean Squared Error based on true probability.\n#'   \\item \\code{acc}: accuracy with a .5 probability threshold.\n#'   \\item \\code{AUC}: Area Under the ROC Curve.\n#'   \\item \\code{brier}: Brier score.\n#'   \\item \\code{ici}: Integrated Calibration Index.\n#' }\n#'\n#' @param obs observed events\n#' @param scores predicted scores\n#' @param true_probas true probabilities from the PGD (to compute MSE)\n#'\n#' @importFrom purrr map\n#' @importFrom tibble tibble\n#' @importFrom dplyr bind_rows\n#'\n#' @export\ncompute_metrics &lt;- function(obs,\n                            scores,\n                            true_probas = NULL) {\n\n  # True MSE\n  if (!is.null(true_probas)) {\n    mse &lt;- mean((true_probas - scores)^2)\n  } else {\n    mse &lt;- NA\n  }\n\n  # True MAE\n  if (!is.null(true_probas)) {\n    mae &lt;- mean(abs(true_probas - scores))\n  } else {\n    mae &lt;- NA\n  }\n\n  # AUC\n  AUC &lt;- pROC::auc(obs, scores, levels = c(\"0\", \"1\"), quiet = TRUE) |&gt;\n    as.numeric()\n\n  # Brier Score\n  brier &lt;- brier_score(obs = as.numeric(as.character(obs)), scores = scores)\n  # gmish::brier(pred = scores, obs = obs) #same results\n\n  # ICI\n  ici_quiet &lt;- purrr::quietly(gmish::ici)\n  ici &lt;- ici_quiet(pred = scores, obs = as.numeric(as.character(obs)))\n  ici &lt;- ici$result\n\n  # Accuracy\n  pred_class &lt;- ifelse(scores &gt; .5, yes = 1, no = 0)\n  acc &lt;- sum(diag(table(obs = obs, pred = pred_class))) / length(scores)\n\n  tibble(\n    mse = mse,\n    mae = mae,\n    acc = acc,\n    AUC = AUC,\n    brier = brier,\n    ici = ici\n  )\n}",
    "crumbs": [
      "II. Metrics",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Metrics</span>"
    ]
  },
  {
    "objectID": "metrics.html#performance-and-calibration-metrics",
    "href": "metrics.html#performance-and-calibration-metrics",
    "title": "3  Metrics",
    "section": "",
    "text": "the true Mean Squared Error (MSE): the average of the quadratic difference between predicted scores and true probabilities (only if the true probabilities are available thanks to the knowledge of the PGD)\nthe accuracy, which gives the proportion of correctly predicted instances; we use a probability threshold of 0.5)\nthe AUC.\n\n\n\nthe Brier score (Brier (1950))\nthe Integrated Calibration Index (Austin and Steyerberg (2019)).\n\n\n\n\n\n\n\nBrier Score\n\n\n\nGiven a sample size \\(n\\), the Brier Score Brier (1950), is expressed as: \\[\n\\begin{equation}\n\\text{BS} = \\frac{1}{n}\\sum_{i=1}^{n} \\big(\\hat{s}(\\mathbf{x}_i) - d_i\\big)^{2}\\enspace ,\n\\end{equation}\n\\tag{3.1}\\]\nwhere \\(\\hat{s}(\\mathbf{x}_i)\\) and \\(d_i \\in \\{0,1\\}\\) are the predicted score and observed event, respectively, for observation \\(i\\).\n\n\n\n\n\n\n\n\nIntegrated Calibration Index\n\n\n\nInstead of defining bins, the Integrated Calibration Index or ICI (Austin and Steyerberg (2019)) measures calibration using a local estimation (loess if the number of observation is lower than 1000 ; using a GAM otherwise).\nThe occurrence of the binary event is regressed on the predicted scores, employing either locally estimated scatterplot smoothing (LOESS) when the number of observations is small (\\(n &lt; 1000\\)) or cubic regression splines for larger datasets. The ICI is defined as \\[\n\\begin{equation}\n    \\text{ICI} = \\int_{0}^{1} f(p)  \\phi(p)\\, dp\n\\end{equation}\n\\tag{3.2}\\] where \\(f(p) = | p - \\g(p) |\\) is the absolute difference between the calibration curve and the bisector where \\(p\\) denotes a predicted score (i.e., \\(p=\\hat{s}(\\mathbf{x})\\)) and \\(\\g(p)\\) is the value of the calibration curve at this predicted score. The density function of the distribution of predicted scores is denoted \\(\\phi(p)\\).\n\n\n\n\nobs: a vector of observed binary events\nscores: a vector of predicted scores\ntrue_probas: if available, a vector of true probabilities from the PGD (to compute MSE).",
    "crumbs": [
      "II. Metrics",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Metrics</span>"
    ]
  },
  {
    "objectID": "metrics.html#dispersion-metrics",
    "href": "metrics.html#dispersion-metrics",
    "title": "3  Metrics",
    "section": "3.2 Dispersion Metrics",
    "text": "3.2 Dispersion Metrics\nWe compute the Kullback-Leibler divergence Kullback and Leibler (1951) between the distribution of the estimated scores and the distribution of the true probabilities. Denoting (Q) the distribution of the scores and (P) the distribution of the true probabilities, the Kullback Leibler divergence of \\(Q\\) with respect to \\(P\\) is :% \\[\\begin{equation}\nD_{KL}(Q || P) = \\sum_{i} Q(i) \\log \\frac{Q(i)}{P(i)}.\n\\end{equation}\\]\nThe distributions both need to be discretized. We divide the segment ([0,1]) into (m) bins.\nIn the dispersion_metrics() that we define to that end, we consider \\(m=20\\) bins. We also consider switching the reference distribution (where \\(Q\\) denotes the distribution of the true probabilities and \\(P\\) denotes the distribution of scores).\n\n#' Computes the dispersion metrics for a set of observed and predicted\n#' probabilities\n#'\n#' @returns\n#' \\itemize{\n#'   \\item \\code{inter_quantile_25_75}: Difference of inter-quantile between 25% and 75%\n#'   \\item \\code{inter_quantile_10_90}: Difference of inter-quantile between 10% and 90%\n#'   \\item \\code{KL_20_true_probas}: KL of of predicted probabilities w.r. to true probabilities with 20 bins\n#'   \\item \\code{KL_20_scores}: KL of of true probabilities w.r. to predicted probabilities with 20 bins\n#'   \\item \\code{ind_cov}: Difference between the variance of true probabilities and the covariance between true probabilities and predicted scores\n#' }\n#'\n#' @param true_probas true probabilities from simulations\n#' @param scores predicted scores\n#'\ndispersion_metrics &lt;- function(true_probas, scores){\n\n  # Inter-quantiles\n  inter_q_80 &lt;- diff(quantile(scores, c(.9, .1))) /\n    diff(quantile(true_probas, c(.9, .1)))\n  inter_q_50 &lt;- diff(quantile(scores, c(.75,.25))) /\n    diff(quantile(true_probas, c(.75, .25)))\n\n  # KL divergences\n  m &lt;- 20 # Number of bins\n  h_p &lt;- hist(true_probas,breaks = seq(0, 1, length = m + 1), plot = FALSE)\n  h_phat &lt;- hist(scores, breaks = seq(0, 1, length = m + 1), plot = FALSE)\n  # Densities\n  h1 &lt;- rbind(h_phat$density / m,h_p$density / m) # Reference : true probabilities\n  h2 &lt;- rbind(h_p$density / m, h_phat$density / m) # Reference : predicted scores\n  KL_20_true_probas &lt;- distance(\n    h1, method = \"kullback-leibler\", unit = \"log2\", mute.message = TRUE)\n  KL_20_scores &lt;- distance(\n    h2, method = \"kullback-leibler\", unit = \"log2\", mute.message = TRUE)\n\n  # Indicator of the difference between variance and covariance\n  var_p &lt;- var(true_probas)\n  cov_p_phat &lt;- cov(true_probas, scores)\n  ind_cov &lt;- abs(cov_p_phat - var_p)\n\n  # Collection\n  dispersion_metrics &lt;- tibble(\n    \"inter_quantile_25_75\" = as.numeric(inter_q_50),\n    \"inter_quantile_10_90\" = as.numeric(inter_q_80),\n    \"KL_20_true_probas\" = as.numeric(KL_20_true_probas),\n    \"KL_20_scores\" = as.numeric(KL_20_scores),\n    \"ind_cov\" = ind_cov\n    )\n\n  dispersion_metrics\n}\n\nLastly, we estimate \\(\\mathbb{P}(q_1 &lt; \\hat{s}(\\mathbf{x}) &lt; q_2)\\), with \\(q_2 = 1-q_1\\), for different values of \\(q_1\\) and \\(q_2\\). To do so, we simply calculate the sample proportion of scores between \\(q_1\\) and \\(q_2\\). The prop_btw_quantiles() does it.\n\n#' Computes \\hat{P}(q_1 &lt; s &lt; q_2)\n#'\n#' @param s scores\n#' @param q1 lower quantile\n#' @param q2 upper quantile (default to 1-q2)\nprop_btw_quantiles &lt;- function(s, q1, q2 = 1 - q1) {\n  tibble(q1 = q1, q2 = q2, freq = mean(s &lt; q2 & s &gt; q1))\n}\n\n\n\n\n\nAustin, Peter C., and Ewout W. Steyerberg. 2019. “The Integrated Calibration Index (ICI) and Related Metrics for Quantifying the Calibration of Logistic Regression Models.” Statistics in Medicine 38 (21): 4051–65. https://doi.org/10.1002/sim.8281.\n\n\nBrier, Glenn W. 1950. “Verification of Forecasts Expressed in Terms of Probability.” Monthly Weather Review 78 (1): 1–3.\n\n\nKullback, S., and R. A. Leibler. 1951. “On Information and Sufficiency.” The Annals of Mathematical Statistics 22 (1): 79–86. https://doi.org/10.1214/aoms/1177729694.",
    "crumbs": [
      "II. Metrics",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Metrics</span>"
    ]
  },
  {
    "objectID": "simul-data.html",
    "href": "simul-data.html",
    "title": "4  Simulated Data",
    "section": "",
    "text": "4.1 Functions\nWe load the functions from Chapter 2 to subsample from a dataset so that the true probability follows a beta distribution.\nlibrary(ks)\nsource(\"../scripts/functions/subsample_target_distribution.R\")\nThe simulate_data() function generates data for one of the 12 first scenarios described in the article or one of our additional 4 scenarios. This is a helper function that is called in the second one, simulate_data_wrapper() which generates datasets.\nCode\n#' Simulates train/test\n#'\n#' @details\n#' This function is a modified version of the function 'simulateData' in the\n#' R script 'functions-for-calibrating-random-forests.R' provided in the\n#' supplementary material of  Dankowski, T., & Ziegler, A. (2016). Calibrating\n#' random forests for probability estimation. Statistics in medicine, 35(22),\n#' 3949-3960.\n#'\n#' @param n_num number of numerical covariates\n#' @param add_categ if `TRUE`, add 5 categorical variables\n#' @param coeff vector of coefficients (of length n_num + 5)\n#' @param n_noise number of noise variables (drawn from N(0,1))\n#' @param mean_num vector of mean for the numerical variables\n#' @param sd_num vector of standard deviations for the numerical variables\n#' @param size_train size for the train set\n#' @param size_calib size for the calibration set\n#' @param size_valid size for the validation set\n#' @param size_test size for the test set\n#' @param transform_probs if `TRUE`, the true probability is taken to the power of 3\n#' @param linear_predictor if `TRUE`, the predictor of the true probability is a\n#'  linear combination of the covariates. Otherwise, the squared term for x1 is\n#'  added, as well as an interaction term between x2 and x3 (`n_num` thus need\n#'  to be at least 3).\n#' @param seed desired seed (default to `NULL`)\n#' @param linear_predictor_factor if `transform_probs = TRUE`, scalar used to\n#'  draw more observation before subsampling. Default to 3 (a sample 3 times\n#'  larger than `the size of the samples will first be generated before\n#'  subsampling so that the true probability follows a Beta(2,2).\n#'\n#' @returns A list with the following components:\n#'  - train: train set\n#'  - calib: calibration set\n#'  - valid: validation set\n#'  - test: test set\n#'  - probs_train: true probabilities for binary event in train set\n#'  - probs_calib: true probabilities for binary event in calibration set \n#'  - probs_valid: true probabilities for binary event in validation set\n#'  - probs_test: true probabilities for binary event in test set\nsimulate_data &lt;- function(n_num = 2,\n                          add_categ = FALSE,\n                          coeff,\n                          n_noise = 0,\n                          mean_num,\n                          sd_num,\n                          size_train,\n                          size_calib,\n                          size_valid,\n                          size_test,\n                          transform_probs = FALSE,\n                          linear_predictor = TRUE,\n                          linear_predictor_factor = 3,\n                          seed = NULL) {\n\n  n_obs &lt;- size_train + size_calib + size_valid + size_test\n  if (linear_predictor == FALSE) {\n    n_obs &lt;- n_obs * linear_predictor_factor\n  }\n\n  if (!is.null(seed)) {\n    set.seed(seed)\n  }\n\n  # Numerical covariates\n  covariates &lt;- map2(\n    .x = mean_num,\n    .y = sd_num,\n    .f = ~rnorm(n = n_obs, mean = .x, sd = .y)\n  )\n  names(covariates) &lt;- str_c(\"x\", 1:n_num)\n  covariates &lt;- as_tibble(covariates)\n\n  # Categorical covariates\n  if (add_categ == TRUE) {\n    x_c1 &lt;- base::sample(c(0, 1), n_obs, replace = TRUE)\n    x_c2 &lt;- base::sample(c(0, 1), n_obs, replace = TRUE)\n    x_c3 &lt;- base::sample(c(1, 2, 3), n_obs, replace = TRUE)\n    x_c4 &lt;- base::sample(c(1, 2, 3, 4), n_obs, replace = TRUE)\n    x_c5 &lt;- base::sample(c(1, 2, 3, 4, 5), n_obs, replace = TRUE)\n\n    categ_covariates &lt;- tibble(x_c1, x_c2, x_c3, x_c4, x_c5)\n    colnames(categ_covariates) &lt;- str_c(\"x\", (n_num + 1):(n_num + 5))\n    covariates &lt;- bind_cols(covariates, categ_covariates)\n  }\n\n  if (linear_predictor == TRUE) {\n    # Linear predictor\n    eta &lt;- as.matrix(covariates) %*% coeff\n  } else {\n    if (n_num &lt; 3) stop(\"If linear_predictor=TRUE, n_num must be greater than 2\")\n    eta &lt;- as.matrix(covariates) %*% coeff +\n      covariates$x1^2 + covariates$x2^2 * covariates$x3\n  }\n\n  # True probability\n  true_prob &lt;- as.numeric(1 / (1 + exp(-eta)))\n  if (transform_probs) true_prob &lt;- true_prob^3\n\n  # Observed event\n  y &lt;- rbinom(n_obs, size = 1, prob = true_prob)\n\n  # Create dataset with observed event and covariates\n  tb &lt;- tibble(y, covariates)\n\n  if (linear_predictor == FALSE) {\n    # We would like the probabilities to be distributed as a Beta(2,2)\n    tb &lt;- tb |&gt; mutate(p = true_prob)\n    tb &lt;- subset_target(\n      data = tb,\n      probs_name = \"p\",\n      target_fun = function(x) dbeta(x,2,2),\n      iter = 1, draw = FALSE,\n      seed = seed,\n      verbose = FALSE\n    )\n    n_obs &lt;- size_train + size_calib + size_valid + size_test\n    if (nrow(tb) &lt; n_obs) {\n      stop(\n        str_c(\"The number of observation generated is lower than the \",\n              \"desired number. Increase `linear_predictor_factor`.\")\n      )\n    }\n    true_prob &lt;- tb$p[1:n_obs]\n    tb &lt;- tb |&gt; select(-p) |&gt; dplyr::slice_head(n = n_obs)\n  }\n\n\n  # Noise variables\n  if (n_noise &gt; 0) {\n    noise &lt;- matrix(\n      rnorm(n_noise * n_obs, mean = 0, sd = 1),\n      ncol = n_noise,\n      nrow = n_obs,\n      byrow = FALSE\n    ) |&gt;\n      as_tibble()\n    colnames(noise) &lt;- str_c(\"noise_\", 1:n_noise)\n    tb &lt;- bind_cols(tb, noise)\n  }\n\n  # Split data into train/calib/valid/test\n  tb_train &lt;- tb |&gt; dplyr::slice(1:size_train)\n  true_prob_train &lt;- true_prob[1:size_train]\n  \n  # Calibration\n  ind_calib &lt;- (size_train + 1):(size_train + size_calib)\n  tb_calib &lt;- tb |&gt; dplyr::slice(ind_calib)\n  true_prob_calib &lt;- true_prob[ind_calib]\n  \n  # Validation\n  ind_valid &lt;- (size_train + size_calib + 1):(size_train + size_calib + size_valid)\n  tb_valid &lt;- tb |&gt; dplyr::slice(ind_valid)\n  true_prob_valid &lt;- true_prob[ind_valid]\n  \n  # Test\n  ind_test &lt;- (size_train + size_calib + size_valid+ 1):n_obs\n  tb_test &lt;- tb |&gt; dplyr::slice(ind_test)\n  true_prob_test &lt;- true_prob[ind_test]\n  \n  list(\n    train = tb_train,\n    calib = tb_calib,\n    valid = tb_valid,\n    test = tb_test,\n    probs_train = true_prob_train,\n    probs_calib = true_prob_calib,\n    probs_valid = true_prob_valid,\n    probs_test = true_prob_test\n  )\n}\nThe simulate_data_wrapper() is the one we call to generate a dataset, given a scenario and a seed.\nCode\n#' Generates data for a given simulation scenario.\n#'\n#' @details\n#' Wrapper of 'simulate_data' function that generates the data for a given\n#' simulation scenario.\n#'\n#' @param scenario simulation scenario number.\n#' @param params_df data frame containing the parameters to be passed to the\n#'  `simulate_data` for each simulation scenario.\n#' @param repn Number of current replication to be generated for the given\n#'  simulation scenario.\n#'\n#' @returns A list with the following components:\n#'  - scenario: the scenario ID\n#'  - params_df: the parameters used for the data generation for the given\n#'               scenario.\n#'  - repn: Number of current replication that was generated for the given\n#'          simulation scenario.\n#'  - data: list with the simulated data (train, valid, test, probs_train,\n#'          probs_valid and probs_test)\n#'          see result of `simulate_data()`.\nsimulate_data_wrapper &lt;- function(scenario, params_df, repn) {\n  params &lt;- params_df[params_df[[\"scenario\"]] == scenario, ]\n  if(nrow(params) != 1) stop(\"More than one row from params_df chosen\")\n\n  seed_for_repn &lt;- pull(params, \"seed\") + repn\n\n  args &lt;- list(\n    coeff = params |&gt; pull(\"coefficients\") |&gt; pluck(1),\n    n_num = params |&gt; pull(\"n_num\"),\n    add_categ = params |&gt; pull(\"add_categ\"),\n    n_noise = params |&gt; pull(\"n_noise\"),\n    mean_num = params |&gt; pull(\"mean_num\") |&gt; pluck(1),\n    sd_num = params |&gt; pull(\"sd_num\") |&gt; pluck(1),\n    size_train = params |&gt; pull(\"size_train\"),\n    size_calib = params |&gt; pull(\"size_calib\"),\n    size_valid = params |&gt; pull(\"size_valid\"),\n    size_test = params |&gt; pull(\"size_test\"),\n    transform_probs = params |&gt; pull(\"transform_probs\"),\n    linear_predictor = params |&gt; pull(\"linear_predictor\"),\n    seed = seed_for_repn\n  )\n  sim_data &lt;- do.call(\"simulate_data\", args)\n\n  list(\n    scenario = scenario,\n    params_df = params,\n    repn = repn,\n    data = sim_data\n  )\n\n}",
    "crumbs": [
      "III. Simulated Data",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Simulated Data</span>"
    ]
  },
  {
    "objectID": "simul-data.html#scenarios",
    "href": "simul-data.html#scenarios",
    "title": "4  Simulated Data",
    "section": "4.2 Scenarios",
    "text": "4.2 Scenarios\nLet us define the 12 first scenarios, using the code provided in Ojeda et al. (2023).\n\nDGP 1:\n\nScenario 1: basic scenario with two continuous predictors, without noise variable\nScenarios 2, 3, 4: same as 1 but with noise variables (10, 50, 100)\n\nDGP 2:\n\nScenarios 5 to 8: similar to 1 to 4 but with right-skewed true probability distribution (true probability taken to the power of 3)\n\nDGP 3:\n\nScenarios 9 to 12: similar to 1 to 4 but with ten predictors instead of two (5 numerical and 5 categorical)\n\n\nWe add four other scenarios, in which the predictor is nonlinear:\n\nDGP 4:\n\nScenarios 13 to 16: similar to 1 to 4 but with 3 covariates instead of 2 and with a nonlinear predictor which also contains an interaction term (\\(\\eta = \\alpha _1x_1 + \\alpha_2 x_2 + \\alpha_3 x_3 + \\beta x_1^2 + \\gamma x_2 \\times x_3\\)). In addition, the distribution of the true probabilities of the observed data follows a Beta(2,2) distribution.\n\n\nThe desired number of replications for each scenario needs to be set:\n\nrepns_vector &lt;- 1:100\n\nWe set the different parameters for each scenario.\n\n# Coefficients beta\ncoefficients &lt;- list(\n  # First category (baseline, 2 covariates)\n  c(0.5, 1),  # scenario 1, 0 noise variable\n  c(0.5, 1),  # scenario 2, 10 noise variables\n  c(0.5, 1),  # scenario 3, 50 noise variables\n  c(0.5, 1),  # scenario 4, 100 noise variables\n  # Second category (same as baseline, with lower number of 1s)\n  c(0.5, 1),  # scenario 5, 0 noise variable\n  c(0.5, 1),  # scenario 6, 10 noise variables\n  c(0.5, 1),  # scenario 7, 50 noise variables\n  c(0.5, 1),  # scenario 8, 100 noise variables\n  # Third category (same as baseline but with 5 num. and 5 categ. covariates)\n  c(0.1, 0.2, 0.3, 0.4, 0.5, 0.01, 0.02, 0.03, 0.04, 0.05),\n  c(0.1, 0.2, 0.3, 0.4, 0.5, 0.01, 0.02, 0.03, 0.04, 0.05),\n  c(0.1, 0.2, 0.3, 0.4, 0.5, 0.01, 0.02, 0.03, 0.04, 0.05),\n  c(0.1, 0.2, 0.3, 0.4, 0.5, 0.01, 0.02, 0.03, 0.04, 0.05),\n  # Fourth category (nonlinear predictor, 3 covariates)\n  c(0.5, 1, .3),  # scenario 5, 0 noise variable\n  c(0.5, 1, .3),  # scenario 6, 10 noise variables\n  c(0.5, 1, .3),  # scenario 7, 50 noise variables\n  c(0.5, 1, .3)  # scenario 8, 100 noise variables\n)\n\n# Mean parameter for the normal distribution to draw from to draw num covariates\nmean_num &lt;- list(\n  # First category (baseline, 2 covariates)\n  rep(0, 2),  # scenario 1, 0 noise variable\n  rep(0, 2),  # scenario 2, 10 noise variables\n  rep(0, 2),  # scenario 3, 50 noise variables\n  rep(0, 2),  # scenario 4, 100 noise variables\n  # Second category (same as baseline, with lower number of 1s)\n  rep(0, 2),  # scenario 5, 0 noise variable\n  rep(0, 2),  # scenario 6, 10 noise variables\n  rep(0, 2),  # scenario 7, 50 noise variables\n  rep(0, 2),  # scenario 8, 100 noise variables\n  # Third category (same as baseline but with 5 num. and 5 categ. covariates)\n  rep(0, 5),\n  rep(0, 5),\n  rep(0, 5),\n  rep(0, 5),\n  # Fourth category (nonlinear predictor, 3 covariates)\n  rep(0, 3),\n  rep(0, 3),\n  rep(0, 3),\n  rep(0, 3)\n)\n# Sd parameter for the normal distribution to draw from to draw num covariates\nsd_num &lt;- list(\n  # First category (baseline, 2 covariates)\n  rep(1, 2),  # scenario 1, 0 noise variable\n  rep(1, 2),  # scenario 2, 10 noise variables\n  rep(1, 2),  # scenario 3, 50 noise variables\n  rep(1, 2),  # scenario 4, 100 noise variables\n  # Second category (same as baseline, with lower number of 1s)\n  rep(1, 2),  # scenario 5, 0 noise variable\n  rep(1, 2),  # scenario 6, 10 noise variables\n  rep(1, 2),  # scenario 7, 50 noise variables\n  rep(1, 2),  # scenario 8, 100 noise variables\n  # Third category (same as baseline but with 5 num. and 5 categ. covariates)\n  rep(1, 5),\n  rep(1, 5),\n  rep(1, 5),\n  rep(1, 5),\n  # Fourth category (nonlinear predictor, 3 covariates)\n  rep(1, 3),\n  rep(1, 3),\n  rep(1, 3),\n  rep(1, 3)\n)\n\nparams_df &lt;- tibble(\n  scenario = 1:16,\n  coefficients = coefficients,\n  n_num = c(rep(2, 8), rep(5, 4), rep(3, 4)),\n  add_categ = c(rep(FALSE, 8), rep(TRUE, 4), rep(FALSE, 4)),\n  n_noise = rep(c(0, 10, 50, 100), 4),\n  mean_num = mean_num,\n  sd_num = sd_num,\n  size_train = rep(10000, 16),\n  size_calib = rep(10000, 16),\n  size_valid = rep(10000, 16),\n  size_test = rep(10000, 16),\n  transform_probs = c(rep(FALSE, 4), rep(TRUE, 4), rep(FALSE, 4), rep(FALSE, 4)),\n  linear_predictor = c(rep(TRUE, 12), rep(FALSE, 4)),\n  seed = 202105\n)\nrm(coefficients, mean_num, sd_num)",
    "crumbs": [
      "III. Simulated Data",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Simulated Data</span>"
    ]
  },
  {
    "objectID": "simul-data.html#example",
    "href": "simul-data.html#example",
    "title": "4  Simulated Data",
    "section": "4.3 Example",
    "text": "4.3 Example\nLet us draw a sample of 10,000 observation in each set (train, calibration, validation, test), for each scenario. We can then plot the histogram of the true probabilities in each sample (Figure 4.1).\n\n\nCode\npar(mfrow = c(16, 4), mar = c(2, 2, 2, 2))\nfor (scenario in 1:16) {\n  simu_data &lt;- simulate_data_wrapper(\n    scenario = scenario,\n    params_df = params_df,\n    repn = repns_vector[1] # only one replication here\n  )\n  colour_samples &lt;- c(\n    \"Train\" = \"#0072B2\",\n    \"Calib\" = \"#CC79A7\",\n    \"Valid\" = \"#009E73\",\n    \"Test\" = \"#D55E00\"\n  )\n  for (sample in c(\"train\", \"calib\", \"valid\", \"test\")) {\n    if (sample == \"train\") {\n      true_prob &lt;- simu_data$data$probs_train\n      i_scores &lt;- 1\n    } else if (sample == \"calib\") {\n      true_prob &lt;- simu_data$data$probs_calib\n      i_scores &lt;- 2\n    } else if (sample == \"valid\") {\n      true_prob &lt;- simu_data$data$probs_valid\n      i_scores &lt;- 3\n    } else {\n      true_prob &lt;- simu_data$data$probs_test\n      i_scores &lt;- 4\n    }\n    hist(\n      true_prob,\n      breaks = seq(0, 1, by = .05),\n      col = colour_samples[i_scores],\n      border = \"white\",\n      xlab = \"p\", ylab = \"\",\n      main = str_c(\n        \"Scen. \", scenario, \", \", c(\"Train\", \"Calib\", \"Valid\", \"Test\")[i_scores]\n      ),\n      xlim = c(0, 1)\n    )\n  }\n}\n\n\n\n\n\nFigure 4.1: Histogram of true probabilities for each scenario\n\n\n\n\n\n\n\n\nFor each group of scenarios, the only thing that varies is the number of noise variables. This has no impact on the distribution of the true probability. Hence, we can create a simple figure with the distribution of the true probability for each group of scenario.\n\n\nOnly on Train test, for each category of scenarios.\nsave_graph &lt;- FALSE\n\nif (save_graph) {\n  cairo_pdf(\n    \"../figs/sim-data-hist-categories.pdf\", \n    width = 8, height = 2\n  )\n}\n\npar(mfrow = c(1, 4), mar = c(4.1, 3.1, 2.1, 1.1))\nfor (i_dgp in 1:4) {\n  scenario &lt;- c(1, 5, 9, 13)[i_dgp]\n  simu_data &lt;- simulate_data_wrapper(\n    scenario = scenario,\n    params_df = params_df,\n    repn = repns_vector[1] # only one replication here\n  )\n  \n  true_prob &lt;- simu_data$data$probs_test\n  title &lt;- str_c(\"DGP \", i_dgp)\n  \n  hist(\n    true_prob,\n    breaks = seq(0, 1, by = .05),\n    # col = ,\n    # border = \"white\",\n    xlab = \"p\", ylab = \"\",\n    main = title,\n    xlim = c(0, 1)\n  )\n}\n\n\nWarning in ks.test.default(probs_01, fun): ties should not be present for the\none-sample Kolmogorov-Smirnov test\n\n\nOnly on Train test, for each category of scenarios.\nif (save_graph) dev.off()\n\n\n\n\n\nFigure 4.2: Distribution of the underlying probabilities in the different categories of scenarios.\n\n\n\n\n\n\n\n\n\n\n\n\nOjeda, Francisco M., Max L. Jansen, Alexandre Thiéry, Stefan Blankenberg, Christian Weimar, Matthias Schmid, and Andreas Ziegler. 2023. “Calibrating Machine Learning Approaches for Probability Estimation: A Comprehensive Comparison.” Statistics in Medicine 42 (29): 5451–78. https://doi.org/10.1002/sim.9921.",
    "crumbs": [
      "III. Simulated Data",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Simulated Data</span>"
    ]
  },
  {
    "objectID": "simul-xgb.html",
    "href": "simul-xgb.html",
    "title": "5  Extreme Gradient Boosting",
    "section": "",
    "text": "5.1 Data",
    "crumbs": [
      "III. Simulated Data",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Extreme Gradient Boosting</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Austin, Peter C., and Ewout W. Steyerberg. 2019. “The Integrated\nCalibration Index (ICI) and Related Metrics for Quantifying the\nCalibration of Logistic Regression Models.” Statistics in\nMedicine 38 (21): 4051–65. https://doi.org/10.1002/sim.8281.\n\n\nBrier, Glenn W. 1950. “Verification of Forecasts Expressed in\nTerms of Probability.” Monthly Weather Review 78 (1):\n1–3.\n\n\nCaffo, Brian S, James G Booth, and AC Davison. 2002. “Empirical\nSupremum Rejection Sampling.” Biometrika 89 (4): 745–54.\n\n\nChen, Song Xi. 1999. “Beta Kernel Estimators for Density\nFunctions.” Computational Statistics & Data Analysis\n31 (2): 131–45.\n\n\nKullback, S., and R. A. Leibler. 1951. “On Information and\nSufficiency.” The Annals of Mathematical Statistics 22\n(1): 79–86. https://doi.org/10.1214/aoms/1177729694.\n\n\nOjeda, Francisco M., Max L. Jansen, Alexandre Thiéry, Stefan\nBlankenberg, Christian Weimar, Matthias Schmid, and Andreas Ziegler.\n2023. “Calibrating Machine Learning Approaches for Probability\nEstimation: A Comprehensive Comparison.” Statistics in\nMedicine 42 (29): 5451–78. https://doi.org/10.1002/sim.9921.\n\n\nRumbell, Timothy, Jaimit Parikh, James Kozloski, and Viatcheslav Gurev.\n2023. “Novel and Flexible Parameter Estimation Methods for\nData-Consistent Inversion in Mechanistic Modelling.” Royal\nSociety Open Science 10 (11): 230668.",
    "crumbs": [
      "References"
    ]
  }
]