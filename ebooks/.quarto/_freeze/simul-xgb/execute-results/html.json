{
  "hash": "278b9369c6ab26f6545676ac3b9e6f66",
  "result": {
    "engine": "knitr",
    "markdown": "# Extreme Gradient Boosting {#sec-simul-xgb}\n\n\n:::{.callout-note}\n\nThis chapter investigates how the distribution of estimated scores by an extreme gradient boosting model evolves with the number of boosting iterations. In the models we train, we vary the maximum depth of trees and consider boosting iterations up to 400. For each configuration, we compute the predicted scores from iteration 1 to 400; for each boosting iteration, we use the predicted scores (both on train set and on test set) to compute various metrics (performance, calibration, divergence between the distribution of scores and that of true underlying probabilities).\n\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(ggh4x)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'ggh4x'\n\nThe following object is masked from 'package:ggplot2':\n\n    guide_axis_logticks\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(ggrepel)\nlibrary(rpart)\nlibrary(locfit)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nlocfit 1.5-9.9 \t 2024-03-01\n\nAttaching package: 'locfit'\n\nThe following object is masked from 'package:purrr':\n\n    none\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(philentropy)\n\n# Colours for train/validation/test\ncolour_samples <- c(\n  \"Train\" = \"#0072B2\",\n  \"Validation\" = \"#009E73\",\n  \"Calibration\" = \"#CC79A7\",\n  \"Test\" = \"#D55E00\"\n)\n\ncolour_recalib <- c(\n  \"None\" = \"#88CCEE\",\n  \"Platt\" = \"#44AA99\",\n  \"Isotonic\" = \"#882255\"\n)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"definition of the `theme_paper()` function (for ggplot2 graphs)\"}\n#' Theme for ggplot2\n#'\n#' @param ... arguments passed to the theme function\n#' @export\n#' @importFrom ggplot2 element_rect element_text element_blank element_line unit\n#'   rel\ntheme_paper <- function (...) {\n  ggthemes::theme_base() +\n    theme(\n      plot.background = element_blank(),\n      legend.background = element_rect(\n        fill = \"transparent\", linetype=\"solid\", colour =\"black\"),\n      legend.position = \"bottom\",\n      legend.direction = \"horizontal\",\n      legend.box = \"horizontal\",\n      legend.key = element_blank()\n    )\n}\n```\n:::\n\n\n## Data\n\n\nWe generate data using the first 12 scenarios from @Ojeda_2023 and an additional set of 4 scenarios in which the true probability does not depend on the predictors in a linear way (see [Chapter -@sec-simul-data]).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsource(\"../scripts/functions/simul-data.R\")\nlibrary(ks)\nsource(\"../scripts/functions/subsample_target_distribution.R\")\n```\n:::\n\n\nWhen we simulate a dataset, we draw the following number of observations:\n\n::: {.cell}\n\n```{.r .cell-code}\nnb_obs <- 10000\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Definition of the 16 scenarios\"}\n# Coefficients beta\ncoefficients <- list(\n  # First category (baseline, 2 covariates)\n  c(0.5, 1),  # scenario 1, 0 noise variable\n  c(0.5, 1),  # scenario 2, 10 noise variables\n  c(0.5, 1),  # scenario 3, 50 noise variables\n  c(0.5, 1),  # scenario 4, 100 noise variables\n  # Second category (same as baseline, with lower number of 1s)\n  c(0.5, 1),  # scenario 5, 0 noise variable\n  c(0.5, 1),  # scenario 6, 10 noise variables\n  c(0.5, 1),  # scenario 7, 50 noise variables\n  c(0.5, 1),  # scenario 8, 100 noise variables\n  # Third category (same as baseline but with 5 num. and 5 categ. covariates)\n  c(0.1, 0.2, 0.3, 0.4, 0.5, 0.01, 0.02, 0.03, 0.04, 0.05),\n  c(0.1, 0.2, 0.3, 0.4, 0.5, 0.01, 0.02, 0.03, 0.04, 0.05),\n  c(0.1, 0.2, 0.3, 0.4, 0.5, 0.01, 0.02, 0.03, 0.04, 0.05),\n  c(0.1, 0.2, 0.3, 0.4, 0.5, 0.01, 0.02, 0.03, 0.04, 0.05),\n  # Fourth category (nonlinear predictor, 3 covariates)\n  c(0.5, 1, .3),  # scenario 5, 0 noise variable\n  c(0.5, 1, .3),  # scenario 6, 10 noise variables\n  c(0.5, 1, .3),  # scenario 7, 50 noise variables\n  c(0.5, 1, .3)  # scenario 8, 100 noise variables\n)\n\n# Mean parameter for the normal distribution to draw from to draw num covariates\nmean_num <- list(\n  # First category (baseline, 2 covariates)\n  rep(0, 2),  # scenario 1, 0 noise variable\n  rep(0, 2),  # scenario 2, 10 noise variables\n  rep(0, 2),  # scenario 3, 50 noise variables\n  rep(0, 2),  # scenario 4, 100 noise variables\n  # Second category (same as baseline, with lower number of 1s)\n  rep(0, 2),  # scenario 5, 0 noise variable\n  rep(0, 2),  # scenario 6, 10 noise variables\n  rep(0, 2),  # scenario 7, 50 noise variables\n  rep(0, 2),  # scenario 8, 100 noise variables\n  # Third category (same as baseline but with 5 num. and 5 categ. covariates)\n  rep(0, 5),\n  rep(0, 5),\n  rep(0, 5),\n  rep(0, 5),\n  # Fourth category (nonlinear predictor, 3 covariates)\n  rep(0, 3),\n  rep(0, 3),\n  rep(0, 3),\n  rep(0, 3)\n)\n# Sd parameter for the normal distribution to draw from to draw num covariates\nsd_num <- list(\n  # First category (baseline, 2 covariates)\n  rep(1, 2),  # scenario 1, 0 noise variable\n  rep(1, 2),  # scenario 2, 10 noise variables\n  rep(1, 2),  # scenario 3, 50 noise variables\n  rep(1, 2),  # scenario 4, 100 noise variables\n  # Second category (same as baseline, with lower number of 1s)\n  rep(1, 2),  # scenario 5, 0 noise variable\n  rep(1, 2),  # scenario 6, 10 noise variables\n  rep(1, 2),  # scenario 7, 50 noise variables\n  rep(1, 2),  # scenario 8, 100 noise variables\n  # Third category (same as baseline but with 5 num. and 5 categ. covariates)\n  rep(1, 5),\n  rep(1, 5),\n  rep(1, 5),\n  rep(1, 5),\n  # Fourth category (nonlinear predictor, 3 covariates)\n  rep(1, 3),\n  rep(1, 3),\n  rep(1, 3),\n  rep(1, 3)\n)\n\nparams_df <- tibble(\n  scenario = 1:16,\n  coefficients = coefficients,\n  n_num = c(rep(2, 8), rep(5, 4), rep(3, 4)),\n  add_categ = c(rep(FALSE, 8), rep(TRUE, 4), rep(FALSE, 4)),\n  n_noise = rep(c(0, 10, 50, 100), 4),\n  mean_num = mean_num,\n  sd_num = sd_num,\n  size_train = rep(nb_obs, 16),\n  size_valid = rep(nb_obs, 16),\n  size_calib = rep(nb_obs, 16),\n  size_test = rep(nb_obs, 16),\n  transform_probs = c(rep(FALSE, 4), rep(TRUE, 4), rep(FALSE, 4), rep(FALSE, 4)),\n  linear_predictor = c(rep(TRUE, 12), rep(FALSE, 4)),\n  seed = 202105\n)\nrm(coefficients, mean_num, sd_num)\n```\n:::\n\n\n## Metrics\n\nWe load the functions from [Chapter -@sec-metrics] to compute performance, calibration and divergence metrics.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsource(\"../scripts/functions/metrics.R\")\n```\n:::\n\n\n## Simulations Setup\n\n\nTo train the models, we rely on the {xgboost} R package.\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(xgboost)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'xgboost'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following object is masked from 'package:dplyr':\n\n    slice\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n#' Recalibrates scores using a calibration\n#' \n#' @param obs_calib vector of observed events in the calibration set\n#' @param scores_calib vector of predicted probabilities in the calibration set\n#' #' @param obs_test vector of observed events in the test set\n#' @param scores_test vector of predicted probabilities in the test set\n#' @param method recalibration method (`\"platt\"` for Platt-Scaling, \n#'   `\"isotonic\"` for isotonic regression)\n#' @returns list of two elements: recalibrated scores on the calibration set,\n#'   recalibrated scores on the test set\nrecalibrate <- function(obs_calib,\n                        obs_test,\n                        pred_calib,\n                        pred_test,\n                        method = c(\"platt\", \"isotonic\")) {\n  data_calib <- tibble(d = obs_calib, scores = pred_calib)\n  data_test <- tibble(d = obs_test, scores = pred_test)\n  \n  if (method == \"platt\") {\n    lr <- glm(d ~ scores, family = binomial(link = 'logit'), data = data_calib)\n    score_c_calib <- predict(lr, newdata = data_calib, type = \"response\")\n    score_c_test <- predict(lr, newdata = data_test, type = \"response\")\n  } else if (method == \"isotonic\") {\n    iso <- isoreg(x = data_calib$scores, y = data_calib$d)\n    fit_iso <- as.stepfun(iso)\n    score_c_calib <- fit_iso(data_calib$scores)\n    score_c_test <- fit_iso(data_test$scores)\n    \n  } else {\n    stop(\"Unrecognized method: platt or isotonic only\")\n  }\n  # Format results in tibbles:\n  # For calibration set\n  tb_score_c_calib <- tibble(\n    d = obs_calib,\n    p_u = pred_calib,\n    p_c = score_c_calib\n  )\n  # For test set\n  tb_score_c_test <- tibble(\n    d = obs_test,\n    p_u = pred_test,\n    p_c = score_c_test\n  )\n  \n  list(\n    tb_score_c_calib = tb_score_c_calib,\n    tb_score_c_test = tb_score_c_test\n  )\n  \n}\n```\n:::\n\n\n\nAs explained in the foreword of this page, we compute metrics based on scores obtained at various boosting iterations. To do so, we define a function, `get_metrics_nb_iter()`{.R}, that will be applied to a fitted model. This function will be called for all the boosting iterations (controlled by the `nb_iter` argument). The function returns a list with the following elements:\n\n- `scenario`: the ID of the scenario\n- `ind`: the index of the grid search (so that we can join with the hyperparameters values, if needed)\n- `repn`: the ID of the replication\n- `nb_iter`: the boosting iteration at which the metrics are computed\n- `tb_metrics`: the tibble with the performance, calibration, and divergence metrics (one row for the train sample, one row for the calibration sample, one row for the validation sample, and one row for the test sample)\n- `tb_prop_scores`: additional metrics ($\\mathbb{P}(q_1 < \\hat{s}(\\mathbf{x}) < q_2)$ for multiple values for $q_1$ and $q_2 = 1-q_1$)\n- `scores_hist`: elements to be able to plot an histogram of the scores on both the train set and the test set (using 20 equally-sized bins over $[0,1]$).\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Function `get_metrics_nb_iter()`{.R}\"}\n#' Computes the performance and calibration metrics for an xgb model,\n#' depending on the number of iterations kept.\n#'\n#' @param nb_iter number of boosting iterations to keep\n#' @param params hyperparameters of the current model\n#' @param fitted_xgb xgb estimated model\n#' @param tb_train_xgb train data (in xgb.DMatrix format)\n#' @param tb_valid_xgb validation data (in xgb.DMatrix format)\n#' @param tb_calib_xgb calibration data (in xgb.DMatrix format)\n#' @param tb_test_xgb test data (in xgb.DMatrix format)\n#' @param simu_data simulated dataset\n#' @param true_prob list with true probabilities on train, calibration,\n#'  validation and test sets\nget_metrics_nb_iter <- function(nb_iter,\n                                params,\n                                fitted_xgb,\n                                tb_train_xgb,\n                                tb_valid_xgb,\n                                tb_calib_xgb,\n                                tb_test_xgb,\n                                simu_data,\n                                true_prob) {\n\n  ind <- params$ind\n  max_depth <- params$max_depth\n  tb_train <- simu_data$data$train |> rename(d = y)\n  tb_valid <- simu_data$data$valid |> rename(d = y)\n  tb_calib <- simu_data$data$calib |> rename(d = y)\n  tb_test <- simu_data$data$test |> rename(d = y)\n\n  # Predicted scores\n  scores_train <- predict(fitted_xgb, tb_train_xgb, iterationrange = c(1, nb_iter))\n  scores_valid <- predict(fitted_xgb, tb_valid_xgb, iterationrange = c(1, nb_iter))\n  scores_calib <- predict(fitted_xgb, tb_calib_xgb, iterationrange = c(1, nb_iter))\n  scores_test <- predict(fitted_xgb, tb_test_xgb, iterationrange = c(1, nb_iter))\n  \n  # Recalibration\n  # Platt scaling\n  res_recalibration_platt <- recalibrate(\n    obs_calib = tb_calib$d, \n    obs_test = tb_test$d, \n    pred_calib = scores_calib, \n    pred_test = scores_test, \n    method = \"platt\"\n  )\n  scores_c_platt_calib <- res_recalibration_platt$tb_score_c_calib$p_c\n  scores_c_platt_test <- res_recalibration_platt$tb_score_c_test$p_c\n  \n  # Isotonic regression\n  res_recalibration_iso <- recalibrate(\n    obs_calib = tb_calib$d, \n    obs_test = tb_test$d, \n    pred_calib = scores_calib, \n    pred_test = scores_test, \n    method = \"isotonic\"\n  )\n  scores_c_iso_calib <- res_recalibration_iso$tb_score_c_calib$p_c\n  scores_c_iso_test <- res_recalibration_iso$tb_score_c_test$p_c\n\n  ## Histogram of scores----\n  breaks <- seq(0, 1, by = .05)\n  scores_train_hist <- hist(scores_train, breaks = breaks, plot = FALSE)\n  scores_calib_hist <- hist(scores_calib, breaks = breaks, plot = FALSE)\n  scores_valid_hist <- hist(scores_valid, breaks = breaks, plot = FALSE)\n  scores_test_hist <- hist(scores_test, breaks = breaks, plot = FALSE)\n  scores_c_platt_calib_hist <- hist(scores_c_platt_calib, breaks = breaks, plot = FALSE)\n  scores_c_platt_test_hist <- hist(scores_c_platt_test, breaks = breaks, plot = FALSE)\n  scores_c_iso_calib_hist <- hist(scores_c_iso_calib, breaks = breaks, plot = FALSE)\n  scores_c_iso_test_hist <- hist(scores_c_iso_test, breaks = breaks, plot = FALSE)\n  \n  scores_hist <- list(\n    train = scores_train_hist,\n    valid = scores_valid_hist,\n    calib = scores_calib_hist,\n    test = scores_test_hist,\n    calib_c_platt = scores_c_platt_calib_hist,\n    test_c_platt = scores_c_platt_test_hist,\n    calib_c_iso = scores_c_iso_calib_hist,\n    test_c_iso = scores_c_iso_test_hist,\n    scenario = simu_data$scenario,\n    ind = ind,\n    repn = simu_data$repn,\n    max_depth = params$max_depth,\n    nb_iter = nb_iter\n  )\n\n  ## Estimation of P(q1 < score < q2)----\n  prop_btw_q_h <- function(s, sample_name, recalib_name) {\n    map(\n      c(.1, .2, .3, .4),\n      ~prop_btw_quantiles(s = s, q1 = .x)\n    ) |>\n      list_rbind() |>\n      mutate(sample = sample_name, recalib = recalib_name)\n  }\n  \n  proq_scores_train <- prop_btw_q_h(\n    scores_train, sample_name = \"train\", recalib_name = \"none\"\n  )\n  proq_scores_valid <- prop_btw_q_h(\n    scores_valid, sample_name = \"valid\", recalib_name = \"none\"\n  )\n  proq_scores_calib <- prop_btw_q_h(\n    scores_calib, sample_name = \"calib\", recalib_name = \"none\"\n  )\n  proq_scores_test <- prop_btw_q_h(\n    scores_test, sample_name = \"test\", recalib_name = \"none\"\n  )\n  proq_scores_c_platt_calib <- prop_btw_q_h(\n    scores_c_platt_calib, sample_name = \"calib\", recalib_name = \"platt\"\n  )\n  proq_scores_c_platt_test <- prop_btw_q_h(\n    scores_c_platt_test, sample_name = \"test\", recalib_name = \"platt\"\n  )\n  proq_scores_c_iso_calib <- prop_btw_q_h(\n    scores_c_iso_calib, sample_name = \"calib\", recalib_name = \"isotonic\"\n  )\n  proq_scores_c_iso_test <- prop_btw_q_h(\n    scores_c_iso_test, sample_name = \"test\", recalib_name = \"isotonic\"\n  )\n  \n\n  ## Dispersion Metrics----\n  disp_train <- dispersion_metrics(\n    true_probas = true_prob$train, scores = scores_train\n  ) |> \n    mutate(sample = \"train\", recalib = \"none\")\n  disp_valid <- dispersion_metrics(\n    true_probas = true_prob$valid, scores = scores_valid\n  ) |>\n    mutate(sample = \"valid\", recalib = \"none\")\n  \n  disp_calib <- dispersion_metrics(\n    true_probas = true_prob$calib, scores = scores_calib\n  ) |>\n    mutate(sample = \"calib\", recalib = \"none\")\n  \n  disp_test <- dispersion_metrics(\n    true_probas = true_prob$test, scores = scores_test\n  ) |> \n    mutate(sample = \"test\", recalib = \"none\")\n  \n  \n  disp_c_platt_calib <- dispersion_metrics(\n    true_probas = true_prob$calib, scores = scores_c_platt_calib\n  ) |>\n    mutate(sample = \"calib\", recalib = \"platt\")\n  \n  disp_c_platt_test <- dispersion_metrics(\n    true_probas = true_prob$test, scores = scores_c_platt_test\n  ) |> \n    mutate(sample = \"test\", recalib = \"platt\")\n  \n  disp_c_iso_calib <- dispersion_metrics(\n    true_probas = true_prob$calib, scores = scores_c_iso_calib\n  ) |>\n    mutate(sample = \"calib\", recalib = \"isotonic\")\n  \n  disp_c_iso_test <- dispersion_metrics(\n    true_probas = true_prob$test, scores = scores_c_iso_test\n  ) |> \n    mutate(sample = \"test\", recalib = \"isotonic\")\n  \n  # Performance and Calibration Metrics\n  # We add very small noise to predicted scores\n  # otherwise the local regression may crash\n  scores_train_noise <- scores_train +\n    runif(n = length(scores_train), min = 0, max = 0.01)\n  scores_train_noise[scores_train_noise > 1] <- 1\n  metrics_train <- compute_metrics(\n    obs = tb_train$d, scores = scores_train_noise, true_probas = true_prob$train\n  ) |> mutate(sample = \"train\", recalib = \"none\")\n  \n  scores_valid_noise <- scores_valid +\n    runif(n = length(scores_valid), min = 0, max = 0.01)\n  scores_valid_noise[scores_valid_noise > 1] <- 1\n  metrics_valid <- compute_metrics(\n    obs = tb_valid$d, scores = scores_valid_noise, true_probas = true_prob$valid\n  ) |> mutate(sample = \"valid\", recalib = \"none\")\n  \n  scores_calib_noise <- scores_calib +\n    runif(n = length(scores_calib), min = 0, max = 0.01)\n  scores_calib_noise[scores_calib_noise > 1] <- 1\n  metrics_calib <- compute_metrics(\n    obs = tb_calib$d, scores = scores_calib_noise, true_probas = true_prob$calib\n  ) |> mutate(sample = \"calib\", recalib = \"none\")\n  \n  scores_test_noise <- scores_test +\n    runif(n = length(scores_test), min = 0, max = 0.01)\n  scores_test_noise[scores_test_noise > 1] <- 1\n  metrics_test <- compute_metrics(\n    obs = tb_test$d, scores = scores_test_noise, true_probas = true_prob$test\n  ) |> mutate(sample = \"test\", recalib = \"none\")\n  \n  # With recalibrated scores (platt)\n  scores_c_platt_calib_noise <- scores_c_platt_calib +\n    runif(n = length(scores_c_platt_calib), min = 0, max = 0.01)\n  scores_c_platt_calib_noise[scores_c_platt_calib_noise > 1] <- 1\n  metrics_c_platt_calib <- compute_metrics(\n    obs = tb_calib$d, scores = scores_c_platt_calib_noise, \n    true_probas = true_prob$calib\n  ) |> mutate(sample = \"calib\", recalib = \"platt\")\n  \n  scores_c_platt_test_noise <- scores_c_platt_test +\n    runif(n = length(scores_c_platt_test), min = 0, max = 0.01)\n  scores_c_platt_test_noise[scores_c_platt_test_noise > 1] <- 1\n  metrics_c_platt_test <- compute_metrics(\n    obs = tb_test$d, scores = scores_c_platt_test_noise, \n    true_probas = true_prob$test\n  ) |> mutate(sample = \"test\", recalib = \"platt\")\n  \n  # With recalibrated scores (isotonic)\n  scores_c_iso_calib_noise <- scores_c_iso_calib +\n    runif(n = length(scores_c_iso_calib), min = 0, max = 0.01)\n  scores_c_iso_calib_noise[scores_c_iso_calib_noise > 1] <- 1\n  metrics_c_iso_calib <- compute_metrics(\n    obs = tb_calib$d, scores = scores_c_iso_calib_noise, \n    true_probas = true_prob$calib\n  ) |> mutate(sample = \"calib\", recalib = \"isotonic\")\n  \n  scores_c_iso_test_noise <- scores_c_iso_test +\n    runif(n = length(scores_c_iso_test), min = 0, max = 0.01)\n  scores_c_iso_test_noise[scores_c_iso_test_noise > 1] <- 1\n  metrics_c_iso_test <- compute_metrics(\n    obs = tb_test$d, scores = scores_c_iso_test_noise, \n    true_probas = true_prob$test\n  ) |> mutate(sample = \"test\", recalib = \"isotonic\")\n  \n  tb_metrics <- metrics_train |>\n    bind_rows(metrics_valid) |>\n    bind_rows(metrics_calib) |>\n    bind_rows(metrics_test) |>\n    bind_rows(metrics_c_platt_calib) |>\n    bind_rows(metrics_c_platt_test) |>\n    bind_rows(metrics_c_iso_calib) |>\n    bind_rows(metrics_c_iso_test) |>\n    left_join(\n      disp_train |>\n        bind_rows(disp_valid) |> \n        bind_rows(disp_calib) |> \n        bind_rows(disp_test) |> \n        bind_rows(disp_c_platt_calib) |> \n        bind_rows(disp_c_platt_test) |> \n        bind_rows(disp_c_iso_calib) |> \n        bind_rows(disp_c_iso_test),\n      by = c(\"sample\", \"recalib\")\n    ) |>\n    mutate(\n      scenario = simu_data$scenario,\n      ind = ind,\n      repn = simu_data$repn,\n      max_depth = params$max_depth,\n      nb_iter = nb_iter\n    )\n  \n  tb_prop_scores <- proq_scores_train |>\n    bind_rows(proq_scores_valid) |>\n    bind_rows(proq_scores_calib) |>\n    bind_rows(proq_scores_test) |>\n    bind_rows(proq_scores_c_platt_calib) |>\n    bind_rows(proq_scores_c_platt_test) |>\n    bind_rows(proq_scores_c_iso_calib) |>\n    bind_rows(proq_scores_c_iso_test) |>\n    mutate(\n      scenario = simu_data$scenario,\n      ind = ind,\n      repn = simu_data$repn,\n      max_depth = params$max_depth,\n      nb_iter = nb_iter\n    )\n\n  list(\n    scenario = simu_data$scenario,     # data scenario\n    ind = ind,                         # index for grid\n    repn = simu_data$repn,             # data replication ID\n    nb_iter = nb_iter,                 # number of boosting iterations\n    tb_metrics = tb_metrics,           # table with performance/calib/divergence\n                                       #  metrics\n    tb_prop_scores = tb_prop_scores,   # table with P(q1 < score < q2)\n    scores_hist = scores_hist          # histogram of scores\n  )\n}\n```\n:::\n\n\n\n\nWe define another function, `simul_xgb()`{.R} which trains an extreme gradient boosting model for a single replication. It calls the `get_metrics_nb_iter()`{.R} on each of the boosting iterations of the model from the second to the last (400th), and returns a list of length 400-1 where each element is a list returned by the `get_metrics_nb_iter()`{.R}.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Function `simul_xgb()`{.R}\"}\n#' Train an xgboost model and compute performance, calibration, and dispersion\n#' metrics\n#'\n#' @param params tibble with hyperparameters for the simulation\n#' @param ind index of the grid (numerical ID)\n#' @param simu_data simulated data obtained with `simulate_data_wrapper()`\nsimul_xgb <- function(params,\n                      ind,\n                      simu_data) {\n  tb_train <- simu_data$data$train |> rename(d = y)\n  tb_valid <- simu_data$data$valid |> rename(d = y)\n  tb_calib <- simu_data$data$calib |> rename(d = y)\n  tb_test <- simu_data$data$test |> rename(d = y)\n  true_prob <-\n    list(\n      train = simu_data$data$probs_train,\n      valid = simu_data$data$probs_valid,\n      calib = simu_data$data$probs_calib,\n      test = simu_data$data$probs_test\n    )\n\n  ## Format data for xgboost----\n  tb_train_xgb <- xgb.DMatrix(\n    data = model.matrix(d ~ -1 + ., tb_train), label = tb_train$d\n  )\n  tb_valid_xgb <- xgb.DMatrix(\n    data = model.matrix(d ~ -1 + ., tb_valid), label = tb_valid$d\n  )\n  tb_calib_xgb <- xgb.DMatrix(\n    data = model.matrix(d ~ -1 + ., tb_calib), label = tb_calib$d\n  )\n  tb_test_xgb <- xgb.DMatrix(\n    data = model.matrix(d ~ -1 + ., tb_test), label = tb_test$d\n  )\n  # Parameters for the algorithm\n  param <- list(\n    max_depth = params$max_depth, #Note: root node is indexed 0\n    eta = params$eta,\n    nthread = 1,\n    objective = \"binary:logistic\",\n    eval_metric = \"auc\"\n  )\n  watchlist <- list(train = tb_train_xgb, eval = tb_valid_xgb)\n\n  ## Estimation----\n  xgb_fit <- xgb.train(\n    param, tb_train_xgb,\n    nrounds = params$nb_iter_total,\n    watchlist,\n    verbose = 0\n  )\n\n  # Then, for each boosting iteration number up to params$nb_iter_total\n  # compute the predicted scores and evaluate the metrics\n  resul <- map(\n    seq(2, params$nb_iter_total),\n    ~get_metrics_nb_iter(\n      nb_iter = .x,\n      params = params,\n      fitted_xgb = xgb_fit,\n      tb_train_xgb = tb_train_xgb,\n      tb_valid_xgb = tb_valid_xgb,\n      tb_calib_xgb = tb_calib_xgb,\n      tb_test_xgb = tb_test_xgb,\n      simu_data = simu_data,\n      true_prob = true_prob\n    ),\n  )\n  resul\n}\n\nsimulate_xgb_scenario <- function(scenario, params_df, repn) {\n  # Generate Data\n  simu_data <- simulate_data_wrapper(\n    scenario = scenario,\n    params_df = params_df,\n    repn = repn\n  )\n\n  # Looping over the grid hyperparameters for the scenario\n  res_simul <- vector(mode = \"list\", length = nrow(grid))\n  cli::cli_progress_bar(\"Iteration grid\", total = nrow(grid), type = \"tasks\")\n  for (j in 1:nrow(grid)) {\n    curent_params <- grid |> dplyr::slice(!!j)\n    res_simul[[j]] <- simul_xgb(\n      params = curent_params,\n      ind = curent_params$ind,\n      simu_data = simu_data\n    )\n    cli::cli_progress_update()\n  }\n\n\n  # The metrics computed for all set of hyperparameters (identified with `ind`)\n  # and for each number of boosting iterations (`nb_iter`), for the current\n  # scenario (`scenario`) and current replication number (`repn`)\n  metrics_simul <- map(\n    res_simul,\n    function(simul_grid_j) map(simul_grid_j, \"tb_metrics\") |> list_rbind()\n  ) |>\n    list_rbind()\n\n  # P(q_1<s(x)<q_2)\n  prop_scores_simul <- map(\n    res_simul,\n    function(simul_grid_j) map(simul_grid_j, \"tb_prop_scores\") |> list_rbind()\n  ) |>\n    list_rbind()\n\n  # Histogram of estimated scores\n  scores_hist <- map(\n    res_simul,\n    function(simul_grid_j) map(simul_grid_j, \"scores_hist\")\n  )\n\n  list(\n    metrics_simul = metrics_simul,\n    scores_hist = scores_hist,\n    prop_scores_simul = prop_scores_simul\n  )\n}\n```\n:::\n\n\n\n### Grid\n\nWe consider the following grid:\n\n::: {.cell}\n\n```{.r .cell-code}\ngrid <- expand_grid(\n  # max_depth = c(2, 4, 6),\n  max_depth = c(2),\n  nb_iter_total = 10,\n  eta = 0.3\n) |>\n  mutate(ind = row_number())\n```\n:::\n\n\nThe desired number of replications for each scenario needs to be set:\n\n::: {.cell}\n\n```{.r .cell-code}\nrepns_vector <- 1:100\n```\n:::\n\n\n\nThe different configurations are reported in @tbl-grid-values-xgb.\n\n\n::: {#tbl-grid-values-xgb .cell tbl-cap='Grid Search Values'}\n\n```{.r .cell-code}\nDT::datatable(grid)\n```\n\n::: {.cell-output-display}\n\n```{=html}\n<div class=\"datatables html-widget html-fill-item\" id=\"htmlwidget-7e4e71c0c1883bacca11\" style=\"width:100%;height:auto;\"></div>\n<script type=\"application/json\" data-for=\"htmlwidget-7e4e71c0c1883bacca11\">{\"x\":{\"filter\":\"none\",\"vertical\":false,\"data\":[[\"1\"],[2],[10],[0.3],[1]],\"container\":\"<table class=\\\"display\\\">\\n  <thead>\\n    <tr>\\n      <th> <\\/th>\\n      <th>max_depth<\\/th>\\n      <th>nb_iter_total<\\/th>\\n      <th>eta<\\/th>\\n      <th>ind<\\/th>\\n    <\\/tr>\\n  <\\/thead>\\n<\\/table>\",\"options\":{\"columnDefs\":[{\"className\":\"dt-right\",\"targets\":[1,2,3,4]},{\"orderable\":false,\"targets\":0},{\"name\":\" \",\"targets\":0},{\"name\":\"max_depth\",\"targets\":1},{\"name\":\"nb_iter_total\",\"targets\":2},{\"name\":\"eta\",\"targets\":3},{\"name\":\"ind\",\"targets\":4}],\"order\":[],\"autoWidth\":false,\"orderClasses\":false}},\"evals\":[],\"jsHooks\":[]}</script>\n```\n\n:::\n:::\n\n\n\nWe define a function, `simulate_xgb_scenario()`{.R} to train the model on a dataset for all different values of the hyperparameters of the grid. This function performs a single replication of the simulations for a single scenario.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Function `simulate_xgb_scenario()`{.R}\"}\nsimulate_xgb_scenario <- function(scenario, params_df, repn) {\n  # Generate Data\n  simu_data <- simulate_data_wrapper(\n    scenario = scenario,\n    params_df = params_df,\n    repn = repn\n  )\n\n  # Looping over the grid hyperparameters for the scenario\n  res_simul <- vector(mode = \"list\", length = nrow(grid))\n  cli::cli_progress_bar(\"Iteration grid\", total = nrow(grid), type = \"tasks\")\n  for (j in 1:nrow(grid)) {\n    curent_params <- grid |> dplyr::slice(!!j)\n    res_simul[[j]] <- simul_xgb(\n      params = curent_params,\n      ind = curent_params$ind,\n      simu_data = simu_data\n    )\n    cli::cli_progress_update()\n  }\n\n\n  # The metrics computed for all set of hyperparameters (identified with `ind`)\n  # and for each number of boosting iterations (`nb_iter`), for the current\n  # scenario (`scenario`) and current replication number (`repn`)\n  metrics_simul <- map(\n    res_simul,\n    function(simul_grid_j) map(simul_grid_j, \"tb_metrics\") |> list_rbind()\n  ) |>\n    list_rbind()\n\n  # Sanity check\n  # metrics_simul |> count(scenario, repn, ind, sample, nb_iter) |>\n  #   filter(n > 1)\n\n  # P(q_1<s(x)<q_2)\n  prop_scores_simul <- map(\n    res_simul,\n    function(simul_grid_j) map(simul_grid_j, \"tb_prop_scores\") |> list_rbind()\n  ) |>\n    list_rbind()\n\n  # Sanity check\n  # prop_scores_simul |> count(scenario, repn, ind, sample, nb_iter)\n\n  # Histogram of estimated scores\n  scores_hist <- map(\n    res_simul,\n    function(simul_grid_j) map(simul_grid_j, \"scores_hist\")\n  )\n\n  list(\n    metrics_simul = metrics_simul,\n    scores_hist = scores_hist,\n    prop_scores_simul = prop_scores_simul\n  )\n}\n```\n:::\n\n\n\n## Estimations\n\n\nWe loop over the 16 scenarios and run the 100 replications in parallel.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Estimation codes\"}\nlibrary(pbapply)\nlibrary(parallel)\nncl <- detectCores()-1\n(cl <- makeCluster(ncl))\n\nclusterEvalQ(cl, {\n  library(tidyverse)\n  library(locfit)\n  library(philentropy)\n  library(xgboost)\n  library(ks)\n}) |>\n  invisible()\n\nclusterExport(\n  cl, c(\n    # Functions\n    \"brier_score\",\n    \"compute_metrics\",\n    \"dispersion_metrics\",\n    \"prop_btw_quantiles\",\n    \"subset_target\",\n    \"simulate_data\",\n    \"simulate_data_wrapper\",\n    \"simul_xgb\",\n    \"simulate_xgb_scenario\",\n    \"get_metrics_nb_iter\",\n    \"recalibrate\",\n    # Objects\n    \"grid\",\n    \"params_df\",\n    \"repns_vector\"\n  )\n)\n\n# make directory if not existing\nif (!dir.exists(\"../output/simul/\")) {\n  dir.create(\"../output/simul/\", recursive = TRUE)\n}\n\nfor (i_scenario in 2:16) {\n  scenario <- i_scenario\n  print(str_c(\"Scenario \", scenario, \"/\", nrow(params_df)))\n  clusterExport(cl, c(\"scenario\"))\n  resul_xgb_scenario <-\n    pblapply(\n      1:length(repns_vector), function(i) simulate_xgb_scenario(\n        scenario = scenario, params_df = params_df, repn = repns_vector[i]\n      ),\n      cl = cl\n    )\n  save(\n    resul_xgb_scenario,\n    file = str_c(\"../output/simul/resul_xgb_scenario_\", scenario, \".rda\")\n  )\n}\nstopCluster(cl)\n```\n:::\n\n\n\nThe results can be loaded as follows:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nscenarios <- 1:16 \nfiles <- str_c(\n  \"../output/simul/resul_xgb_scenario_\", scenarios, \".rda\"\n)\nresul_xgb <- map(files[file.exists(files)], ~{load(.x) ; resul_xgb_scenario})\n```\n:::\n\n\n\n\nThe `resul_rf` object is of length 16: each element contains the simulations for a scenario. For each scenario, the elements are a list of length `max(repns_vector)`, i.e., the number of replications. Each replication gives, in a list, the following elements:\n\n- `metrics_simul`: the metrics (AUC, Calibration, KL Divergence, etc.) for each model from the grid search, for all boosting iterations\n- `scores_hist`: the counts on bins defined on estimated scores (on train, validation, calibration, and test sets ; for calibration and test sets, the counts are given with or without recalibration)\n- `prop_scores_simul`: the estimations of $\\mathbb{P}(q_1 < \\hat{\\mathbf{x}}< q_2)$ for various values of `q_1` and `q_2`.\n\n## Results\n\nWe can now extract some information from the results.\n\nWe first aggregate all the computed metrics performance/calibration/divergence in a single tibble, `metrics_xgb_all`.\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Codes to create the metrics table\"}\nmetrics_xgb_all <- map(\n  resul_xgb,\n  function(resul_xgb_sc) map(resul_xgb_sc, \"metrics_simul\") |> list_rbind()\n) |>\n  list_rbind() |>\n  mutate(\n    sample = factor(\n      sample,\n      levels = c(\"train\", \"valid\", \"calib\", \"test\"),\n      labels = c(\"Train\",\"Validation\", \"Calibration\" ,\"Test\")\n    ),\n    recalib = factor(\n      recalib,\n      levels = c(\"none\", \"platt\", \"isotonic\"),\n      labels = c(\"None\", \"Platt\", \"Isotonic\")\n    )\n  )\n\n# Sanity check\n# metrics_xgb_all |> count(scenario, ind, sample, nb_iter) |>\n#   filter(n != max(repns_vector))\n```\n:::\n\n\nFor each replication, we made some hyperparameters vary. Let us identify some models of interest:\n\n- `smallest`: model with the lowest number of boosting iteration\n- `largest`: model with the highest number of boosting iteration\n- `largest_auc`: model with the highest AUC on validation set\n- `lowest_mse`: model with the lowest MSE on validation set\n- `lowest_ici`: model with the lowest ICI on validation set\n- `lowest_kl`: model with the lowest KL Divergence on validation set\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n# Identify the model with the smallest number of boosting iterations\nsmallest_xgb <-\n  metrics_xgb_all |>\n  filter(sample == \"Validation\", recalib == \"None\") |>\n  group_by(scenario, repn) |>\n  arrange(nb_iter) |>\n  slice_head(n = 1) |>\n  select(scenario, repn, ind, nb_iter, recalib) |>\n  mutate(result_type = \"smallest\") |>\n  ungroup()\n\n# Identify the largest tree\nlargest_xgb <-\n  metrics_xgb_all |>\n  filter(sample == \"Validation\", recalib == \"None\") |>\n  group_by(scenario, repn) |>\n  arrange(desc(nb_iter)) |>\n  slice_head(n = 1) |>\n  select(scenario, repn, ind, nb_iter, recalib) |>\n  mutate(result_type = \"largest\") |>\n  ungroup()\n\n# Identify tree with highest AUC on test set\nhighest_auc_xgb <-\n  metrics_xgb_all |>\n  filter(sample == \"Validation\", recalib == \"None\") |>\n  group_by(scenario, repn) |>\n  arrange(desc(AUC)) |>\n  slice_head(n = 1) |>\n  select(scenario, repn, ind, nb_iter, recalib) |>\n  mutate(result_type = \"largest_auc\") |>\n  ungroup()\n\n# Identify tree with lowest MSE\nlowest_mse_xgb <-\n  metrics_xgb_all |>\n  filter(sample == \"Validation\", recalib == \"None\") |>\n  group_by(scenario, repn) |>\n  arrange(mse) |>\n  slice_head(n = 1) |>\n  select(scenario, repn, ind, nb_iter, recalib) |>\n  mutate(result_type = \"lowest_mse\") |>\n  ungroup()\n\n# Identify tree with lowest brier\nlowest_brier_xgb <-\n  metrics_xgb_all |>\n  filter(sample == \"Validation\", recalib == \"None\") |>\n  group_by(scenario, repn) |>\n  arrange(brier) |>\n  slice_head(n = 1) |>\n  select(scenario, repn, ind, nb_iter, recalib) |>\n  mutate(result_type = \"lowest_brier\") |>\n  ungroup()\n\n# Identify tree with lowest ICI\nlowest_ici_xgb <-\n  metrics_xgb_all |>\n  filter(sample == \"Validation\", recalib == \"None\") |>\n  group_by(scenario, repn) |>\n  arrange(ici) |>\n  slice_head(n = 1) |>\n  select(scenario, repn, ind, nb_iter, recalib) |>\n  mutate(result_type = \"lowest_ici\") |>\n  ungroup()\n\n# Identify tree with lowest KL\nlowest_kl_xgb <-\n  metrics_xgb_all |>\n  filter(sample == \"Validation\", recalib == \"None\") |>\n  group_by(scenario, repn) |>\n  arrange(KL_20_true_probas) |>\n  slice_head(n = 1) |>\n  select(scenario, repn, ind, nb_iter, recalib) |>\n  mutate(result_type = \"lowest_kl\") |>\n  ungroup()\n\n# Merge these\nmodels_of_interest_xgb <-\n  smallest_xgb |>\n  bind_rows(largest_xgb) |>\n  bind_rows(highest_auc_xgb) |>\n  bind_rows(lowest_mse_xgb) |>\n  bind_rows(lowest_brier_xgb) |>\n  bind_rows(lowest_ici_xgb) |>\n  bind_rows(lowest_kl_xgb)\n\n# Add metrics now\nmodels_of_interest_metrics <-\n  models_of_interest_xgb |>\n  left_join(\n    metrics_xgb_all,\n    by = c(\"scenario\", \"repn\", \"ind\", \"nb_iter\"),\n    relationship = \"many-to-many\" # (train, valid, calib, test)\n  )\n\n# Sanity check\n# models_of_interest_metrics |> count(scenario, sample, result_type)\n```\n:::\n\n\n### Metrics vs Number of Iterations\n\nWe define a function, `plot_metrics()`{.R} to plot selected metrics (AUC, ICI, and KL Divergence) as a function of the number of boosting iterations, for a given value for the hyperparameter `max_depth`. Each curve corresponds to a value of the maximal depth hyperparameter.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Function `plot_metrics()`{.R}\"}\nplot_metrics <- function(dgp, max_depth = 2) {\n  df_plot <-\n    metrics_xgb_all |>\n    mutate(\n      dgp = case_when(\n        scenario %in% 1:4 ~ 1,\n        scenario %in% 5:8 ~ 2,\n        scenario %in% 9:12 ~ 3,\n        scenario %in% 13:16 ~ 4\n      ),\n      no_noise = c(0, 10, 50, 100)[(scenario-1)%%4 + 1],\n      no_noise = factor(\n        no_noise,\n        levels = c(no_noise),\n        labels = str_c(no_noise, \" noise variables\")\n      )\n    ) |>\n    filter(dgp == !!dgp) |>\n    select(\n      dgp, no_noise, scenario, recalib, ind, sample, nb_iter, max_depth,\n      AUC, brier, ici, KL_20_true_probas\n    ) |>\n    pivot_longer(\n      cols = -c(dgp, no_noise, scenario, recalib, ind, sample, nb_iter, max_depth),\n      names_to = \"metric\", values_to = \"value\"\n    ) |>\n    group_by(\n      dgp, no_noise, scenario, recalib, ind, sample, nb_iter, max_depth, metric\n    ) |>\n    summarise(\n      value_lower = quantile(value, probs = 2.5/100),\n      value_upper = quantile(value, probs = 97.5/100),\n      value = mean(value)\n    ) |>\n    mutate(\n      max_depth = factor(max_depth),\n      metric = factor(\n        metric,\n        levels = c(\"AUC\", \"brier\", \"ici\", \"KL_20_true_probas\"),\n        labels = c(\"AUC\", \"brier\", \"ICI\", \"KL Divergence\")\n      )\n    ) |>\n    filter(max_depth %in% max_depth, sample %in% c(\"Calibration\", \"Test\"))\n  \n  ggplot(\n    data = df_plot,\n    mapping = aes(x = nb_iter, y = value)\n  ) +\n    geom_ribbon(\n      mapping = aes(\n        ymin = value_lower, ymax = value_upper,\n        fill = recalib\n      ),\n      alpha = .2\n    ) +\n    geom_line(mapping = aes(colour = recalib, linetype = recalib)) +\n    facet_grid(metric~no_noise+sample, scales = \"free_y\") +\n    labs(\n      x = \"Boosting Iterations\", y = NULL\n    ) +\n    scale_colour_manual(\n      \"Recalibration\", values = colour_recalib\n    ) +\n    scale_linetype_discrete(\"Recalibration\") +\n    scale_fill_manual(\"Recalibration\", values = colour_recalib) +\n    theme_paper()\n}\n```\n:::\n\n\n:::{.panel-tabset}\n#### DGP 1\n::::{.panel-tabset}\n##### Max Depth = 2\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nplot_metrics(dgp = 1, max_depth = 2)\n```\n\n::: {.cell-output-display}\n![Metrics for DGP 1](simul-xgb_files/figure-html/fig-xgb-metrics-1-1.png){#fig-xgb-metrics-1 width=960}\n:::\n:::\n\n::::\n:::\n::::\n\n\n\n",
    "supporting": [
      "simul-xgb_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"site_libs/htmltools-fill-0.5.8.1/fill.css\" rel=\"stylesheet\" />\n<script src=\"site_libs/htmlwidgets-1.6.4/htmlwidgets.js\"></script>\n<link href=\"site_libs/datatables-css-0.0.0/datatables-crosstalk.css\" rel=\"stylesheet\" />\n<script src=\"site_libs/datatables-binding-0.33/datatables.js\"></script>\n<script src=\"site_libs/jquery-3.6.0/jquery-3.6.0.min.js\"></script>\n<link href=\"site_libs/dt-core-1.13.6/css/jquery.dataTables.min.css\" rel=\"stylesheet\" />\n<link href=\"site_libs/dt-core-1.13.6/css/jquery.dataTables.extra.css\" rel=\"stylesheet\" />\n<script src=\"site_libs/dt-core-1.13.6/js/jquery.dataTables.min.js\"></script>\n<link href=\"site_libs/crosstalk-1.2.1/css/crosstalk.min.css\" rel=\"stylesheet\" />\n<script src=\"site_libs/crosstalk-1.2.1/js/crosstalk.min.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}