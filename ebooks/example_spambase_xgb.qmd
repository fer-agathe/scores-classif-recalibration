# Illustration on Abalone dataset II {#sec-real-example-xgb}

:::{.callout-note}

This chapter illustrates the application of the method using one example of real-world datasets. We have trained a GAMSEL model on a binary variable to estimate the underlying event probabilities using available covariates. We therefore consider the Beta prior as the "true probability distribution". Here, we train a certain XGBoost model (the hyperparameters are selected randomly on the validation sample), and predict it to train/validation/calibration/test sets. Recalibration models are fitted on the calibration sets and then applied to the test to calculate performance, dispersion and calibration metrics.

:::


:::{.callout-warning}

## Code Availability

The functions for data preprocessing, model estimation, and Beta distribution fitting are stored in `functions/real-data.R` and will be used in subsequent chapters.

:::

```{r load-libraries}
library(tidyverse)
library(philentropy)
library(ranger)
library(xgboost)
library(pbapply)
library(parallel)
library(gam)
library(gamsel)

# Colours for train/validation/calibration/test
colour_samples <- c(
  "Train" = "#0072B2",
  "Validation" = "#009E73",
  "Calibration" = "#CC79A7",
  "Test" = "#D55E00"
)

colour_recalib <- c(
  "None" = "#88CCEE",
  "Platt" = "#44AA99",
  "Isotonic" = "#882255"
)

# Functions
source("functions/real-data.R")
source("../scripts/functions/metrics.R")
```

```{r define-theme_paper}
#| code-fold: true
#| code-summary: definition of the `theme_paper()` function (for ggplot2 graphs)
#' Theme for ggplot2
#'
#' @param ... arguments passed to the theme function
#' @export
#' @importFrom ggplot2 element_rect element_text element_blank element_line unit
#'   rel
theme_paper <- function (...) {
  ggthemes::theme_base() +
    theme(
      plot.background = element_blank(),
      legend.background = element_rect(
        fill = "transparent", linetype="solid", colour ="black"),
      legend.position = "bottom",
      legend.direction = "horizontal",
      legend.box = "horizontal",
      legend.key = element_blank()
    )
}
```

## Setup

### XGBoost

To train the models, we rely on the {xgboost} R package.
```{r load-xgboost}
library(xgboost)
```
### Recalibration

Here, we define a function to recalibrate predicted scores using either Platt scaling or isotonic regression. The recalibration algorithm is first trained on the calibration set and then applied to both the calibration and test sets.

```{r define-recalibrate}
#' Recalibrates scores using a calibration
#' 
#' @param obs_calib vector of observed events in the calibration set
#' @param scores_calib vector of predicted probabilities in the calibration set
#' @param obs_test vector of observed events in the test set
#' @param scores_test vector of predicted probabilities in the test set
#' @param method recalibration method (`"platt"` for Platt scaling, 
#'   `"isotonic"` for isotonic regression)
#' @returns list of two elements: recalibrated scores on the calibration set,
#'   recalibrated scores on the test set
recalibrate <- function(obs_calib,
                        obs_test,
                        pred_calib,
                        pred_test,
                        method = c("platt", "isotonic")) {
  data_calib <- tibble(d = obs_calib, scores = pred_calib)
  data_test <- tibble(d = obs_test, scores = pred_test)
  
  if (method == "platt") {
    lr <- glm(d ~ scores, family = binomial(link = 'logit'), data = data_calib)
    score_c_calib <- predict(lr, newdata = data_calib, type = "response")
    score_c_test <- predict(lr, newdata = data_test, type = "response")
  } else if (method == "isotonic") {
    iso <- isoreg(x = data_calib$scores, y = data_calib$d)
    fit_iso <- as.stepfun(iso)
    score_c_calib <- fit_iso(data_calib$scores)
    score_c_test <- fit_iso(data_test$scores)
    
  } else {
    stop("Unrecognized method: platt or isotonic only")
  }
  # Format results in tibbles:
  # For calibration set
  tb_score_c_calib <- tibble(
    d = obs_calib,
    p_u = pred_calib,
    p_c = score_c_calib
  )
  # For test set
  tb_score_c_test <- tibble(
    d = obs_test,
    p_u = pred_test,
    p_c = score_c_test
  )
  
  list(
    tb_score_c_calib = tb_score_c_calib,
    tb_score_c_test = tb_score_c_test
  )
  
}
```

### Dispersion Metrics

We modify our `dispersion_metrics()`{.R} function to replace the vector of simulated true probabilities with the parameters of a Beta distribution. This adjustment allows us to compute the divergence between the model-estimated scores and the specified Beta distribution.

```{r}
#| code-fold: true
#| code-summary: Function `dispersion_metrics_beta()`{.R}
#' Computes the dispersion and divergence metrics for a vector of scores and
#' a Beta distribution
#'
#' @param shape_1 first parameter of the beta distribution
#' @param shape_2 second parameter of the beta distribution
#' @param scores predicted scores
#'
#' @returns
#' \itemize{
#'   \item \code{inter_quantile_25_75}: Difference of inter-quantile between 25% and 75%
#'   \item \code{inter_quantile_10_90}: Difference of inter-quantile between 10% and 90%
#'   \item \code{KL_10_true_probas}: KL of of predicted probabilities w.r. to true probabilities with 10 bins
#'   \item \code{KL_10_scores}: KL of of true probabilities w.r. to predicted probabilities with 10 bins
#'   \item \code{KL_20_true_probas}: KL of of predicted probabilities w.r. to true probabilities with 20 bins
#'   \item \code{KL_20_scores}: KL of of true probabilities w.r. to predicted probabilities with 20 bins
#'   \item \code{ind_cov}: Difference between the variance of true probabilities and the covariance between true probabilities and predicted scores
#' }
dispersion_metrics_beta <- function(shape_1 = 1, shape_2 = 1, scores){

  # Inter-quantiles
  inter_q_80 <- diff(quantile(scores, c(.9, .1))) /
    diff(qbeta(c(.9, .1), shape_1, shape_2))
  inter_q_50 <- diff(quantile(scores, c(.75,.25))) /
    diff(qbeta(c(.75,.25), shape_1, shape_1))

  # KL divergences
  m <- 10 # Number of bins
  h_phat <- hist(scores, breaks = seq(0, 1, length = m + 1), plot = FALSE)
  h_p <- list(breaks = h_phat$breaks, mids = h_phat$mids)
  h_p$density = diff(pbeta(h_p$breaks, shape_1, shape_2))
  h_p$counts =  h_p$density*length(scores)

  # Densities
  h1 <- rbind(h_phat$density / m, h_p$density / m) # Reference : true probabilities
  h2 <- rbind(h_p$density / m, h_phat$density / m) # Reference : predicted scores
  KL_10_true_probas <- distance(
    h1, method = "kullback-leibler", unit = "log2", mute.message = TRUE)
  KL_10_scores <- distance(
    h2, method = "kullback-leibler", unit = "log2", mute.message = TRUE)


  m <- 20 # Number of bins
  h_phat <- hist(scores, breaks = seq(0, 1, length = m + 1), plot = FALSE)
  h_p <- list(breaks = h_phat$breaks, mids = h_phat$mids)
  h_p$density = diff(pbeta(h_p$breaks, shape_1, shape_2))
  h_p$counts =  h_p$density * length(scores)
  # Densities
  h1 <- rbind(h_phat$density / m, h_p$density) # Reference : true probabilities
  h2 <- rbind(h_p$density, h_phat$density / m) # Reference : predicted scores
  KL_20_true_probas <- distance(
    h1, method = "kullback-leibler", unit = "log2", mute.message = TRUE)
  KL_20_scores <- distance(
    h2, method = "kullback-leibler", unit = "log2", mute.message = TRUE)

  # Indicator of the difference between variance and covariance
  var_p <- shape_1 * shape_2 / ((shape_1 + shape_2)^2 * (shape_1 + shape_2 + 1))
  cov_p_phat <- cov(
    qbeta(
      rank(scores, ties.method = "average") / (1 + length(scores)),
      shape_1,
      shape_2),
    scores
  )
  ind_cov <- abs(cov_p_phat - var_p)

  # Collection
  dispersion_metrics <- tibble(
    "inter_quantile_25_75" = as.numeric(inter_q_50),
    "inter_quantile_10_90" = as.numeric(inter_q_80),
    "KL_10_true_probas" = as.numeric(KL_10_true_probas),
    "KL_10_scores" = as.numeric(KL_10_scores),
    "KL_20_true_probas" = as.numeric(KL_20_true_probas),
    "KL_20_scores" = as.numeric(KL_20_scores),
    "ind_cov" = ind_cov
  )

  dispersion_metrics
}

disp_metrics_dataset <- function(prior, scores) {
  # GAMSEL prior
  shape_1 <- prior$mle_gamsel$estimate["shape1"]
  shape_2 <- prior$mle_gamsel$estimate["shape2"]

  # Divergence metrics
  dist_prior_gamsel <- dispersion_metrics_beta(
    shape_1 = shape_1, shape_2 = shape_2, scores = scores
  )

  dist_prior_gamsel |>
    mutate(
      prior = "gamsel", shape_1 = shape_1, shape_2 = shape_2
      )
}
```

### Estimation Functions: Extreme Gradient Boosting

We train XGB models on the dataset "spambase", varying the maximum tree depth (with values of 2, 4, or 6) and the number of boosting iterations (ranging from 2 to 500). At each iteration, we compute metrics based on scores from both the train, calibration, validation and test samples, as well as the recalibrated scores.

This function allows us to calculate the performance, dispersion and calibration metrics on all sets of the dataset at a given iteration for a single XGBoost algorithm:
```{r define-get_metrics_xgb_iter}
#| code-fold: true
#| code-summary: Function `get_metrics_xgb_iter()`{.R}
#' Get the metrics based on scores estimated at a given boosting iteration
#'
#' @param scores scores estimated a boosting iteration `nb_iter` (list with
#'   train and test scores, returned by `predict_score_iter()`)
#' @param data_train train set
#' @param data_valid validation set
#' @param data_calib calibration set
#' @param data_test test set
#' @param target_name name of the target variable
#' @param ind index of the grid search
#' @param nb_iter boosting iteration to consider
#' @param params hyperparameters to consider
#'
#' @returns A list with 4 elements:
#'  - `tb_metrics`: performance / calibration metrics
#'  - `tb_disp_metrics`: disp and div metrics
#'  - `tb_prop_scores`: table with P(q1 < score < q2)
#'  - `scores_hist`: histogram of scores
get_metrics_xgb_iter <- function(scores,
                                 prior,
                                 data_train,
                                 data_valid,
                                 data_calib,
                                 data_test,
                                 target_name,
                                 ind,
                                 nb_iter,
                                 params) {

  scores_train <- scores$scores_train
  scores_valid <- scores$scores_valid
  scores_calib <- scores$scores_calib
  scores_test <- scores$scores_test
  
  # Recalibration
  # Platt scaling
  res_recalibration_platt <- recalibrate(
    obs_calib = data_calib |> dplyr::pull(!!target_name), 
    obs_test = data_test |> dplyr::pull(!!target_name), 
    pred_calib = scores_calib, 
    pred_test = scores_test, 
    method = "platt"
  )
  scores_c_platt_calib <- res_recalibration_platt$tb_score_c_calib$p_c
  scores_c_platt_test <- res_recalibration_platt$tb_score_c_test$p_c
  
  # Isotonic regression
  res_recalibration_iso <- recalibrate(
    obs_calib = data_calib |> dplyr::pull(!!target_name), 
    obs_test = data_test |> dplyr::pull(!!target_name), 
    pred_calib = scores_calib, 
    pred_test = scores_test, 
    method = "isotonic"
  )
  scores_c_iso_calib <- res_recalibration_iso$tb_score_c_calib$p_c
  scores_c_iso_test <- res_recalibration_iso$tb_score_c_test$p_c

  ## Metrics----
  ## Histogram of scores----
  breaks <- seq(0, 1, by = .05)
  scores_train_hist <- hist(scores_train, breaks = breaks, plot = FALSE)
  scores_calib_hist <- hist(scores_calib, breaks = breaks, plot = FALSE)
  scores_valid_hist <- hist(scores_valid, breaks = breaks, plot = FALSE)
  scores_test_hist <- hist(scores_test, breaks = breaks, plot = FALSE)
  scores_c_platt_calib_hist <- hist(scores_c_platt_calib, breaks = breaks, plot = FALSE)
  scores_c_platt_test_hist <- hist(scores_c_platt_test, breaks = breaks, plot = FALSE)
  scores_c_iso_calib_hist <- hist(scores_c_iso_calib, breaks = breaks, plot = FALSE)
  scores_c_iso_test_hist <- hist(scores_c_iso_test, breaks = breaks, plot = FALSE)
  
  scores_hist <- list(
    train = scores_train_hist,
    valid = scores_valid_hist,
    calib = scores_calib_hist,
    test = scores_test_hist,
    calib_c_platt = scores_c_platt_calib_hist,
    test_c_platt = scores_c_platt_test_hist,
    calib_c_iso = scores_c_iso_calib_hist,
    test_c_iso = scores_c_iso_test_hist,
    ind = ind,
    nb_iter = nb_iter,
    max_depth = params$max_depth
    
  )
  
  ## Estimation of P(q1 < score < q2)----
  prop_btw_q_h <- function(s, sample_name, recalib_name) {
    map(
      c(.1, .2, .3, .4),
      ~prop_btw_quantiles(s = s, q1 = .x)
    ) |>
      list_rbind() |>
      mutate(sample = sample_name, recalib = recalib_name)
  }
  
  proq_scores_train <- prop_btw_q_h(
    scores_train, sample_name = "train", recalib_name = "none"
  )
  proq_scores_valid <- prop_btw_q_h(
    scores_valid, sample_name = "valid", recalib_name = "none"
  )
  proq_scores_calib <- prop_btw_q_h(
    scores_calib, sample_name = "calib", recalib_name = "none"
  )
  proq_scores_test <- prop_btw_q_h(
    scores_test, sample_name = "test", recalib_name = "none"
  )
  proq_scores_c_platt_calib <- prop_btw_q_h(
    scores_c_platt_calib, sample_name = "calib", recalib_name = "platt"
  )
  proq_scores_c_platt_test <- prop_btw_q_h(
    scores_c_platt_test, sample_name = "test", recalib_name = "platt"
  )
  proq_scores_c_iso_calib <- prop_btw_q_h(
    scores_c_iso_calib, sample_name = "calib", recalib_name = "isotonic"
  )
  proq_scores_c_iso_test <- prop_btw_q_h(
    scores_c_iso_test, sample_name = "test", recalib_name = "isotonic"
  )

  ## Dispersion Metrics----
  disp_train <- disp_metrics_dataset(
    prior = prior, scores = scores_train
  ) |> 
    mutate(sample = "train", recalib = "none")
  
  disp_valid <- disp_metrics_dataset(
    prior = prior, scores = scores_valid
  ) |>
    mutate(sample = "valid", recalib = "none")
  
  disp_calib <- disp_metrics_dataset(
    prior = prior, scores = scores_calib
  ) |> 
    mutate(sample = "calib", recalib = "none")
  
  disp_test <- disp_metrics_dataset(
    prior = prior, scores = scores_test
  ) |> 
    mutate(sample = "test", recalib = "none")
  
  disp_c_platt_calib <- disp_metrics_dataset(
    prior = prior, scores = scores_c_platt_calib
  ) |>
    mutate(sample = "calib", recalib = "platt")
  
  disp_c_platt_test <- disp_metrics_dataset(
    prior = prior, scores = scores_c_platt_test
  ) |>
    mutate(sample = "test", recalib = "platt")
  
  disp_c_iso_calib <- disp_metrics_dataset(
    prior = prior, scores = scores_c_iso_calib
  ) |>
    mutate(sample = "calib", recalib = "isotonic")
  
  disp_c_iso_test <- disp_metrics_dataset(
    prior = prior, scores = scores_c_iso_test
  ) |>
    mutate(sample = "test", recalib = "isotonic")

  # Performance and Calibration Metrics----
  # We add very small noise to predicted scores
  # otherwise the local regression may crash
  scores_train_noise <- scores_train +
    runif(n = length(scores_train), min = 0, max = 0.01)
  scores_train_noise[scores_train_noise > 1] <- 1
  metrics_train <- compute_metrics(
    obs = data_train |> pull(!!target_name),
    scores = scores_train_noise, true_probas = NULL
  ) |> mutate(sample = "train", recalib = "none")

  scores_valid_noise <- scores_valid +
    runif(n = length(scores_valid), min = 0, max = 0.01)
  scores_valid_noise[scores_valid_noise > 1] <- 1
  metrics_valid <- compute_metrics(
    obs = data_valid |> pull(!!target_name),
    scores = scores_valid_noise, true_probas = NULL
  ) |> mutate(sample = "valid", recalib = "none")
  
  scores_calib_noise <- scores_calib +
    runif(n = length(scores_calib), min = 0, max = 0.01)
  scores_calib_noise[scores_calib_noise > 1] <- 1
  metrics_calib <- compute_metrics(
    obs = data_calib |> pull(!!target_name),
    scores = scores_calib_noise, true_probas = NULL
  ) |> mutate(sample = "calib", recalib = "none")
  
  scores_test_noise <- scores_test +
    runif(n = length(scores_test), min = 0, max = 0.01)
  scores_test_noise[scores_test_noise > 1] <- 1
  metrics_test <- compute_metrics(
    obs = data_test |> pull(!!target_name),
    scores = scores_test_noise, true_probas = NULL
  ) |> mutate(sample = "test", recalib = "none")
  
  # With recalibrated scores (platt)
  scores_c_platt_calib_noise <- scores_c_platt_calib +
    runif(n = length(scores_calib), min = 0, max = 0.01)
  scores_c_platt_calib_noise[scores_c_platt_calib_noise > 1] <- 1
  metrics_c_platt_calib <- compute_metrics(
    obs = data_calib |> pull(!!target_name),
    scores = scores_c_platt_calib_noise, true_probas = NULL
  ) |> mutate(sample = "calib", recalib = "platt")
  
  scores_c_platt_test_noise <- scores_c_platt_test +
    runif(n = length(scores_test), min = 0, max = 0.01)
  scores_c_platt_test_noise[scores_c_platt_test_noise > 1] <- 1
  metrics_c_platt_test <- compute_metrics(
    obs = data_test |> pull(!!target_name),
    scores = scores_c_platt_test_noise, true_probas = NULL
  ) |> mutate(sample = "test", recalib = "platt")
  
  # With recalibrated scores (isotonic)
  scores_c_iso_calib_noise <- scores_c_iso_calib +
    runif(n = length(scores_c_iso_calib), min = 0, max = 0.01)
  scores_c_iso_calib_noise[scores_c_iso_calib_noise > 1] <- 1
  metrics_c_iso_calib <- compute_metrics(
    obs = data_calib |> pull(!!target_name),
    scores = scores_c_iso_calib_noise, true_probas = NULL
  ) |> mutate(sample = "calib", recalib = "isotonic")
  
  scores_c_iso_test_noise <- scores_c_iso_test +
    runif(n = length(scores_c_iso_test), min = 0, max = 0.01)
  scores_c_iso_test_noise[scores_c_iso_test_noise > 1] <- 1
  metrics_c_iso_test <- compute_metrics(
    obs = data_test |> pull(!!target_name),
    scores = scores_c_iso_test_noise, true_probas = NULL
  ) |> mutate(sample = "test", recalib = "isotonic")
  
  tb_metrics <- metrics_train |>
    bind_rows(metrics_valid) |>
    bind_rows(metrics_calib) |>
    bind_rows(metrics_test) |>
    bind_rows(metrics_c_platt_calib) |>
    bind_rows(metrics_c_platt_test) |>
    bind_rows(metrics_c_iso_calib) |>
    bind_rows(metrics_c_iso_test) |>
    mutate(ind = ind, nb_iter = nb_iter, max_depth = params$max_depth) |>
    select(-c(mse, mae))
  
  tb_disp_metrics <- disp_train |>
    bind_rows(disp_valid) |> 
    bind_rows(disp_calib) |> 
    bind_rows(disp_test) |> 
    bind_rows(disp_c_platt_calib) |> 
    bind_rows(disp_c_platt_test) |> 
    bind_rows(disp_c_iso_calib) |> 
    bind_rows(disp_c_iso_test) |>
    mutate(ind = ind, nb_iter = nb_iter, max_depth = params$max_depth)
  
  tb_metrics <- tb_metrics |> left_join(
    tb_disp_metrics, by = c("sample", "recalib", "ind", "nb_iter", "max_depth")
  )

  tb_prop_scores <- proq_scores_train |>
    bind_rows(proq_scores_valid) |>
    bind_rows(proq_scores_calib) |>
    bind_rows(proq_scores_test) |>
    bind_rows(proq_scores_c_platt_calib) |>
    bind_rows(proq_scores_c_platt_test) |>
    bind_rows(proq_scores_c_iso_calib) |>
    bind_rows(proq_scores_c_iso_test) |>
    mutate(ind = ind, nb_iter = nb_iter, max_depth = params$max_depth)
  
   list(
    ind = ind,                         # index for grid
    nb_iter = nb_iter,                 # number of boosting iterations
    max_depth = params$max_depth,      # max depth of used trees
    tb_metrics = tb_metrics,           # # table with performance/calib/divergence
    tb_prop_scores = tb_prop_scores,   # table with P(q1 < score < q2)
    scores_hist = scores_hist          # histogram of scores
  )
}
```


We then define a function to predict a XGBoost algorithm on train, validation, calibration and test samples given an xgboost object (fit). We are able to obtain the predicted scores at a specified iteration during the training of the algorithm:
```{r define-predict_score_iter}
#| code-fold: true
#| code-summary: Function `predict_score_iter()`{.R}
#' Predicts the scores at a given iteration of the XGB model
#'
#' @param fit_xgb estimated XGB model
#' @param tb_train_xgb train set
#' @param tb_valid_xgb validation set
#' @param tb_test_xgb test set
#' @param ind index of the grid search
#' @param nb_iter boosting iteration to consider
#'
#' @returns A list with three elements: `scores_train`, `scores_valid`, and
#'  `scores_train` which contain the estimated scores on the train and on the 
#'  test score, resp.
predict_score_iter <- function(fit_xgb,
                               tb_train_xgb,
                               tb_valid_xgb,
                               tb_calib_xgb,
                               tb_test_xgb,
                               nb_iter) {

  ## Predicted scores----
  scores_train <- predict(fit_xgb, tb_train_xgb, iterationrange = c(1, nb_iter))
  scores_valid <- predict(fit_xgb, tb_valid_xgb, iterationrange = c(1, nb_iter))
  scores_calib <- predict(fit_xgb, tb_calib_xgb, iterationrange = c(1, nb_iter))
  scores_test <- predict(fit_xgb, tb_test_xgb, iterationrange = c(1, nb_iter))

  list(
    scores_train = scores_train,
    scores_valid = scores_valid,
    scores_calib = scores_calib,
    scores_test = scores_test
  )
}
```

This function calculates the different metrics on train, validation, calibration and test samples for a given XGBoost algorithm at all iterations:
```{r define-simul_xgb_helper}
#| code-fold: true
#| code-summary: Function `simul_xgb_helper()`{.R}
#' Fit an XGB and returns metrics based on scores. The divergence metrics are
#' obtained using the prior distributions.
#'
#' @param data_train train set
#' @param data_valid validation set
#' @param data_test test set
#' @param target_name name of the target variable
#' @param parms tibble with hyperparameters for the current estimation
#' @param prior prior obtained with `get_beta_fit()`
#'
#' @returns A list with 4 elements:
#'  - `tb_metrics`: performance / calibration metrics
#'  - `tb_disp_metrics`: disp and div metrics
#'  - `tb_prop_scores`: table with P(q1 < score < q2)
#'  - `scores_hist`: histogram of scores
simul_xgb_helper <- function(data_train,
                             data_valid,
                             data_calib,
                             data_test,
                             target_name,
                             params,
                             prior) {

  ## Format data for xgboost----
  tb_train_xgb <- xgb.DMatrix(
    data = data_train |> dplyr::select(-!!target_name) |> as.matrix(),
    label = data_train |> dplyr::pull(!!target_name) |> as.matrix()
  )
  tb_valid_xgb <- xgb.DMatrix(
    data = data_valid |> dplyr::select(-!!target_name) |> as.matrix(),
    label = data_valid |> dplyr::pull(!!target_name) |> as.matrix()
  )
  tb_calib_xgb <- xgb.DMatrix(
    data = data_calib |> dplyr::select(-!!target_name) |> as.matrix(),
    label = data_calib |> dplyr::pull(!!target_name) |> as.matrix()
  )
  tb_test_xgb <- xgb.DMatrix(
    data = data_test |> dplyr::select(-!!target_name) |> as.matrix(),
    label = data_test |> dplyr::pull(!!target_name) |> as.matrix()
  )
  # Parameters for the algorithm
  param <- list(
    max_depth = params$max_depth, #Note: root node is indexed 0
    eta = params$eta,
    nthread = 1,
    objective = "binary:logistic",
    eval_metric = "auc"
  )
  watchlist <- list(train = tb_train_xgb, eval = tb_valid_xgb)
  ## Estimation----
  fit_xgb <- xgb.train(
    param, tb_train_xgb,
    nrounds = params$nb_iter_total,
    watchlist,
    verbose = 0
  )

  # First, we estimate the scores at each boosting iteration
  # As the xgb.Dmatrix objects cannot be easily serialised, we first estimate
  # these scores in a classical way, without parallelism...
  scores_iter <- vector(mode = "list", length = params$nb_iter_total)
  for (i_iter in 1:params$nb_iter_total) {
    scores_iter[[i_iter]] <- predict_score_iter(
      fit_xgb = fit_xgb,
      tb_train_xgb = tb_train_xgb,
      tb_valid_xgb = tb_valid_xgb,
      tb_calib_xgb = tb_calib_xgb,
      tb_test_xgb = tb_test_xgb,
      nb_iter = i_iter)
  }

  # Then, to compute the metrics, as it is a bit slower, we can use parallelism

  ncl <- detectCores() - 1
  (cl <- makeCluster(ncl))
  clusterEvalQ(cl, {
    library(tidyverse)
    library(locfit)
    library(philentropy)
  }) |>
    invisible()

  clusterExport(cl, c(
    "scores_iter", "prior", "data_train", "data_valid", "data_calib", "data_test", "params", 
    "target_name"
  ), envir = environment())
  clusterExport(cl, c(
    "get_metrics_xgb_iter",
    "brier_score",
    "compute_metrics",
    "disp_metrics_dataset", "dispersion_metrics_beta",
    "recalibrate", "prop_btw_quantiles"
  ))

  metrics_iter <-
    pbapply::pblapply(
      X = seq_len(params$nb_iter_total),
      FUN = function(i_iter) {
        get_metrics_xgb_iter(
          scores = scores_iter[[i_iter]],
          prior = prior,
          data_train = data_train,
          data_valid = data_valid,
          data_calib = data_calib,
          data_test = data_test,
          target_name = target_name,
          ind = params$ind,
          nb_iter = i_iter,
          params = params
        )
      },
      cl = cl
    )
  stopCluster(cl)
  
  # Merge tibbles from each iteration into a single one
  tb_metrics <-
    map(metrics_iter, "tb_metrics") |>
    list_rbind()
  tb_prop_scores <-
    map(metrics_iter, "tb_prop_scores") |>
    list_rbind()
  scores_hist <- map(metrics_iter, "scores_hist")

  list(
    tb_metrics = tb_metrics,
    tb_prop_scores = tb_prop_scores,
    scores_hist = scores_hist
  )
}
```

## Import Data and Prior

We load the dataset "spambase" and the associated Beta prior on both train and test sets.
```{r load-data-priors, eval=FALSE}
load("output/real-data/tb_spambase.rda") # dataset
load("output/real-data/priors_spambase.rda") # scores_gamsel

prior <- priors_spambase
data <- tb_spambase
```

The target variable is `is_spam`.
```{r define-target_name}
name <- "spambase"
target_name <- "is_spam"
```

## Grid

We consider the following grid:
```{r define-grid-xgb}
grid <- expand_grid(
  max_depth = c(2, 4, 6),
  nb_iter_total = 500,
  eta = 0.3
) |>
  mutate(ind = row_number())
```

The different configurations are reported in @tbl-grid-values-xgb-real-data.
```{r}
#| tbl-cap: "Grid Search Values"
#| label: tbl-grid-values-xgb-real-data
DT::datatable(grid)
```

We define a function, `simul_xgb_real()`{.R} to train the model on a dataset for all different values of the hyperparameters of the grid (and for all iterations of each algorithm, defined by a row in the grid).
```{r define-simul_xgb_real}
#| code-fold: true
#| code-summary: Function `simul_xgb_real()`{.R}
#' Train an XGB on a dataset for a binary task for various
#' hyperparameters and computes metrics based on scores and on a set of prior
#' distributions of the underlying probability
#'
#' @param data dataset
#' @param target_name name of the target variable
#' @param prior prior obtained with `get_beta_fit()`
#' @param seed desired seed (default to `NULL`)
#'
#' @returns A list with two elements:
#'  - `res`: results for each estimated model of the grid. Each element is a
#'  list with the following elements:
#'      - `tb_metrics`: performance / calibration metrics
#'      - `tb_disp_metrics`: disp and div metrics
#'      - `tb_prop_scores`: table with P(q1 < score < q2)
#'      - `scores_hist`: histogram of scores.
#'  - `grid`: the grid search.
simul_xgb_real <- function(data,
                           target_name,
                           prior,
                           seed = NULL) {

  if (!is.null(seed)) set.seed(seed)

  # Split data into train and test set
  data_splitted <- split_train_test(data = data, prop_train = .5, seed = seed)
  data_encoded <- encode_dataset(
    data_train = data_splitted$train,
    data_test = data_splitted$test,
    target_name = target_name,
    intercept = FALSE
  )

  # Further split train into two samples (train/valid)
  data_splitted_train <- 
    split_train_test(data = data_encoded$train, prop_train = .8, seed = seed)
  
  # Further split test into two samples (calib/test)
  data_splitted_test <- 
    split_train_test(data = data_encoded$test, prop_train = .8, seed = seed)
  
  res_grid <- vector(mode = "list", length = nrow(grid))
  for (i_grid in 1:nrow(grid)) {
    res_grid[[i_grid]] <- simul_xgb_helper(
      data_train = data_splitted_train$train,
      data_valid = data_splitted_train$test,
      data_calib = data_splitted_test$train,
      data_test = data_splitted_test$test,
      target_name = target_name,
      params = grid |> dplyr::slice(i_grid),
      prior = prior
    )
  }
  
  # The metrics computed for all set of hyperparameters (identified with `ind`)
  # and for each number of boosting iterations (`nb_iter`)
  metrics_simul <- map(res_grid, "tb_metrics") |> 
    list_rbind()
  
  # P(q_1<s(x)<q_2)
  prop_scores_simul <- map(res_grid, "tb_prop_scores") |> 
    list_rbind()
  
  # Histogram of estimated scores
  scores_hist <- map(res_grid, "scores_hist")

  list(
    metrics_simul = metrics_simul,
    scores_hist = scores_hist,
    prop_scores_simul = prop_scores_simul
  )
}
```

## Example: "spambase" dataset

We start to split the dataset "spambase" into train/validation/calibration/test sets.
```{r}
data_splitted <- split_train_test(data = data, prop_train = .7, seed = 1234)
data_encoded <- encode_dataset(
  data_train = data_splitted$train,
  data_test = data_splitted$test,
  target_name = target_name,
  intercept = FALSE
  )

# Further split train into two samples (train/valid)
data_splitted_train <- 
  split_train_test(data = data_encoded$train, prop_train = .8, seed = 1234)
  
# Further split test into two samples (calib/test)
data_splitted_test <- 
  split_train_test(data = data_encoded$test, prop_train = .6, seed = 1234)

# The different sets
data_train = data_splitted_train$train
data_valid = data_splitted_train$test
data_calib = data_splitted_test$train
data_test = data_splitted_test$test
```

We try to fit and predict XGBoost with one set of hyperparameters. Here, we don't need to use the validation set because we have specified the hyperparameters. In practice, hyperparameters will be selected based on different metrics (calibration, performance, dispersion) calculated on the validation set.

Let's take the first row of the grid:
```{r}
i_grid <- 1
params <- grid |> dplyr::slice(i_grid)

# Parameters for the algorithm
param <- list(
  max_depth = params$max_depth, #Note: root node is indexed 0
  eta = params$eta,
  nthread = 1,
  objective = "binary:logistic",
  eval_metric = "auc"
)
```

Next, we transform the different sets of the dataset in the right format to apply XGBoost:
```{r}
## Format data for xgboost----
tb_train_xgb <- xgb.DMatrix(
  data = data_train |> dplyr::select(-!!target_name) |> as.matrix(),
  label = data_train |> dplyr::pull(!!target_name) |> as.matrix()
)
tb_valid_xgb <- xgb.DMatrix(
  data = data_valid |> dplyr::select(-!!target_name) |> as.matrix(),
  label = data_valid |> dplyr::pull(!!target_name) |> as.matrix()
)
tb_calib_xgb <- xgb.DMatrix(
  data = data_calib |> dplyr::select(-!!target_name) |> as.matrix(),
  label = data_calib |> dplyr::pull(!!target_name) |> as.matrix()
)
tb_test_xgb <- xgb.DMatrix(
  data = data_test |> dplyr::select(-!!target_name) |> as.matrix(),
  label = data_test |> dplyr::pull(!!target_name) |> as.matrix()
)
  
watchlist <- list(train = tb_train_xgb, eval = tb_valid_xgb)
```

We fit the specified algorithm on the train set:
```{r}
## Estimation----
fit_xgb <- xgb.train(
  param, tb_train_xgb,
  nrounds = params$nb_iter_total,
  watchlist,
  verbose = 0
)
```

Then, we estimate the scores at each boosting iteration to specify it for the final model, by using the function `predict_score_iter()`{.R}. We obtain a list of 400 elements, corresponding to the predicted scores at each iteration (total of 400 iterations for a specified XGBoost algorithm):
```{r}
# As the xgb.Dmatrix objects cannot be easily serialised, we first estimate
# these scores in a classical way, without parallelism...
scores_iter <- vector(mode = "list", length = params$nb_iter_total)
for (i_iter in 1:params$nb_iter_total) {
  scores_iter[[i_iter]] <- predict_score_iter(
    fit_xgb = fit_xgb,
    tb_train_xgb = tb_train_xgb,
    tb_valid_xgb = tb_valid_xgb,
    tb_calib_xgb = tb_calib_xgb,
    tb_test_xgb = tb_test_xgb,
    nb_iter = i_iter
  )
}
```

We are going to calculate different metrics for a single number of iterations (here 1 iteration only):
```{r}
i_iter <- 1
scores <- scores_iter[[i_iter]]
scores_train <- scores$scores_train
scores_valid <- scores$scores_valid
scores_calib <- scores$scores_calib
scores_test <- scores$scores_test
```

We recalibrate those predicted scores (calibration and test sets only) using either Platt scaling or isotonic regression:
```{r}
# Recalibration
# Platt scaling
res_recalibration_platt <- recalibrate(
  obs_calib = data_calib |> dplyr::pull(!!target_name),
  obs_test = data_test |> dplyr::pull(!!target_name), 
  pred_calib = scores_calib, 
  pred_test = scores_test, 
  method = "platt"
)
scores_c_platt_calib <- res_recalibration_platt$tb_score_c_calib$p_c
scores_c_platt_test <- res_recalibration_platt$tb_score_c_test$p_c
  
# Isotonic regression
res_recalibration_iso <- recalibrate(
  obs_calib = data_calib |> dplyr::pull(!!target_name), 
  obs_test = data_test |> dplyr::pull(!!target_name), 
  pred_calib = scores_calib, 
  pred_test = scores_test, 
  method = "isotonic"
)
scores_c_iso_calib <- res_recalibration_iso$tb_score_c_calib$p_c
scores_c_iso_test <- res_recalibration_iso$tb_score_c_test$p_c
```

Here are the histograms of predicted scores:
```{r}
## Histogram of scores----
breaks <- seq(0, 1, by = .05)
scores_train_hist <- hist(scores_train, breaks = breaks, plot = FALSE)
scores_calib_hist <- hist(scores_calib, breaks = breaks, plot = FALSE)
scores_valid_hist <- hist(scores_valid, breaks = breaks, plot = FALSE)
scores_test_hist <- hist(scores_test, breaks = breaks, plot = FALSE)
scores_c_platt_calib_hist <- hist(scores_c_platt_calib, breaks = breaks, plot = FALSE)
scores_c_platt_test_hist <- hist(scores_c_platt_test, breaks = breaks, plot = FALSE)
scores_c_iso_calib_hist <- hist(scores_c_iso_calib, breaks = breaks, plot = FALSE)
scores_c_iso_test_hist <- hist(scores_c_iso_test, breaks = breaks, plot = FALSE)

ind <- params$ind
nb_iter <- i_iter
scores_hist <- list(
  train = scores_train_hist,
  valid = scores_valid_hist,
  calib = scores_calib_hist,
  test = scores_test_hist,
  calib_c_platt = scores_c_platt_calib_hist,
  test_c_platt = scores_c_platt_test_hist,
  calib_c_iso = scores_c_iso_calib_hist,
  test_c_iso = scores_c_iso_test_hist,
  ind = ind,
  nb_iter = nb_iter,
  max_depth = params$max_depth
)
```

We can also calculate the interquantile distances of recalibrated and non-recalibrated predicted scores:
```{r}
 ## Estimation of P(q1 < score < q2)----
prop_btw_q_h <- function(s, sample_name, recalib_name) {
  map(
    c(.1, .2, .3, .4),
    ~prop_btw_quantiles(s = s, q1 = .x)
    ) |>
    list_rbind() |>
    mutate(sample = sample_name, recalib = recalib_name)
}
  
proq_scores_train <- prop_btw_q_h(
  scores_train, sample_name = "train", recalib_name = "none"
)
proq_scores_valid <- prop_btw_q_h(
  scores_valid, sample_name = "valid", recalib_name = "none"
)
proq_scores_calib <- prop_btw_q_h(
  scores_calib, sample_name = "calib", recalib_name = "none"
)
proq_scores_test <- prop_btw_q_h(
  scores_test, sample_name = "test", recalib_name = "none"
)
proq_scores_c_platt_calib <- prop_btw_q_h(
  scores_c_platt_calib, sample_name = "calib", recalib_name = "platt"
)
proq_scores_c_platt_test <- prop_btw_q_h(
  scores_c_platt_test, sample_name = "test", recalib_name = "platt"
)
proq_scores_c_iso_calib <- prop_btw_q_h(
  scores_c_iso_calib, sample_name = "calib", recalib_name = "isotonic"
)
proq_scores_c_iso_test <- prop_btw_q_h(
  scores_c_iso_test, sample_name = "test", recalib_name = "isotonic"
)
```

And the Kullback-Leibler divergence between predicted scores and Beta prior (obtained with GAMSEL model):
```{r}
 ## Dispersion Metrics----
disp_train <- disp_metrics_dataset(
  prior = prior, scores = scores_train
  ) |> 
  mutate(sample = "train", recalib = "none")
  
disp_valid <- disp_metrics_dataset(
  prior = prior, scores = scores_valid
  ) |>
  mutate(sample = "valid", recalib = "none")
  
disp_calib <- disp_metrics_dataset(
  prior = prior, scores = scores_calib
  ) |> 
  mutate(sample = "calib", recalib = "none")
  
disp_test <- disp_metrics_dataset(
  prior = prior, scores = scores_test
  ) |> 
  mutate(sample = "test", recalib = "none")

disp_c_platt_calib <- disp_metrics_dataset(
  prior = prior, scores = scores_c_platt_calib
  ) |>
  mutate(sample = "calib", recalib = "platt")

disp_c_platt_test <- disp_metrics_dataset(
  prior = prior, scores = scores_c_platt_test
  ) |>
  mutate(sample = "test", recalib = "platt")

disp_c_iso_calib <- disp_metrics_dataset(
  prior = prior, scores = scores_c_iso_calib
  ) |>
  mutate(sample = "calib", recalib = "isotonic")
  
disp_c_iso_test <- disp_metrics_dataset(
  prior = prior, scores = scores_c_iso_test
  ) |>
  mutate(sample = "test", recalib = "isotonic")
```

Finally, the performance metrics:
```{r}
# Performance and Calibration Metrics----
# We add very small noise to predicted scores
# otherwise the local regression may crash
scores_train_noise <- scores_train +
  runif(n = length(scores_train), min = 0, max = 0.01)
scores_train_noise[scores_train_noise > 1] <- 1
metrics_train <- compute_metrics(
  obs = data_train |> pull(!!target_name),
  scores = scores_train_noise, true_probas = NULL
  ) |> mutate(sample = "train", recalib = "none")

scores_valid_noise <- scores_valid +
  runif(n = length(scores_valid), min = 0, max = 0.01)
scores_valid_noise[scores_valid_noise > 1] <- 1
metrics_valid <- compute_metrics(
  obs = data_valid |> pull(!!target_name),
  scores = scores_valid_noise, true_probas = NULL
  ) |> mutate(sample = "valid", recalib = "none")
  
scores_calib_noise <- scores_calib +
  runif(n = length(scores_calib), min = 0, max = 0.01)
scores_calib_noise[scores_calib_noise > 1] <- 1
metrics_calib <- compute_metrics(
  obs = data_calib |> pull(!!target_name),
  scores = scores_calib_noise, true_probas = NULL
  ) |> mutate(sample = "calib", recalib = "none")
  
scores_test_noise <- scores_test +
  runif(n = length(scores_test), min = 0, max = 0.01)
scores_test_noise[scores_test_noise > 1] <- 1
metrics_test <- compute_metrics(
  obs = data_test |> pull(!!target_name),
  scores = scores_test_noise, true_probas = NULL
  ) |> mutate(sample = "test", recalib = "none")
  
# With recalibrated scores (platt)
scores_c_platt_calib_noise <- scores_c_platt_calib +
  runif(n = length(scores_calib), min = 0, max = 0.01)
scores_c_platt_calib_noise[scores_c_platt_calib_noise > 1] <- 1
metrics_c_platt_calib <- compute_metrics(
  obs = data_calib |> pull(!!target_name),
  scores = scores_c_platt_calib_noise, true_probas = NULL
  ) |> mutate(sample = "calib", recalib = "platt")
  
scores_c_platt_test_noise <- scores_c_platt_test +
  runif(n = length(scores_test), min = 0, max = 0.01)
scores_c_platt_test_noise[scores_c_platt_test_noise > 1] <- 1
metrics_c_platt_test <- compute_metrics(
  obs = data_test |> pull(!!target_name),
  scores = scores_c_platt_test_noise, true_probas = NULL
  ) |> mutate(sample = "test", recalib = "platt")
  
# With recalibrated scores (isotonic)
scores_c_iso_calib_noise <- scores_c_iso_calib +
  runif(n = length(scores_c_iso_calib), min = 0, max = 0.01)
scores_c_iso_calib_noise[scores_c_iso_calib_noise > 1] <- 1
metrics_c_iso_calib <- compute_metrics(
  obs = data_calib |> pull(!!target_name),
  scores = scores_c_iso_calib_noise, true_probas = NULL
  ) |> mutate(sample = "calib", recalib = "isotonic")
  
scores_c_iso_test_noise <- scores_c_iso_test +
  runif(n = length(scores_c_iso_test), min = 0, max = 0.01)
scores_c_iso_test_noise[scores_c_iso_test_noise > 1] <- 1
metrics_c_iso_test <- compute_metrics(
  obs = data_test |> pull(!!target_name),
  scores = scores_c_iso_test_noise, true_probas = NULL
  ) |> mutate(sample = "test", recalib = "isotonic")
```

We wrap-up all metrics (calibration, dispersion and performance):
```{r}
tb_metrics <- metrics_train |>
  bind_rows(metrics_valid) |>
  bind_rows(metrics_calib) |>
  bind_rows(metrics_test) |>
  bind_rows(metrics_c_platt_calib) |>
  bind_rows(metrics_c_platt_test) |>
  bind_rows(metrics_c_iso_calib) |>
  bind_rows(metrics_c_iso_test) |>
  mutate(ind = ind, nb_iter = nb_iter, max_depth = params$max_depth) |>
  select(-c(mse, mae))

tb_disp_metrics <- disp_train |>
  bind_rows(disp_valid) |> 
  bind_rows(disp_calib) |> 
  bind_rows(disp_test) |> 
  bind_rows(disp_c_platt_calib) |> 
  bind_rows(disp_c_platt_test) |> 
  bind_rows(disp_c_iso_calib) |> 
  bind_rows(disp_c_iso_test) |>
  mutate(ind = ind, nb_iter = nb_iter, max_depth = params$max_depth)

tb_metrics <- tb_metrics |> left_join(
  tb_disp_metrics, by = c("sample", "recalib", "ind", "nb_iter", "max_depth")
)

tb_prop_scores <- proq_scores_train |>
  bind_rows(proq_scores_valid) |>
  bind_rows(proq_scores_calib) |>
  bind_rows(proq_scores_test) |>
  bind_rows(proq_scores_c_platt_calib) |>
  bind_rows(proq_scores_c_platt_test) |>
  bind_rows(proq_scores_c_iso_calib) |>
  bind_rows(proq_scores_c_iso_test) |>
  mutate(ind = ind, nb_iter = nb_iter, max_depth = params$max_depth)

metrics_iter_1 <- list(
  ind = ind,
  nb_iter = nb_iter,
  max_depth = params$max_depth,
  tb_metrics = tb_metrics,
  tb_prop_scores = tb_prop_scores,
  scores_hist = scores_hist
  )
metrics_iter <- list(metrics_iter_1)

tb_metrics <-
  map(metrics_iter, "tb_metrics") |>
  list_rbind()
tb_prop_scores <-
  map(metrics_iter, "tb_prop_scores") |>
  list_rbind()
scores_hist <- map(metrics_iter, "scores_hist")

res_grid <- vector(mode = "list", length = 1)
res_grid[[i_grid]] <- list(
  tb_metrics = tb_metrics,
  tb_prop_scores = tb_prop_scores,
  scores_hist = scores_hist
  )
```

We can also the reproduce the previous steps using the different functions defined above:
```{r}
# Results for a single iteration
resul_iter_1 <- get_metrics_xgb_iter(
  scores = scores,
  prior = prior,
  data_train = data_train,
  data_valid = data_valid,
  data_calib = data_calib,
  data_test = data_test,
  target_name = target_name,
  ind = ind,
  nb_iter = i_iter,
  params = params
)
```

## Final results: "spambase" dataset

We calculate the performance/calibration/dispersion metrics for all iterations and all hyperparameters of the grid:
```{r}
xgb_resul <- simul_xgb_real(data,
                            target_name,
                            prior,
                            seed = 1234)
```

We save the results for the dataset "spambase":
```{r}
save(xgb_resul, file = str_c("output/real-data/xgb_resul_", name, ".rda"))
```

We can now load the results:
```{r}
load(str_c("output/real-data/xgb_resul_", name, ".rda"))
```

## Results 

```{r}
metrics_xgb_all <- xgb_resul$metrics_simul |>
  mutate(
    sample = factor(
      sample,
      levels = c("train", "valid", "calib", "test"),
      labels = c("Train","Validation", "Calibration" ,"Test")
    ),
    recalib = factor(
      recalib,
      levels = c("none", "platt", "isotonic"),
      labels = c("None", "Platt", "Isotonic")
    )
  )
```

For each replication, we made some hyperparameters vary. Let us identify some models of interest:

- `smallest`: model with the lowest number of boosting iteration
- `largest`: model with the highest number of boosting iteration
- `largest_auc`: model with the highest AUC on validation set
- `lowest_mse`: model with the lowest MSE on validation set
- `lowest_ici`: model with the lowest ICI on validation set
- `lowest_kl`: model with the lowest KL Divergence on validation set

```{r xgb-identify-trees-of-interest}
#| code-fold: true
# Identify the smallest tree on the validation set, when the scores are not
# recalibrated
smallest_xgb <-
  metrics_xgb_all |>
  filter(sample == "Validation", recalib == "None") |>
  arrange(nb_iter) |>
  slice_head(n = 1) |>
  select(ind, nb_iter, recalib) |>
  mutate(result_type = "smallest")

# Identify the largest tree
largest_xgb <-
  metrics_xgb_all |>
  filter(sample == "Validation", recalib == "None") |>
  arrange(desc(nb_iter)) |>
  slice_head(n = 1) |>
  select(ind, nb_iter, recalib) |>
  mutate(result_type = "largest")

# Identify tree with highest AUC on test set
highest_auc_xgb <-
  metrics_xgb_all |>
  filter(sample == "Validation", recalib == "None") |>
  arrange(desc(AUC)) |>
  slice_head(n = 1) |>
  select(ind, nb_iter, recalib) |>
  mutate(result_type = "largest_auc")

# Identify tree with lowest brier
lowest_brier_xgb <-
  metrics_xgb_all |>
  filter(sample == "Validation", recalib == "None") |>
  arrange(brier) |>
  slice_head(n = 1) |>
  select(ind, nb_iter, recalib) |>
  mutate(result_type = "lowest_brier")

# Identify tree with lowest ICI
lowest_ici_xgb <-
  metrics_xgb_all |>
  filter(sample == "Validation", recalib == "None") |>
  arrange(ici) |>
  slice_head(n = 1) |>
  select(ind, nb_iter, recalib) |>
  mutate(result_type = "lowest_ici")

# Identify tree with lowest KL
lowest_kl_xgb <-
  metrics_xgb_all |>
  filter(sample == "Validation", recalib == "None") |>
  arrange(KL_20_true_probas) |>
  slice_head(n = 1) |>
  select(ind, nb_iter, recalib) |>
  mutate(result_type = "lowest_kl")

mediocre_ici_xgb <- 
  metrics_xgb_all |>
  filter(sample == "Validation", recalib == "None") |>
  arrange(desc(ici)) |>
  #slice_head(n = 1) |>
  filter(nb_iter == 10 & ind == 1) |>
  select(ind, nb_iter, recalib) |>
  mutate(result_type = "mediocre_ici")

# Merge these
models_of_interest_xgb <-
  smallest_xgb |>
  bind_rows(largest_xgb) |>
  bind_rows(highest_auc_xgb) |>
  bind_rows(lowest_brier_xgb) |>
  bind_rows(lowest_ici_xgb) |>
  bind_rows(lowest_kl_xgb) |> 
  bind_rows(mediocre_ici_xgb)

models_of_interest_metrics <- NULL
for (recalibration_method in c("None", "Platt", "Isotonic")) {
  # Add metrics now
  models_of_interest_metrics <-
    models_of_interest_metrics |>
    bind_rows(
      models_of_interest_xgb |> select(-recalib) |>
        left_join(
          metrics_xgb_all |>
            filter(
              recalib == recalibration_method,
              sample %in% c("Validation", "Test")
            ),
          by = c("ind", "nb_iter"),
          relationship = "many-to-many" # (calib, test)
        )
    )
}


models_of_interest_metrics <-
  models_of_interest_metrics |>
  mutate(
    result_type = factor(
      result_type,
      levels = c(
        "smallest", "largest", "lowest_mse", "largest_auc",
        "lowest_brier", "lowest_ici", "lowest_kl", "mediocre_ici_xgb"),
      labels = c(
        "Smallest", "Largest", "MSE*", "AUC*",
        "Brier*", "ICI*", "KL*", "Mediocre ICI"
      )
    )
  )

```

```{r}
#| tbl-cap: "Metrics for spambase dataset"
#| label: tbl-results-xgb-spambase
DT::datatable(models_of_interest_metrics)
```

### Metrics vs Number of Iterations

We define a function, `plot_metrics()`{.R} to plot selected metrics (AUC, ICI, and KL Divergence) as a function of the number of boosting iterations, for a given value for the hyperparameter `max_depth`. Each curve corresponds to a value of the maximal depth hyperparameter.

TBD

### Distribution of Scores

Let us extract all the histogram information computed over the simulations and put that in a single object, `scores_hist_all`.

```{r define-scores_hist_all}
scores_hist_all <- xgb_resul$scores_hist
```

We then define a function, `plot_bp_xgb()`{.R} which plots the distribution of scores on the test set for the "spambase" dataset. We also define a helper function, `plot_bp_interest()`, which plots the histogram of the scores at a specific iteration number. We will then be able to plot the distributions at the beginning of the boosting iterations, at the end, at a point where the AUC was the highest on the validation set, and at a point where the KL divergence between the distribution of scores on the validation set and the distribution of the true probabilities was the lowest. We will plot the distributions of the scores returned by the classifier, as well as those obtained with the reclibrators.

```{r define-plot_metrics}
#| code-fold: true
#| code-summary: Function `plot_metrics()`{.R}
plot_bp_interest <- function(metrics_interest,
                             scores_hist_interest,
                             label,
                             recalib_method) {
  subtitle <- str_c(
    "Depth = ", metrics_interest$max_depth, ", ",
    "AUC = ", round(metrics_interest$AUC, 2), ", \n",
    "Brier = ", round(metrics_interest$brier, 2), ",",
    "ICI = ", round(metrics_interest$ici, 2), ", ",
    "KL = ", round(metrics_interest$KL_20_true_probas, 2)
  )

  if (recalib_method == "none") {
    plot(
      main = str_c(label, " (iter = ", metrics_interest$nb_iter,")"),
      scores_hist_interest$test,
      xlab = latex2exp::TeX("$\\hat{s}(x)$"),
      ylab = ""
    )
  } else if (recalib_method == "platt") {
    plot(
      main = str_c(label, " (iter = ", metrics_interest$nb_iter,")"),
      scores_hist_interest$test_c_platt,
      xlab = latex2exp::TeX("$\\hat{s}(x)$"),
      ylab = "",
      col = colour_recalib[["Platt"]]
    )
  } else if (recalib_method == "iso") {
    plot(
      main = str_c(label, " (iter = ", metrics_interest$nb_iter,")"),
      scores_hist_interest$test_c_iso,
      xlab = latex2exp::TeX("$\\hat{s}(x)$"),
      ylab = "",
      col = colour_recalib[["Isotonic"]]
    )
  }
  mtext(side = 3, line = -0.25, adj = .5, subtitle, cex = .5)
}

plot_bp_xgb <- function(paper_version = FALSE) {
  # Focus on current scenario
  #scores_hist_scenario <- scores_hist_all[[scenario]]
  # Focus on a particular replication
  #scores_hist_repn <- scores_hist_scenario[[repn]]
  # # Focus on a value for max_depth
  max_depth_val <- map_dbl(scores_hist_all, ~.x[[1]]$max_depth)
  # i_max_depth <- which(max_depth_val == max_depth)
  # scores_hist <- scores_hist_repn[[i_max_depth]]
  
  # True Probabilities
  #simu_data <- simulate_data_wrapper(
    #scenario = scenario,
    #params_df = params_df,
   # repn = repn # only one replication here
  #)
  true_prob <- prior$scores_gamsel$scores_train
  #true_prob <-  simu_data$data$probs_train
  
  for (recalib_method in c("none", "platt", "iso")) {
    
    i_method <- match(recalib_method, c("none", "platt", "iso"))
    recalib_method_lab <- c("None", "Platt", "Isotonic")[i_method]
    
    # The metrics for the corresponding results, on the validation set
    metrics_xgb_current_valid <-
      metrics_xgb_all |>
      filter(
        sample == "Validation",
        recalib == "None"
      )
    # and on the test set
    metrics_xgb_current_test <-
      metrics_xgb_all |>
      filter(
        sample == "Test",
        recalib == recalib_method_lab
      )
    
    if (paper_version == FALSE) {
      hist(
        true_prob,
        breaks = seq(0, 1, by = .05),
        xlab = "p", ylab = "",
        main = "Prior Probabilities",
        xlim = c(0, 1)
      )
      mtext(
        side = 2, recalib_method_lab, line = 2.5, cex = 1,
        font.lab = 2
      )
      # Iterations of interest----
      ## Start of iterations
      scores_hist_start <- scores_hist_all[[1]][[1]]
      metrics_start <- metrics_xgb_current_test |>
        filter(
          nb_iter == scores_hist_start$nb_iter,
          max_depth == scores_hist_start$max_depth
        )
      
      plot_bp_interest(
        metrics_interest = metrics_start,
        scores_hist_interest = scores_hist_start,
        label = "Start",
        recalib_method = recalib_method
      )
      
      ## End of iterations
      scores_hist_end <- scores_hist_all[[1]][[length(scores_hist_all[[1]])]]
      metrics_end <- metrics_xgb_current_test |>
        filter(
          nb_iter == scores_hist_end$nb_iter,
          max_depth == scores_hist_start$max_depth
        )
      plot_bp_interest(
        metrics_interest = metrics_end,
        scores_hist_interest = scores_hist_end,
        label = "End",
        recalib_method = recalib_method
      )
      
      ## Iteration with min MSE on validation set
      #metrics_valid_mse_star <- metrics_xgb_current_valid |> arrange(mse) |>
      #  dplyr::slice(1)
      #nb_iter_mse <- metrics_valid_mse_star$nb_iter
      #max_depth_mse_star <- metrics_valid_mse_star$max_depth
      #i_max_depth_mse_star <- which(max_depth_val == max_depth_mse_star)
      # Metrics at the same iteration on the test set
      #metrics_min_mse <-
     #   metrics_xgb_current_test |>
     #   filter(
      #    nb_iter == !!nb_iter_mse,
      #    max_depth == max_depth_mse_star
      #  )
      # Note: indexing at 0 here...
      #ind_mse <- which(map_dbl(scores_hist_repn[[i_max_depth_mse_star]], "nb_iter") == nb_iter_mse)
      #scores_hist_min_mse <- scores_hist_repn[[i_max_depth_mse_star]][[ind_mse]]
      #plot_bp_interest(
      #  metrics_interest = metrics_min_mse,
      #  scores_hist_interest = scores_hist_min_mse,
       # label = "MSE*",
      #  recalib_method = recalib_method
      #)
    }
    ## Iteration with max AUC on validation set
    metrics_valid_auc_star <-
      metrics_xgb_current_valid |> arrange(desc(AUC)) |> dplyr::slice(1)
    nb_iter_auc <- metrics_valid_auc_star$nb_iter
    max_depth_auc_star <- metrics_valid_auc_star$max_depth
    i_max_depth_auc_star <- which(max_depth_val == max_depth_auc_star)
    
    metrics_max_auc <-
      metrics_xgb_current_test |>
      filter(nb_iter == !!nb_iter_auc, max_depth == max_depth_auc_star)
    # Note: indexing at 0 here...
    ind_auc <- which(map_dbl(scores_hist_all[[i_max_depth_auc_star]], "nb_iter") == nb_iter_auc)
    scores_hist_max_auc <- scores_hist_all[[i_max_depth_auc_star]][[ind_auc]]
    plot_bp_interest(
      metrics_interest = metrics_max_auc,
      scores_hist_interest = scores_hist_max_auc,
      label = "AUC*",
      recalib_method = recalib_method
    )
    if (paper_version == TRUE) {
      mtext(
        side = 2, recalib_method_lab, line = 2.5, cex = 1,
        font.lab = 2
      )
    }
    
    ## Min Brier on validation set
    metrics_valid_brier_star <-
      metrics_xgb_current_valid |> arrange(brier) |> dplyr::slice(1)
    nb_iter_brier <- metrics_valid_brier_star$nb_iter
    max_depth_brier_star <- metrics_valid_brier_star$max_depth
    i_max_depth_brier_star <- which(max_depth_val == max_depth_brier_star)
    
    metrics_min_brier <-
      metrics_xgb_current_test |>
      filter(nb_iter == !!nb_iter_brier, max_depth == max_depth_brier_star)
    ind_brier <- which(map_dbl(scores_hist_all[[i_max_depth_brier_star]], "nb_iter") == nb_iter_brier)
    scores_hist_min_brier <- scores_hist_all[[i_max_depth_brier_star]][[ind_brier]]
    plot_bp_interest(
      metrics_interest = metrics_min_brier,
      scores_hist_interest = scores_hist_min_brier,
      label = "Brier*",
      recalib_method = recalib_method
    )
    
    ## Min ICI on validation set
    metrics_valid_ici_star <-
      metrics_xgb_current_valid |> arrange(ici) |> dplyr::slice(1)
    nb_iter_ici <-   metrics_valid_ici_star$nb_iter
    max_depth_ici_star <- metrics_valid_ici_star$max_depth
    i_max_depth_ici_star <- which(max_depth_val == max_depth_ici_star)
    
    metrics_min_ici <-
      metrics_xgb_current_test |>
      filter(nb_iter == !!nb_iter_ici, max_depth == max_depth_ici_star)
    ind_ici <- which(map_dbl(scores_hist_all[[i_max_depth_ici_star]], "nb_iter") == nb_iter_ici)
    scores_hist_min_ici <- scores_hist_all[[i_max_depth_ici_star]][[ind_ici]]
    plot_bp_interest(
      metrics_interest = metrics_min_ici,
      scores_hist_interest = scores_hist_min_ici,
      label = "ICI*",
      recalib_method = recalib_method
    )
    
    ## Min KL on validation set
    metrics_valid_kl_star <-
      metrics_xgb_current_valid |> arrange(KL_20_true_probas) |> dplyr::slice(1)
    nb_iter_kl <- metrics_valid_kl_star$nb_iter
    max_depth_kl_star <- metrics_valid_kl_star$max_depth
    i_max_depth_kl_star <- which(max_depth_val == max_depth_kl_star)
    
    metrics_min_kl <-
      metrics_xgb_current_test |>
      filter(nb_iter == !!nb_iter_kl, max_depth == max_depth_kl_star)
    ind_kl <- which(map_dbl(scores_hist_all[[i_max_depth_kl_star]], "nb_iter") == nb_iter_kl)
    scores_hist_min_kl <- scores_hist_all[[i_max_depth_kl_star]][[ind_kl]]
    plot_bp_interest(
      metrics_interest = metrics_min_kl,
      scores_hist_interest = scores_hist_min_kl,
      label = "KL*",
      recalib_method = recalib_method
    )
    
    ## Mediocre ICI on validation set
    identified_mici <-  mediocre_ici_xgb
    
    metrics_valid_mici_star <- metrics_xgb_current_valid |> 
      filter(ind == identified_mici$ind, nb_iter == identified_mici$nb_iter)
    nb_iter_mici <- metrics_valid_mici_star$nb_iter
    max_depth_mici_star <- metrics_valid_mici_star$max_depth
    i_max_depth_mici_star <- which(max_depth_val == max_depth_mici_star)
    
    metrics_mici <-
      metrics_xgb_current_test |>
      filter(nb_iter == !!nb_iter_mici, max_depth == max_depth_mici_star)
    ind_mici <- which(map_dbl(scores_hist_all[[i_max_depth_mici_star]], "nb_iter") == nb_iter_mici)
    scores_hist_mici <- scores_hist_all[[i_max_depth_mici_star]][[ind_mici]]
    plot_bp_interest(
      metrics_interest = metrics_mici,
      scores_hist_interest = scores_hist_mici,
      label = "Med. ICI",
      recalib_method = recalib_method
    )
  }
}
```

```{r}
#| fig-cap: Distribution of scores on the test set (spambase)
#| label: fig-xgb-spambase-scores
#| code-fold: true
#| fig-height: 6
#| fig-width: 14
par(mfrow = c(3,8), mar = c(4.1, 4, 3.5, 1.5))
plot_bp_xgb()
```
```{r}

table_models_interest <- 
  models_of_interest_metrics |> 
  filter(sample == "Test") |> 
  select(
    recalib, sample, result_type, 
    brier, ici, kl = KL_20_true_probas
  ) |> 
  filter(
    result_type %in% c("AUC*", "KL*", "Largest")
  )


initial_points <- table_models_interest |> 
  filter(recalib == "None")

points_after_c <- initial_points |> 
  select(result_type, ici, kl) |> 
  left_join(
    table_models_interest |> 
      filter(recalib %in% c("Platt", "Isotonic")) |> 
      select(recalib, result_type, ici, kl) |> 
      rename(ici_end = ici, kl_end = kl),
    relationship = "many-to-many" # (Platt and Isotonic)
  )


ggplot() +
  geom_point(
    data = initial_points,
    mapping = aes(x = ici, y = kl, shape = result_type),
    size = 2
    ) +
  geom_segment(
    data = points_after_c,
    mapping = aes(
      x = ici, y = kl, xend = ici_end, yend = kl_end,
      colour = recalib, linetype = recalib
      ),
    arrow = arrow(length=unit(.2, 'cm'))
  ) +
  scale_shape_manual(
    NULL,
    values = c(
      # "Smallest" = 1,
      "Largest" = 2,
      "AUC*" = 19,
      "KL*" = 15
    )
  ) +
  labs(x = "ICI", y = "KL Divergence") +
  scale_colour_manual("Recalibration", values = colour_recalib) +
  scale_linetype_discrete("Recalibration") +
  theme_paper()
```


